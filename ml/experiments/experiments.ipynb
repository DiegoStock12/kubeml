{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Testing the experiment classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# from common.experiment import KubemlExperiment, History, TrainOptions, TrainRequest\n",
    "import pandas as pd\n",
    "import glob\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "import torch.utils.data as tdata\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class LeNet(nn.Module):\n",
    "    \"\"\" Definition of the LeNet network as per the 1998 paper\n",
    "\n",
    "    Credits to https://github.com/ChawDoe/LeNet5-MNIST-PyTorch for the\n",
    "    convenience of the network definition and the train loop found there\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.fc1 = nn.Linear(256, 120)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        self.relu5 = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.conv1(x)\n",
    "        y = self.relu1(y)\n",
    "        y = self.pool1(y)\n",
    "        y = self.conv2(y)\n",
    "        y = self.relu2(y)\n",
    "        y = self.pool2(y)\n",
    "        y = y.view(y.shape[0], -1)\n",
    "        y = self.fc1(y)\n",
    "        y = self.relu3(y)\n",
    "        y = self.fc2(y)\n",
    "        y = self.relu4(y)\n",
    "        y = self.fc3(y)\n",
    "        y = self.relu5(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n = LeNet()\n",
    "summary(n, input_size=(32, 1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mode = models.resnet.resnet34()\n",
    "\n",
    "summary(mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Properly implemented ResNet-s for CIFAR10 as described in paper [1].\n",
    "The implementation and structure of this file is hugely influenced by [2]\n",
    "which is implemented for ImageNet and doesn't have option A for identity.\n",
    "Moreover, most of the implementations on the web is copy-paste from\n",
    "torchvision's resnet and has wrong number of params.\n",
    "Proper ResNet-s for CIFAR10 (for fair comparision and etc.) has following\n",
    "number of layers and parameters:\n",
    "name      | layers | params\n",
    "ResNet20  |    20  | 0.27M\n",
    "ResNet32  |    32  | 0.46M\n",
    "ResNet44  |    44  | 0.66M\n",
    "ResNet56  |    56  | 0.85M\n",
    "ResNet110 |   110  |  1.7M\n",
    "ResNet1202|  1202  | 19.4m\n",
    "which this implementation indeed has.\n",
    "Reference:\n",
    "[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n",
    "    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n",
    "[2] https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py\n",
    "If you use this implementation in you work, please don't forget to mention the\n",
    "author, Yerlan Idelbayev.\n",
    "'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "__all__ = ['ResNet', 'resnet20', 'resnet32', 'resnet44', 'resnet56', 'resnet110', 'resnet1202']\n",
    "\n",
    "def _weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    #print(classname)\n",
    "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
    "        init.kaiming_normal_(m.weight)\n",
    "\n",
    "class LambdaLayer(nn.Module):\n",
    "    def __init__(self, lambd):\n",
    "        super(LambdaLayer, self).__init__()\n",
    "        self.lambd = lambd\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lambd(x)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1, option='A'):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes:\n",
    "            if option == 'A':\n",
    "                \"\"\"\n",
    "                For CIFAR10 ResNet paper uses option A.\n",
    "                \"\"\"\n",
    "                self.shortcut = LambdaLayer(lambda x:\n",
    "                                            F.pad(x[:, :, ::2, ::2], (0, 0, 0, 0, planes//4, planes//4), \"constant\", 0))\n",
    "            elif option == 'B':\n",
    "                self.shortcut = nn.Sequential(\n",
    "                     nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
    "                     nn.BatchNorm2d(self.expansion * planes)\n",
    "                )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 16\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 64, num_blocks[2], stride=2)\n",
    "        self.linear = nn.Linear(64, num_classes)\n",
    "\n",
    "        self.apply(_weights_init)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = F.avg_pool2d(out, out.size()[3])\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def resnet20():\n",
    "    return ResNet(BasicBlock, [3, 3, 3])\n",
    "\n",
    "\n",
    "def resnet32():\n",
    "    return ResNet(BasicBlock, [5, 5, 5])\n",
    "\n",
    "\n",
    "def resnet44():\n",
    "    return ResNet(BasicBlock, [7, 7, 7])\n",
    "\n",
    "\n",
    "def resnet56():\n",
    "    return ResNet(BasicBlock, [9, 9, 9])\n",
    "\n",
    "\n",
    "def resnet110():\n",
    "    return ResNet(BasicBlock, [18, 18, 18])\n",
    "\n",
    "\n",
    "def resnet1202():\n",
    "    return ResNet(BasicBlock, [200, 200, 200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = resnet56()\n",
    "\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, labels = np.load('./datasets/cifar10/cifar10_x_train.npy'), np.load('./datasets/cifar10/cifar10_y_train.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "train_transf = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, 4),\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "])\n",
    "\n",
    "val_transf = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import nll_loss, cross_entropy\n",
    "\n",
    "def train(model: nn.Module, device,\n",
    "          train_loader: tdata.DataLoader,\n",
    "          optimizer: torch.optim.Optimizer, epoch) -> float:\n",
    "    \"\"\"Loop used to train the network\"\"\"\n",
    "\n",
    "    # create optimizer\n",
    "    # optimizer = optim.SGD(model.parameters(), lr=0.1, weight_decay=1e-4, momentum=0.9)\n",
    "\n",
    "    # load_state(optimizer)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    model.train()\n",
    "    loss, tot = 0, 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "\n",
    "\n",
    "        loss = criterion(output, target)\n",
    "        tot += loss.item()\n",
    "        \n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "\n",
    "        if batch_idx % 30 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                   100. * batch_idx / len(train_loader), loss.item()))\n",
    "            \n",
    "\n",
    "    # save the optimizer state\n",
    "    # save_state(optimizer)\n",
    "\n",
    "    return tot/len(train_loader)\n",
    "\n",
    "\n",
    "def validate(model, device, val_loader: tdata.DataLoader) -> (float, float):\n",
    "    \"\"\"Loop used to validate the network\"\"\"\n",
    "\n",
    "    criterion =nn.CrossEntropyLoss()\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in val_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            test_loss += cross_entropy(output, target).item()  # sum up batch loss\n",
    "            correct += predicted.eq(target).sum().item()\n",
    "\n",
    "    test_loss /= len(val_loader)\n",
    "\n",
    "    accuracy = 100. * correct / len(val_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(val_loader.dataset),\n",
    "        100. * correct / len(val_loader.dataset)))\n",
    "    return accuracy, test_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to load and save state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def load_state(optimizer):\n",
    "    if os.path.isfile('state.pkl'):\n",
    "        with open('state.pkl', 'rb') as f:\n",
    "            state = pickle.load(f)\n",
    "            update_state(optimizer, state)\n",
    "\n",
    "    else:\n",
    "        print('no state found')\n",
    "\n",
    "\n",
    "def update_state(optimizer, state):\n",
    "    state = {\n",
    "      'param_groups': optimizer.state_dict()['param_groups'],\n",
    "      'state': state\n",
    "    }\n",
    "    optimizer.load_state_dict(state)\n",
    "\n",
    "def save_state(optimizer):\n",
    "    print('saving optimizer state')\n",
    "    with open('state.pkl', 'wb') as f:\n",
    "        pickle.dump(optimizer.state_dict()['state'], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.resnet import resnet18\n",
    "\n",
    "torch.manual_seed(42) \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "\n",
    "# Create the model\n",
    "# model = create_model(init=True).to(device)\n",
    "model = resnet18().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistData(tdata.Dataset):\n",
    "    def __init__(self, feat, labels):\n",
    "        self.feat = feat\n",
    "        self.labels = labels.flatten()\n",
    "        self.normalize = transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "        self.transf = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            self.normalize\n",
    "            ])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.feat)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.transf(self.feat[idx]), self.labels[idx].astype(np.int64)\n",
    "    \n",
    "train_data = MnistData(x_train, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.vgg16()\n",
    "input_lastLayer = model.classifier[6].in_features\n",
    "model.classifier[6] = nn.Linear(input_lastLayer,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "├─Sequential: 1-1                        [32, 512, 1, 1]           --\n",
       "|    └─Conv2d: 2-1                       [32, 64, 32, 32]          1,792\n",
       "|    └─ReLU: 2-2                         [32, 64, 32, 32]          --\n",
       "|    └─Conv2d: 2-3                       [32, 64, 32, 32]          36,928\n",
       "|    └─ReLU: 2-4                         [32, 64, 32, 32]          --\n",
       "|    └─MaxPool2d: 2-5                    [32, 64, 16, 16]          --\n",
       "|    └─Conv2d: 2-6                       [32, 128, 16, 16]         73,856\n",
       "|    └─ReLU: 2-7                         [32, 128, 16, 16]         --\n",
       "|    └─Conv2d: 2-8                       [32, 128, 16, 16]         147,584\n",
       "|    └─ReLU: 2-9                         [32, 128, 16, 16]         --\n",
       "|    └─MaxPool2d: 2-10                   [32, 128, 8, 8]           --\n",
       "|    └─Conv2d: 2-11                      [32, 256, 8, 8]           295,168\n",
       "|    └─ReLU: 2-12                        [32, 256, 8, 8]           --\n",
       "|    └─Conv2d: 2-13                      [32, 256, 8, 8]           590,080\n",
       "|    └─ReLU: 2-14                        [32, 256, 8, 8]           --\n",
       "|    └─Conv2d: 2-15                      [32, 256, 8, 8]           590,080\n",
       "|    └─ReLU: 2-16                        [32, 256, 8, 8]           --\n",
       "|    └─MaxPool2d: 2-17                   [32, 256, 4, 4]           --\n",
       "|    └─Conv2d: 2-18                      [32, 512, 4, 4]           1,180,160\n",
       "|    └─ReLU: 2-19                        [32, 512, 4, 4]           --\n",
       "|    └─Conv2d: 2-20                      [32, 512, 4, 4]           2,359,808\n",
       "|    └─ReLU: 2-21                        [32, 512, 4, 4]           --\n",
       "|    └─Conv2d: 2-22                      [32, 512, 4, 4]           2,359,808\n",
       "|    └─ReLU: 2-23                        [32, 512, 4, 4]           --\n",
       "|    └─MaxPool2d: 2-24                   [32, 512, 2, 2]           --\n",
       "|    └─Conv2d: 2-25                      [32, 512, 2, 2]           2,359,808\n",
       "|    └─ReLU: 2-26                        [32, 512, 2, 2]           --\n",
       "|    └─Conv2d: 2-27                      [32, 512, 2, 2]           2,359,808\n",
       "|    └─ReLU: 2-28                        [32, 512, 2, 2]           --\n",
       "|    └─Conv2d: 2-29                      [32, 512, 2, 2]           2,359,808\n",
       "|    └─ReLU: 2-30                        [32, 512, 2, 2]           --\n",
       "|    └─MaxPool2d: 2-31                   [32, 512, 1, 1]           --\n",
       "├─AdaptiveAvgPool2d: 1-2                 [32, 512, 7, 7]           --\n",
       "├─Sequential: 1-3                        [32, 10]                  --\n",
       "|    └─Linear: 2-32                      [32, 4096]                102,764,544\n",
       "|    └─ReLU: 2-33                        [32, 4096]                --\n",
       "|    └─Dropout: 2-34                     [32, 4096]                --\n",
       "|    └─Linear: 2-35                      [32, 4096]                16,781,312\n",
       "|    └─ReLU: 2-36                        [32, 4096]                --\n",
       "|    └─Dropout: 2-37                     [32, 4096]                --\n",
       "|    └─Linear: 2-38                      [32, 10]                  40,970\n",
       "==========================================================================================\n",
       "Total params: 134,301,514\n",
       "Trainable params: 134,301,514\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 567.06\n",
       "==========================================================================================\n",
       "Input size (MB): 0.39\n",
       "Forward/backward pass size (MB): 72.88\n",
       "Params size (MB): 537.21\n",
       "Estimated Total Size (MB): 610.48\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model, (32, 3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "  (1): ReLU(inplace=True)\n",
       "  (2): Dropout(p=0.5, inplace=False)\n",
       "  (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "  (4): ReLU(inplace=True)\n",
       "  (5): Dropout(p=0.5, inplace=False)\n",
       "  (6): Linear(in_features=4096, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Train Epoch: 0 [0/50000 (0%)]\tLoss: 6.899667\n",
      "Train Epoch: 0 [30/50000 (0%)]\tLoss: nan\n",
      "Train Epoch: 0 [60/50000 (0%)]\tLoss: nan\n",
      "Train Epoch: 0 [90/50000 (0%)]\tLoss: nan\n",
      "Train Epoch: 0 [120/50000 (0%)]\tLoss: nan\n",
      "Train Epoch: 0 [150/50000 (0%)]\tLoss: nan\n",
      "Train Epoch: 0 [180/50000 (0%)]\tLoss: nan\n",
      "Train Epoch: 0 [210/50000 (0%)]\tLoss: nan\n",
      "Train Epoch: 0 [240/50000 (0%)]\tLoss: nan\n",
      "Train Epoch: 0 [270/50000 (1%)]\tLoss: nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-57e736b785d6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Epoch'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;31m#     save_state(optimizer)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m#     validate(model, device, val_loader)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-7a80c4734088>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, device, train_loader, optimizer, epoch)\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = models.vgg11().to(device)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=1)\n",
    "# val_loader = torch.utils.data.DataLoader(val_data, batch_size=128)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.1, weight_decay=5e-4)\n",
    "\n",
    "for epoch in range(1):\n",
    "    # create the optimizer in each iteration and load state\n",
    "#     optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4)\n",
    "#     load_state(optimizer)\n",
    "\n",
    "    print('Epoch', epoch)\n",
    "    \n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "#     save_state(optimizer)\n",
    "#     validate(model, device, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 1            |        cudaMalloc retries: 2         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |    2422 MB |    2814 MB |    3188 MB |  784719 KB |\n",
      "|       from large pool |    2420 MB |    2812 MB |    3170 MB |  767976 KB |\n",
      "|       from small pool |       1 MB |       7 MB |      17 MB |   16743 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |    2422 MB |    2814 MB |    3188 MB |  784719 KB |\n",
      "|       from large pool |    2420 MB |    2812 MB |    3170 MB |  767976 KB |\n",
      "|       from small pool |       1 MB |       7 MB |      17 MB |   16743 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |    2454 MB |    2846 MB |    2868 MB |  423936 KB |\n",
      "|       from large pool |    2446 MB |    2838 MB |    2858 MB |  421888 KB |\n",
      "|       from small pool |       8 MB |      10 MB |      10 MB |    2048 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   32421 KB |   48891 KB |  372748 KB |  340326 KB |\n",
      "|       from large pool |   25728 KB |   45568 KB |  349660 KB |  323932 KB |\n",
      "|       from small pool |    6693 KB |    7386 KB |   23088 KB |   16394 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |      94    |      96    |     222    |     128    |\n",
      "|       from large pool |      37    |      39    |      72    |      35    |\n",
      "|       from small pool |      57    |      60    |     150    |      93    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |      94    |      96    |     222    |     128    |\n",
      "|       from large pool |      37    |      39    |      72    |      35    |\n",
      "|       from small pool |      57    |      60    |     150    |      93    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      23    |      25    |      26    |       3    |\n",
      "|       from large pool |      19    |      20    |      21    |       2    |\n",
      "|       from small pool |       4    |       5    |       5    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      12    |      14    |      64    |      52    |\n",
      "|       from large pool |       5    |       7    |      25    |      20    |\n",
      "|       from small pool |       7    |       9    |      39    |      32    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_summary(abbreviated=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see how the VGG performs\n",
    "vgg = models.vgg11()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "├─Sequential: 1-1                        [128, 512, 1, 1]          --\n",
       "|    └─Conv2d: 2-1                       [128, 64, 32, 32]         1,792\n",
       "|    └─ReLU: 2-2                         [128, 64, 32, 32]         --\n",
       "|    └─MaxPool2d: 2-3                    [128, 64, 16, 16]         --\n",
       "|    └─Conv2d: 2-4                       [128, 128, 16, 16]        73,856\n",
       "|    └─ReLU: 2-5                         [128, 128, 16, 16]        --\n",
       "|    └─MaxPool2d: 2-6                    [128, 128, 8, 8]          --\n",
       "|    └─Conv2d: 2-7                       [128, 256, 8, 8]          295,168\n",
       "|    └─ReLU: 2-8                         [128, 256, 8, 8]          --\n",
       "|    └─Conv2d: 2-9                       [128, 256, 8, 8]          590,080\n",
       "|    └─ReLU: 2-10                        [128, 256, 8, 8]          --\n",
       "|    └─MaxPool2d: 2-11                   [128, 256, 4, 4]          --\n",
       "|    └─Conv2d: 2-12                      [128, 512, 4, 4]          1,180,160\n",
       "|    └─ReLU: 2-13                        [128, 512, 4, 4]          --\n",
       "|    └─Conv2d: 2-14                      [128, 512, 4, 4]          2,359,808\n",
       "|    └─ReLU: 2-15                        [128, 512, 4, 4]          --\n",
       "|    └─MaxPool2d: 2-16                   [128, 512, 2, 2]          --\n",
       "|    └─Conv2d: 2-17                      [128, 512, 2, 2]          2,359,808\n",
       "|    └─ReLU: 2-18                        [128, 512, 2, 2]          --\n",
       "|    └─Conv2d: 2-19                      [128, 512, 2, 2]          2,359,808\n",
       "|    └─ReLU: 2-20                        [128, 512, 2, 2]          --\n",
       "|    └─MaxPool2d: 2-21                   [128, 512, 1, 1]          --\n",
       "├─AdaptiveAvgPool2d: 1-2                 [128, 512, 7, 7]          --\n",
       "├─Sequential: 1-3                        [128, 1000]               --\n",
       "|    └─Linear: 2-22                      [128, 4096]               102,764,544\n",
       "|    └─ReLU: 2-23                        [128, 4096]               --\n",
       "|    └─Dropout: 2-24                     [128, 4096]               --\n",
       "|    └─Linear: 2-25                      [128, 4096]               16,781,312\n",
       "|    └─ReLU: 2-26                        [128, 4096]               --\n",
       "|    └─Dropout: 2-27                     [128, 4096]               --\n",
       "|    └─Linear: 2-28                      [128, 1000]               4,097,000\n",
       "==========================================================================================\n",
       "Total params: 132,863,336\n",
       "Trainable params: 132,863,336\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 409.25\n",
       "==========================================================================================\n",
       "Input size (MB): 1.57\n",
       "Forward/backward pass size (MB): 164.60\n",
       "Params size (MB): 531.45\n",
       "Estimated Total Size (MB): 697.63\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(vgg, (128, 3, 32, 32))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
