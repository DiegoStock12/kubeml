{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Gather insights from the experiments run on GCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from common.utils import check_missing_experiments, join_df\n",
    "from common.metrics import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the TTA Formula "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate time to accuracy for different accuracies\n",
    "def tta_crossbow(acc:int, df: pd.DataFrame, acc_column='accuracy', time_column='epoch_duration'):\n",
    "    \"\"\"Computes the tta as in the crossbow paper\n",
    "    where the tta is the median of the last 5 epochs\"\"\"\n",
    "\n",
    "    res = []\n",
    "    for _, row in df.iterrows():\n",
    "        done = False\n",
    "        dur, accuracy = row[time_column], row[acc_column]\n",
    "        \n",
    "        for idx, (t, a) in enumerate(zip(dur, accuracy[:len(dur)])):\n",
    "            \n",
    "            # if there are less than 5 elements behind, continue\n",
    "            if idx < 4:\n",
    "                continue\n",
    "                \n",
    "            # calculate the median of the next five elements\n",
    "            if np.median(accuracy[idx - 4:idx+1]) >= acc:\n",
    "                res.append(t)\n",
    "                done = True\n",
    "                break\n",
    "\n",
    "        if not done:\n",
    "            res.append(np.nan)\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "def tta(acc:int, df:pd.DataFrame,  acc_column='accuracy', time_column='epoch_duration'):\n",
    "    \n",
    "    \n",
    "    res = []\n",
    "    for _, row in df.iterrows():\n",
    "        done=False\n",
    "        dur, accuracy = row[time_column], row[acc_column]\n",
    "        \n",
    "        for idx, (t, a) in enumerate(zip(dur, accuracy[:len(dur)])):\n",
    "         \n",
    "            if a >= acc:\n",
    "                res.append(t)\n",
    "                done = True\n",
    "                break\n",
    "\n",
    "        if not done:\n",
    "            res.append(np.nan)\n",
    "\n",
    "    return res\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KubeML Experiments\n",
    "\n",
    "How to treat the kubeml experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the train results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = join_df('./results/resnet/train')\n",
    "df = df[df.default_parallelism > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>hash</th>\n",
       "      <th>model_type</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>epochs</th>\n",
       "      <th>dataset</th>\n",
       "      <th>lr</th>\n",
       "      <th>function_name</th>\n",
       "      <th>default_parallelism</th>\n",
       "      <th>static_parallelism</th>\n",
       "      <th>validate_every</th>\n",
       "      <th>k</th>\n",
       "      <th>goal_accuracy</th>\n",
       "      <th>validation_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>parallelism</th>\n",
       "      <th>epoch_duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0386935d</td>\n",
       "      <td>69979e67b7622197</td>\n",
       "      <td>example</td>\n",
       "      <td>128</td>\n",
       "      <td>30</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>0.1</td>\n",
       "      <td>resnet</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>100</td>\n",
       "      <td>[2.0691979338851154, 1.761023279986804, 1.5237...</td>\n",
       "      <td>[21.41020569620253, 32.070806962025316, 47.784...</td>\n",
       "      <td>[3.4358344075022913, 1.9820176490715573, 1.680...</td>\n",
       "      <td>[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, ...</td>\n",
       "      <td>[46.101114115, 92.594095861, 138.901519929, 18...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>05666e5d</td>\n",
       "      <td>a2cc2a5d5cc402f9</td>\n",
       "      <td>example</td>\n",
       "      <td>128</td>\n",
       "      <td>30</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>0.1</td>\n",
       "      <td>resnet</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>100</td>\n",
       "      <td>[2.1832512511482722, 1.6482713418670847, 1.520...</td>\n",
       "      <td>[20.352056962025316, 39.47784810126582, 45.203...</td>\n",
       "      <td>[3.633183332122102, 1.9733640703619744, 1.6998...</td>\n",
       "      <td>[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, ...</td>\n",
       "      <td>[19.251981886, 39.583568774, 60.468725951, 79....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>06804c15</td>\n",
       "      <td>8b0f4cc587901704</td>\n",
       "      <td>example</td>\n",
       "      <td>256</td>\n",
       "      <td>30</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>0.1</td>\n",
       "      <td>resnet</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>100</td>\n",
       "      <td>[2.592677044868469, 1.7636539191007614, 1.6012...</td>\n",
       "      <td>[17.958984375, 33.1640625, 39.794921875, 44.40...</td>\n",
       "      <td>[4.071233475694851, 2.1830299715606536, 1.8489...</td>\n",
       "      <td>[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, ...</td>\n",
       "      <td>[26.678817886, 53.53629918, 78.251857194, 103....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0bb3e9ac</td>\n",
       "      <td>50f218a144063794</td>\n",
       "      <td>example</td>\n",
       "      <td>64</td>\n",
       "      <td>30</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>0.1</td>\n",
       "      <td>resnet</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>100</td>\n",
       "      <td>[2.01867682842692, 1.906603324944806, 1.419383...</td>\n",
       "      <td>[28.54299363057325, 28.61265923566879, 47.2730...</td>\n",
       "      <td>[2.624373005631635, 1.756249717129466, 1.54868...</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[102.032926309, 201.191873608, 299.563335538, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13190b0b</td>\n",
       "      <td>02b1f56c9c27a5b4</td>\n",
       "      <td>example</td>\n",
       "      <td>128</td>\n",
       "      <td>30</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>0.1</td>\n",
       "      <td>resnet</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>100</td>\n",
       "      <td>[5.0395004236245455, 1.5423524047754988, 1.449...</td>\n",
       "      <td>[10.116693037974683, 44.11590189873418, 47.567...</td>\n",
       "      <td>[2.962399608322552, 1.6866237937795874, 1.4655...</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[23.07288706, 46.800417874, 74.283571081, 98.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>f86d546d</td>\n",
       "      <td>4cf9aff8b0e3ec9e</td>\n",
       "      <td>example</td>\n",
       "      <td>64</td>\n",
       "      <td>30</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>0.1</td>\n",
       "      <td>resnet</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>100</td>\n",
       "      <td>[3.8288797876637455, 2.403817397014351, 1.9774...</td>\n",
       "      <td>[10, 10.26, 26.85, 35.03, 40.550000000000004, ...</td>\n",
       "      <td>[3.360841138029304, 2.3875012396939574, 1.9948...</td>\n",
       "      <td>[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, ...</td>\n",
       "      <td>[14.801709896, 31.24002004, 48.253630979, 64.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>f9dc984a</td>\n",
       "      <td>9c49eeb8a706275a</td>\n",
       "      <td>example</td>\n",
       "      <td>64</td>\n",
       "      <td>30</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>0.1</td>\n",
       "      <td>resnet</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>100</td>\n",
       "      <td>[1.705808477037272, 1.4686595649476264, 1.3200...</td>\n",
       "      <td>[37.430334394904456, 46.34753184713376, 52.478...</td>\n",
       "      <td>[2.712390104218212, 1.6115226571822105, 1.4244...</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[38.899608234, 82.870589755, 126.082686868, 16...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>f9ff89fc</td>\n",
       "      <td>7e945a487feff74e</td>\n",
       "      <td>example</td>\n",
       "      <td>128</td>\n",
       "      <td>30</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>0.1</td>\n",
       "      <td>resnet</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>100</td>\n",
       "      <td>[2.1471189894253695, 1.759268745591369, 1.5828...</td>\n",
       "      <td>[22.34, 33.46, 41.81, 47.64, 48.33999999999999...</td>\n",
       "      <td>[3.2209879354174644, 1.9946543223091535, 1.743...</td>\n",
       "      <td>[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, ...</td>\n",
       "      <td>[27.999510971, 57.792588364, 86.948315407, 115...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>fbcccc6c</td>\n",
       "      <td>afe503f94d4d8264</td>\n",
       "      <td>example</td>\n",
       "      <td>64</td>\n",
       "      <td>30</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>0.1</td>\n",
       "      <td>resnet</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>100</td>\n",
       "      <td>[1.8421584147556571, 1.573136512640935, 1.5360...</td>\n",
       "      <td>[31.22014331210191, 41.49084394904459, 42.8841...</td>\n",
       "      <td>[3.297370414573881, 1.8065422634400665, 1.6034...</td>\n",
       "      <td>[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, ...</td>\n",
       "      <td>[31.308027951, 61.831227029, 90.963939092, 120...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>fd57f4d1</td>\n",
       "      <td>1d0577c2b0263cbe</td>\n",
       "      <td>example</td>\n",
       "      <td>256</td>\n",
       "      <td>30</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>0.1</td>\n",
       "      <td>resnet</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>100</td>\n",
       "      <td>[1.930098184943199, 1.718866342306137, 1.75740...</td>\n",
       "      <td>[27.470703125, 36.1328125, 41.89453125, 45.966...</td>\n",
       "      <td>[3.2220769907747, 1.9234242487926871, 1.651999...</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[32.766331703, 66.356944417, 101.061452874, 13...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>96 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id              hash model_type  batch_size  epochs  dataset   lr  \\\n",
       "0    0386935d  69979e67b7622197    example         128      30  cifar10  0.1   \n",
       "1    05666e5d  a2cc2a5d5cc402f9    example         128      30  cifar10  0.1   \n",
       "2    06804c15  8b0f4cc587901704    example         256      30  cifar10  0.1   \n",
       "3    0bb3e9ac  50f218a144063794    example          64      30  cifar10  0.1   \n",
       "4    13190b0b  02b1f56c9c27a5b4    example         128      30  cifar10  0.1   \n",
       "..        ...               ...        ...         ...     ...      ...  ...   \n",
       "97   f86d546d  4cf9aff8b0e3ec9e    example          64      30  cifar10  0.1   \n",
       "98   f9dc984a  9c49eeb8a706275a    example          64      30  cifar10  0.1   \n",
       "99   f9ff89fc  7e945a487feff74e    example         128      30  cifar10  0.1   \n",
       "100  fbcccc6c  afe503f94d4d8264    example          64      30  cifar10  0.1   \n",
       "101  fd57f4d1  1d0577c2b0263cbe    example         256      30  cifar10  0.1   \n",
       "\n",
       "    function_name  default_parallelism  static_parallelism  validate_every  \\\n",
       "0          resnet                    4                True               1   \n",
       "1          resnet                    4                True               1   \n",
       "2          resnet                    4                True               1   \n",
       "3          resnet                    2                True               1   \n",
       "4          resnet                    2                True               1   \n",
       "..            ...                  ...                 ...             ...   \n",
       "97         resnet                    4                True               1   \n",
       "98         resnet                    2                True               1   \n",
       "99         resnet                    4                True               1   \n",
       "100        resnet                    4                True               1   \n",
       "101        resnet                    2                True               1   \n",
       "\n",
       "      k  goal_accuracy                                    validation_loss  \\\n",
       "0     8            100  [2.0691979338851154, 1.761023279986804, 1.5237...   \n",
       "1    32            100  [2.1832512511482722, 1.6482713418670847, 1.520...   \n",
       "2     8            100  [2.592677044868469, 1.7636539191007614, 1.6012...   \n",
       "3     8            100  [2.01867682842692, 1.906603324944806, 1.419383...   \n",
       "4    32            100  [5.0395004236245455, 1.5423524047754988, 1.449...   \n",
       "..   ..            ...                                                ...   \n",
       "97   -1            100  [3.8288797876637455, 2.403817397014351, 1.9774...   \n",
       "98   32            100  [1.705808477037272, 1.4686595649476264, 1.3200...   \n",
       "99   16            100  [2.1471189894253695, 1.759268745591369, 1.5828...   \n",
       "100  32            100  [1.8421584147556571, 1.573136512640935, 1.5360...   \n",
       "101   8            100  [1.930098184943199, 1.718866342306137, 1.75740...   \n",
       "\n",
       "                                              accuracy  \\\n",
       "0    [21.41020569620253, 32.070806962025316, 47.784...   \n",
       "1    [20.352056962025316, 39.47784810126582, 45.203...   \n",
       "2    [17.958984375, 33.1640625, 39.794921875, 44.40...   \n",
       "3    [28.54299363057325, 28.61265923566879, 47.2730...   \n",
       "4    [10.116693037974683, 44.11590189873418, 47.567...   \n",
       "..                                                 ...   \n",
       "97   [10, 10.26, 26.85, 35.03, 40.550000000000004, ...   \n",
       "98   [37.430334394904456, 46.34753184713376, 52.478...   \n",
       "99   [22.34, 33.46, 41.81, 47.64, 48.33999999999999...   \n",
       "100  [31.22014331210191, 41.49084394904459, 42.8841...   \n",
       "101  [27.470703125, 36.1328125, 41.89453125, 45.966...   \n",
       "\n",
       "                                            train_loss  \\\n",
       "0    [3.4358344075022913, 1.9820176490715573, 1.680...   \n",
       "1    [3.633183332122102, 1.9733640703619744, 1.6998...   \n",
       "2    [4.071233475694851, 2.1830299715606536, 1.8489...   \n",
       "3    [2.624373005631635, 1.756249717129466, 1.54868...   \n",
       "4    [2.962399608322552, 1.6866237937795874, 1.4655...   \n",
       "..                                                 ...   \n",
       "97   [3.360841138029304, 2.3875012396939574, 1.9948...   \n",
       "98   [2.712390104218212, 1.6115226571822105, 1.4244...   \n",
       "99   [3.2209879354174644, 1.9946543223091535, 1.743...   \n",
       "100  [3.297370414573881, 1.8065422634400665, 1.6034...   \n",
       "101  [3.2220769907747, 1.9234242487926871, 1.651999...   \n",
       "\n",
       "                                           parallelism  \\\n",
       "0    [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, ...   \n",
       "1    [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, ...   \n",
       "2    [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, ...   \n",
       "3    [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "4    [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "..                                                 ...   \n",
       "97   [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, ...   \n",
       "98   [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "99   [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, ...   \n",
       "100  [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, ...   \n",
       "101  [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "\n",
       "                                        epoch_duration  \n",
       "0    [46.101114115, 92.594095861, 138.901519929, 18...  \n",
       "1    [19.251981886, 39.583568774, 60.468725951, 79....  \n",
       "2    [26.678817886, 53.53629918, 78.251857194, 103....  \n",
       "3    [102.032926309, 201.191873608, 299.563335538, ...  \n",
       "4    [23.07288706, 46.800417874, 74.283571081, 98.0...  \n",
       "..                                                 ...  \n",
       "97   [14.801709896, 31.24002004, 48.253630979, 64.1...  \n",
       "98   [38.899608234, 82.870589755, 126.082686868, 16...  \n",
       "99   [27.999510971, 57.792588364, 86.948315407, 115...  \n",
       "100  [31.308027951, 61.831227029, 90.963939092, 120...  \n",
       "101  [32.766331703, 66.356944417, 101.061452874, 13...  \n",
       "\n",
       "[96 rows x 18 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get duplicated rows\n",
    "df[df.duplicated(['hash'], keep=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the extra variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Set the acc to the final accuracy\n",
    "df['acc'] = df.accuracy.map(lambda a: a[-1])\n",
    "\n",
    "# Set the time to the sum of the epoch durations\n",
    "df['time'] = df.epoch_duration.map(lambda t: t[-1])\n",
    "\n",
    "# Set the parallelism to the first since it is constant\n",
    "df.parallelism = df.parallelism.map(lambda l:l[0])\n",
    "\n",
    "# change -1 to inf so the order is right in the plot\n",
    "df.k = df.k.map(lambda val: float('inf') if val == -1 else val)\n",
    "\n",
    "df['global_batch'] = df.batch_size * df.parallelism\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the ttas\n",
    "df['tta_69'] = tta(69, df)\n",
    "df['tta_cross_69'] = tta_crossbow(69, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For the metrics in resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = join_df('./results/resnet/metrics')\n",
    "m = metrics.rename(columns={'exp_name':'id'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For the metrics in the lenet\n",
    "\n",
    "The first replication does not have the proper format, so we need to reformat it and combine it with the train results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics1 = join_df('./results/lenet/metrics/1/')\n",
    "metrics2 = join_df('./results/lenet/metrics/2/')\n",
    "metrics3 = join_df('./results/lenet/metrics/3/')\n",
    "\n",
    "cpu = metrics1.groupby('exp_name')['cpu'].apply(list)\n",
    "mem = metrics1.groupby('exp_name')['mem'].apply(list)\n",
    "exps = metrics1.groupby('exp_name')['exp_name']\n",
    "\n",
    "metrics1 = pd.DataFrame({\n",
    "    'cpu':cpu,\n",
    "    'mem':mem\n",
    "})\n",
    "metrics1['exp_name'] = metrics1.index\n",
    "\n",
    "# concat all metrics and rename the exp_name as in the train\n",
    "m = pd.concat([metrics1, metrics2, metrics3], ignore_index=True)\n",
    "m.rename(columns={'exp_name':'id'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to add extra summary columns to the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to compute the mean of each and add columns\n",
    "m['mem'] = m['mem'].map(lambda l: l[0] if isinstance(l[0], list) else l)\n",
    "m['cpu'] = m['cpu'].map(lambda l: l[0] if isinstance(l[0], list) else l)\n",
    "\n",
    "# cpu util\n",
    "m['cpu_mean'] = m['cpu'].map(lambda l: np.mean([s.percent for s in l]))\n",
    "\n",
    "# gpu mean mem and util\n",
    "m['gpu_0_mean_usage'] = m['gpu_0'].map(lambda l: np.mean([s.load for s in l if s.mem_used != 0]) if not isinstance(l, float) else l)\n",
    "m['gpu_1_mean_usage'] = m['gpu_1'].map(lambda l: np.mean([s.load for s in l if s.mem_used !=0]) if not isinstance(l, float) else l)\n",
    "m['gpu_0_mean_memory'] = m['gpu_0'].map(lambda l: np.mean([s.mem_used for s in l if s.mem_used != 0])if not isinstance(l, float) else l)\n",
    "m['gpu_1_mean_memory'] = m['gpu_1'].map(lambda l: np.mean([s.mem_used for s in l if s.mem_used != 0])if not isinstance(l, float) else l)\n",
    "m['gpu_usage'] = (m['gpu_0_mean_usage'] + m['gpu_1_mean_usage']) /2\n",
    "\n",
    "# memory mean util\n",
    "m['mem_mean'] = m['mem'].map(lambda l: np.mean([s.percent for s in l]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine and Save the whole experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = df.merge(m, on='id')\n",
    "d.to_pickle('./dataframes/resnet_kubeml.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Experiments\n",
    "\n",
    "How to treat the TF experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = join_df('./results/tf/lenet/train/1/', './results/tf/lenet/train/2', './results/tf/lenet/train/3')\n",
    "\n",
    "# Set the acc to the final accuracy\n",
    "df['acc'] = df.val_accuracy.map(lambda a: a[-1])\n",
    "\n",
    "# Set the time to the sum of the epoch durations\n",
    "df['time'] = df.times.map(lambda t: t[-1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the TTA\n",
    "resnet['tta_67'] = tta(0.69, resnet, time_column='times', acc_column='val_accuracy')\n",
    "resnet['tta_cross_67'] = tta_crossbow(0.69, resnet, time_column='times', acc_column='val_accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge the metrics from different  folders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "m = join_df('./results/tf/lenet/metrics/1/', './results/tf/lenet/metrics/2', './results/tf/lenet/metrics/3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to compute the mean of each and add columns\n",
    "m['mem'] = m['mem'].map(lambda l: l[0] if isinstance(l[0], list) else l)\n",
    "m['cpu'] = m['cpu'].map(lambda l: l[0] if isinstance(l[0], list) else l)\n",
    "\n",
    "# cpu util\n",
    "m['cpu_mean'] = m['cpu'].map(lambda l: np.mean([s.percent for s in l]))\n",
    "\n",
    "# gpu mean mem and util\n",
    "m['gpu_0_mean_usage'] = m['gpu_0'].map(lambda l: np.mean([s.load for s in l if s.mem_used != 0]) if not isinstance(l, float) else l)\n",
    "m['gpu_1_mean_usage'] = m['gpu_1'].map(lambda l: np.mean([s.load for s in l if s.mem_used !=0]) if not isinstance(l, float) else l)\n",
    "m['gpu_0_mean_memory'] = m['gpu_0'].map(lambda l: np.mean([s.mem_used for s in l if s.mem_used != 0])if not isinstance(l, float) else l)\n",
    "m['gpu_1_mean_memory'] = m['gpu_1'].map(lambda l: np.mean([s.mem_used for s in l if s.mem_used != 0])if not isinstance(l, float) else l)\n",
    "m['gpu_usage'] = (m['gpu_0_mean_usage'] + m['gpu_1_mean_usage']) /2\n",
    "\n",
    "# memory mean util\n",
    "m['mem_mean'] = m['mem'].map(lambda l: np.mean([s.percent for s in l]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join on the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.join(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet.to_pickle('./dataframes/resnet_tensorflow.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization of Results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the color palette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "green_red_palette = ['#264653', '#2A9D8F', '#E9C46A', '#F4A261', '#E76F51']\n",
    "blue_yellow_palette=  ['#0077b6', '#d62828', '#f77f00', '#fcbf49', '#eae2b7']\n",
    "cool_p = ['#f87575', '#ffa9a3', '#b9e6ff', '#5c95ff', '#7e6c6c']\n",
    "wall_p = ['#e63946', '#f1faee', '#a8dadc', '#457b9d', '#1d3557']\n",
    "\n",
    "sns.palplot(sns.color_palette(blue_yellow_palette))\n",
    "\n",
    "\n",
    "sns.set_theme(style='whitegrid', palette=blue_yellow_palette, )\n",
    "# sns.set_palette(blue_yellow_palette)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Read the experiments file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load all of the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load the tf experiments\n",
    "resnet = pd.read_pickle('./dataframes/resnet_tensorflow.pkl')\n",
    "lenet = pd.read_pickle('./dataframes/lenet_tensorflow.pkl')\n",
    "\n",
    "resnet.rename(columns={\n",
    "    'loss':'train_loss',\n",
    "    'val_accuracy':'accuracy',\n",
    "    'val_loss':'validation_loss',\n",
    "    'times':'epoch_duration',\n",
    "    'accuracy':'train_accuracy',\n",
    "    'val_accuracy':'accuracy'\n",
    "}, inplace=True)\n",
    "resnet['system'] = 'tensorflow'\n",
    "\n",
    "\n",
    "# set the columns of the \n",
    "\n",
    "# load the kubeml experiments\n",
    "kuberesnet = pd.read_pickle('./dataframes/resnet_kubeml.pkl')\n",
    "kuberesnet['model'] = 'resnet'\n",
    "kuberesnet['system'] = 'kubeml'\n",
    "\n",
    "kubelenet = pd.read_pickle('./dataframes/lenet_kubeml.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the full resnet stuff\n",
    "r.to_pickle('./dataframes/resnet.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>hash</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>epochs</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>validation_loss</th>\n",
       "      <th>epoch_duration</th>\n",
       "      <th>acc</th>\n",
       "      <th>...</th>\n",
       "      <th>dataset</th>\n",
       "      <th>lr</th>\n",
       "      <th>function_name</th>\n",
       "      <th>default_parallelism</th>\n",
       "      <th>static_parallelism</th>\n",
       "      <th>validate_every</th>\n",
       "      <th>k</th>\n",
       "      <th>goal_accuracy</th>\n",
       "      <th>parallelism</th>\n",
       "      <th>global_batch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>resnet</td>\n",
       "      <td>3b64b0be38fb8e94</td>\n",
       "      <td>32</td>\n",
       "      <td>30</td>\n",
       "      <td>[0.17032000422477722, 0.2808600068092346, 0.35...</td>\n",
       "      <td>[2.386634588241577, 1.8890058994293213, 1.7486...</td>\n",
       "      <td>[0.2615000009536743, 0.3276999890804291, 0.395...</td>\n",
       "      <td>[1.8862396478652954, 1.725894570350647, 1.6532...</td>\n",
       "      <td>[86.82871699333191, 161.10458970069885, 235.44...</td>\n",
       "      <td>0.654200</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>resnet</td>\n",
       "      <td>9848e15a8cb9456b</td>\n",
       "      <td>256</td>\n",
       "      <td>30</td>\n",
       "      <td>[0.20927999913692474, 0.35989999771118164, 0.4...</td>\n",
       "      <td>[2.9395194053649902, 1.7702909708023071, 1.570...</td>\n",
       "      <td>[0.14980000257492065, 0.17270000278949738, 0.3...</td>\n",
       "      <td>[2.747114896774292, 2.5870492458343506, 1.9928...</td>\n",
       "      <td>[26.537360668182373, 38.13520431518555, 49.665...</td>\n",
       "      <td>0.656100</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>resnet</td>\n",
       "      <td>e551b774a7b5ccc0</td>\n",
       "      <td>128</td>\n",
       "      <td>30</td>\n",
       "      <td>[0.21724000573158264, 0.33597999811172485, 0.4...</td>\n",
       "      <td>[2.6788625717163086, 1.7895755767822266, 1.611...</td>\n",
       "      <td>[0.23649999499320984, 0.2770000100135803, 0.44...</td>\n",
       "      <td>[2.0208323001861572, 1.976898431777954, 1.5212...</td>\n",
       "      <td>[33.218318939208984, 53.329391956329346, 73.47...</td>\n",
       "      <td>0.653100</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>resnet</td>\n",
       "      <td>eeaea0a56be5c6c9</td>\n",
       "      <td>64</td>\n",
       "      <td>30</td>\n",
       "      <td>[0.21930000185966492, 0.346560001373291, 0.435...</td>\n",
       "      <td>[2.442678451538086, 1.7620742321014404, 1.5449...</td>\n",
       "      <td>[0.20569999516010284, 0.38609999418258667, 0.4...</td>\n",
       "      <td>[2.4119229316711426, 1.6152931451797485, 1.422...</td>\n",
       "      <td>[51.08732867240906, 89.5283989906311, 127.9574...</td>\n",
       "      <td>0.658100</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>resnet</td>\n",
       "      <td>3b64b0be38fb8e94</td>\n",
       "      <td>32</td>\n",
       "      <td>30</td>\n",
       "      <td>[0.17776000499725342, 0.25637999176979065, 0.3...</td>\n",
       "      <td>[2.443981170654297, 1.9975420236587524, 1.8435...</td>\n",
       "      <td>[0.2281000018119812, 0.31769999861717224, 0.39...</td>\n",
       "      <td>[2.1482324600219727, 1.9316531419754028, 1.673...</td>\n",
       "      <td>[91.24218130111694, 167.12129759788513, 242.37...</td>\n",
       "      <td>0.682500</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>resnet</td>\n",
       "      <td>4cf9aff8b0e3ec9e</td>\n",
       "      <td>64</td>\n",
       "      <td>30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[3.360841138029304, 2.3875012396939574, 1.9948...</td>\n",
       "      <td>[10, 10.26, 26.85, 35.03, 40.550000000000004, ...</td>\n",
       "      <td>[3.8288797876637455, 2.403817397014351, 1.9774...</td>\n",
       "      <td>[14.801709896, 31.24002004, 48.253630979, 64.1...</td>\n",
       "      <td>72.910000</td>\n",
       "      <td>...</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>0.1</td>\n",
       "      <td>resnet</td>\n",
       "      <td>4.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>inf</td>\n",
       "      <td>100.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>256.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>resnet</td>\n",
       "      <td>9c49eeb8a706275a</td>\n",
       "      <td>64</td>\n",
       "      <td>30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[2.712390104218212, 1.6115226571822105, 1.4244...</td>\n",
       "      <td>[37.430334394904456, 46.34753184713376, 52.478...</td>\n",
       "      <td>[1.705808477037272, 1.4686595649476264, 1.3200...</td>\n",
       "      <td>[38.899608234, 82.870589755, 126.082686868, 16...</td>\n",
       "      <td>69.725318</td>\n",
       "      <td>...</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>0.1</td>\n",
       "      <td>resnet</td>\n",
       "      <td>2.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>128.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>resnet</td>\n",
       "      <td>7e945a487feff74e</td>\n",
       "      <td>128</td>\n",
       "      <td>30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[3.2209879354174644, 1.9946543223091535, 1.743...</td>\n",
       "      <td>[22.34, 33.46, 41.81, 47.64, 48.33999999999999...</td>\n",
       "      <td>[2.1471189894253695, 1.759268745591369, 1.5828...</td>\n",
       "      <td>[27.999510971, 57.792588364, 86.948315407, 115...</td>\n",
       "      <td>67.620000</td>\n",
       "      <td>...</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>0.1</td>\n",
       "      <td>resnet</td>\n",
       "      <td>4.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>512.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>resnet</td>\n",
       "      <td>afe503f94d4d8264</td>\n",
       "      <td>64</td>\n",
       "      <td>30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[3.297370414573881, 1.8065422634400665, 1.6034...</td>\n",
       "      <td>[31.22014331210191, 41.49084394904459, 42.8841...</td>\n",
       "      <td>[1.8421584147556571, 1.573136512640935, 1.5360...</td>\n",
       "      <td>[31.308027951, 61.831227029, 90.963939092, 120...</td>\n",
       "      <td>70.989252</td>\n",
       "      <td>...</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>0.1</td>\n",
       "      <td>resnet</td>\n",
       "      <td>4.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>256.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>resnet</td>\n",
       "      <td>1d0577c2b0263cbe</td>\n",
       "      <td>256</td>\n",
       "      <td>30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[3.2220769907747, 1.9234242487926871, 1.651999...</td>\n",
       "      <td>[27.470703125, 36.1328125, 41.89453125, 45.966...</td>\n",
       "      <td>[1.930098184943199, 1.718866342306137, 1.75740...</td>\n",
       "      <td>[32.766331703, 66.356944417, 101.061452874, 13...</td>\n",
       "      <td>67.402344</td>\n",
       "      <td>...</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>0.1</td>\n",
       "      <td>resnet</td>\n",
       "      <td>2.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>512.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>108 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      model              hash  batch_size  epochs  \\\n",
       "0    resnet  3b64b0be38fb8e94          32      30   \n",
       "1    resnet  9848e15a8cb9456b         256      30   \n",
       "2    resnet  e551b774a7b5ccc0         128      30   \n",
       "3    resnet  eeaea0a56be5c6c9          64      30   \n",
       "4    resnet  3b64b0be38fb8e94          32      30   \n",
       "..      ...               ...         ...     ...   \n",
       "103  resnet  4cf9aff8b0e3ec9e          64      30   \n",
       "104  resnet  9c49eeb8a706275a          64      30   \n",
       "105  resnet  7e945a487feff74e         128      30   \n",
       "106  resnet  afe503f94d4d8264          64      30   \n",
       "107  resnet  1d0577c2b0263cbe         256      30   \n",
       "\n",
       "                                        train_accuracy  \\\n",
       "0    [0.17032000422477722, 0.2808600068092346, 0.35...   \n",
       "1    [0.20927999913692474, 0.35989999771118164, 0.4...   \n",
       "2    [0.21724000573158264, 0.33597999811172485, 0.4...   \n",
       "3    [0.21930000185966492, 0.346560001373291, 0.435...   \n",
       "4    [0.17776000499725342, 0.25637999176979065, 0.3...   \n",
       "..                                                 ...   \n",
       "103                                                NaN   \n",
       "104                                                NaN   \n",
       "105                                                NaN   \n",
       "106                                                NaN   \n",
       "107                                                NaN   \n",
       "\n",
       "                                            train_loss  \\\n",
       "0    [2.386634588241577, 1.8890058994293213, 1.7486...   \n",
       "1    [2.9395194053649902, 1.7702909708023071, 1.570...   \n",
       "2    [2.6788625717163086, 1.7895755767822266, 1.611...   \n",
       "3    [2.442678451538086, 1.7620742321014404, 1.5449...   \n",
       "4    [2.443981170654297, 1.9975420236587524, 1.8435...   \n",
       "..                                                 ...   \n",
       "103  [3.360841138029304, 2.3875012396939574, 1.9948...   \n",
       "104  [2.712390104218212, 1.6115226571822105, 1.4244...   \n",
       "105  [3.2209879354174644, 1.9946543223091535, 1.743...   \n",
       "106  [3.297370414573881, 1.8065422634400665, 1.6034...   \n",
       "107  [3.2220769907747, 1.9234242487926871, 1.651999...   \n",
       "\n",
       "                                              accuracy  \\\n",
       "0    [0.2615000009536743, 0.3276999890804291, 0.395...   \n",
       "1    [0.14980000257492065, 0.17270000278949738, 0.3...   \n",
       "2    [0.23649999499320984, 0.2770000100135803, 0.44...   \n",
       "3    [0.20569999516010284, 0.38609999418258667, 0.4...   \n",
       "4    [0.2281000018119812, 0.31769999861717224, 0.39...   \n",
       "..                                                 ...   \n",
       "103  [10, 10.26, 26.85, 35.03, 40.550000000000004, ...   \n",
       "104  [37.430334394904456, 46.34753184713376, 52.478...   \n",
       "105  [22.34, 33.46, 41.81, 47.64, 48.33999999999999...   \n",
       "106  [31.22014331210191, 41.49084394904459, 42.8841...   \n",
       "107  [27.470703125, 36.1328125, 41.89453125, 45.966...   \n",
       "\n",
       "                                       validation_loss  \\\n",
       "0    [1.8862396478652954, 1.725894570350647, 1.6532...   \n",
       "1    [2.747114896774292, 2.5870492458343506, 1.9928...   \n",
       "2    [2.0208323001861572, 1.976898431777954, 1.5212...   \n",
       "3    [2.4119229316711426, 1.6152931451797485, 1.422...   \n",
       "4    [2.1482324600219727, 1.9316531419754028, 1.673...   \n",
       "..                                                 ...   \n",
       "103  [3.8288797876637455, 2.403817397014351, 1.9774...   \n",
       "104  [1.705808477037272, 1.4686595649476264, 1.3200...   \n",
       "105  [2.1471189894253695, 1.759268745591369, 1.5828...   \n",
       "106  [1.8421584147556571, 1.573136512640935, 1.5360...   \n",
       "107  [1.930098184943199, 1.718866342306137, 1.75740...   \n",
       "\n",
       "                                        epoch_duration        acc  ...  \\\n",
       "0    [86.82871699333191, 161.10458970069885, 235.44...   0.654200  ...   \n",
       "1    [26.537360668182373, 38.13520431518555, 49.665...   0.656100  ...   \n",
       "2    [33.218318939208984, 53.329391956329346, 73.47...   0.653100  ...   \n",
       "3    [51.08732867240906, 89.5283989906311, 127.9574...   0.658100  ...   \n",
       "4    [91.24218130111694, 167.12129759788513, 242.37...   0.682500  ...   \n",
       "..                                                 ...        ...  ...   \n",
       "103  [14.801709896, 31.24002004, 48.253630979, 64.1...  72.910000  ...   \n",
       "104  [38.899608234, 82.870589755, 126.082686868, 16...  69.725318  ...   \n",
       "105  [27.999510971, 57.792588364, 86.948315407, 115...  67.620000  ...   \n",
       "106  [31.308027951, 61.831227029, 90.963939092, 120...  70.989252  ...   \n",
       "107  [32.766331703, 66.356944417, 101.061452874, 13...  67.402344  ...   \n",
       "\n",
       "     dataset   lr  function_name default_parallelism static_parallelism  \\\n",
       "0        NaN  NaN            NaN                 NaN                NaN   \n",
       "1        NaN  NaN            NaN                 NaN                NaN   \n",
       "2        NaN  NaN            NaN                 NaN                NaN   \n",
       "3        NaN  NaN            NaN                 NaN                NaN   \n",
       "4        NaN  NaN            NaN                 NaN                NaN   \n",
       "..       ...  ...            ...                 ...                ...   \n",
       "103  cifar10  0.1         resnet                 4.0               True   \n",
       "104  cifar10  0.1         resnet                 2.0               True   \n",
       "105  cifar10  0.1         resnet                 4.0               True   \n",
       "106  cifar10  0.1         resnet                 4.0               True   \n",
       "107  cifar10  0.1         resnet                 2.0               True   \n",
       "\n",
       "    validate_every     k goal_accuracy  parallelism  global_batch  \n",
       "0              NaN   NaN           NaN          NaN           NaN  \n",
       "1              NaN   NaN           NaN          NaN           NaN  \n",
       "2              NaN   NaN           NaN          NaN           NaN  \n",
       "3              NaN   NaN           NaN          NaN           NaN  \n",
       "4              NaN   NaN           NaN          NaN           NaN  \n",
       "..             ...   ...           ...          ...           ...  \n",
       "103            1.0   inf         100.0          4.0         256.0  \n",
       "104            1.0  32.0         100.0          2.0         128.0  \n",
       "105            1.0  16.0         100.0          4.0         512.0  \n",
       "106            1.0  32.0         100.0          4.0         256.0  \n",
       "107            1.0   8.0         100.0          2.0         512.0  \n",
       "\n",
       "[108 rows x 40 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = pd.concat([resnet, kuberesnet], ignore_index=True)\n",
    "r\n",
    "# resnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Create new columns for representation\n",
    "\n",
    "- Final accuracy\n",
    "- Total time taken\n",
    "- Parallelism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Plot the Correlations between the K, Batch and Parallelism with time and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corr = df[['k', 'batch_size', 'parallelism', 'acc', 'time']].corr()\n",
    "sns.heatmap(corr,\n",
    "            annot=True,\n",
    ")\n",
    "\n",
    "# plt.savefig('./figures/resnet34/heat.png', dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df.sort_values('time')\n",
    "\n",
    "mean = df.groupby('hash').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Get the max accuracies and times and check the parameters used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# get the max accuracies\n",
    "df[['k', 'parallelism', 'acc','batch_size','time']].sort_values(by='time', ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Calculate TTA with different accuracies\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "s = df.sort_values('tta_cross_99')[['k', 'batch_size', 'parallelism', 'tta_cross_99', 'tta_99', 'acc', 'accuracy', 'epoch_duration']]\n",
    "\n",
    "# plot the best\n",
    "best = s.iloc[0]\n",
    "best\n",
    "\n",
    "\n",
    "x = range(1, len(best.accuracy)+1)\n",
    "plt.figure()\n",
    "plt.title(f'Best tta_99 (B={best.batch_size}, k={best.k}, P={best.parallelism})')\n",
    "sns.lineplot(x=best.epoch_duration, y = best.accuracy)\n",
    "sns.lineplot(x=best.epoch_duration, y= 99)\n",
    "plt.scatter(best.tta_cross_99, 99, marker='X', s=60, c='r')\n",
    "plt.xlabel('Time (s)', fontsize=15)\n",
    "plt.ylabel('Accuracy (%)', fontsize=15)\n",
    "\n",
    "# plt.savefig('./figures/gpu/best.png', dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>epochs</th>\n",
       "      <th>acc</th>\n",
       "      <th>time</th>\n",
       "      <th>tta_67</th>\n",
       "      <th>tta_cross_67</th>\n",
       "      <th>cpu_mean</th>\n",
       "      <th>gpu_0_mean_usage</th>\n",
       "      <th>gpu_1_mean_usage</th>\n",
       "      <th>gpu_0_mean_memory</th>\n",
       "      <th>gpu_1_mean_memory</th>\n",
       "      <th>gpu_usage</th>\n",
       "      <th>mem_mean</th>\n",
       "      <th>tta_69</th>\n",
       "      <th>tta_cross_69</th>\n",
       "      <th>lr</th>\n",
       "      <th>default_parallelism</th>\n",
       "      <th>validate_every</th>\n",
       "      <th>goal_accuracy</th>\n",
       "      <th>global_batch</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>batch_size</th>\n",
       "      <th>system</th>\n",
       "      <th>k</th>\n",
       "      <th>parallelism</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">32</th>\n",
       "      <th rowspan=\"8\" valign=\"top\">kubeml</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">8.0</th>\n",
       "      <th>2.0</th>\n",
       "      <td>30</td>\n",
       "      <td>68.947183</td>\n",
       "      <td>5923.931139</td>\n",
       "      <td>2881.126330</td>\n",
       "      <td>3473.112642</td>\n",
       "      <td>4.239913</td>\n",
       "      <td>0.115340</td>\n",
       "      <td>0.049599</td>\n",
       "      <td>2020.842444</td>\n",
       "      <td>872.712403</td>\n",
       "      <td>0.082470</td>\n",
       "      <td>7.716206</td>\n",
       "      <td>3876.004124</td>\n",
       "      <td>4367.454878</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>30</td>\n",
       "      <td>69.346150</td>\n",
       "      <td>4517.181873</td>\n",
       "      <td>1858.362935</td>\n",
       "      <td>2213.384787</td>\n",
       "      <td>4.947685</td>\n",
       "      <td>0.127413</td>\n",
       "      <td>0.122867</td>\n",
       "      <td>2759.478114</td>\n",
       "      <td>2984.310141</td>\n",
       "      <td>0.125140</td>\n",
       "      <td>9.476524</td>\n",
       "      <td>3602.101572</td>\n",
       "      <td>4051.582116</td>\n",
       "      <td>0.1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>128.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">16.0</th>\n",
       "      <th>2.0</th>\n",
       "      <td>30</td>\n",
       "      <td>67.657583</td>\n",
       "      <td>3400.371185</td>\n",
       "      <td>2417.470112</td>\n",
       "      <td>3057.576740</td>\n",
       "      <td>4.439632</td>\n",
       "      <td>0.189830</td>\n",
       "      <td>0.075637</td>\n",
       "      <td>2123.003923</td>\n",
       "      <td>857.614829</td>\n",
       "      <td>0.132734</td>\n",
       "      <td>7.693127</td>\n",
       "      <td>3153.281437</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>30</td>\n",
       "      <td>69.545948</td>\n",
       "      <td>2539.809555</td>\n",
       "      <td>1304.888241</td>\n",
       "      <td>1500.779184</td>\n",
       "      <td>5.617721</td>\n",
       "      <td>0.195410</td>\n",
       "      <td>0.192263</td>\n",
       "      <td>2933.913220</td>\n",
       "      <td>2941.718915</td>\n",
       "      <td>0.193836</td>\n",
       "      <td>9.727973</td>\n",
       "      <td>1953.288993</td>\n",
       "      <td>2058.124568</td>\n",
       "      <td>0.1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>128.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">32.0</th>\n",
       "      <th>2.0</th>\n",
       "      <td>30</td>\n",
       "      <td>69.352577</td>\n",
       "      <td>2161.323538</td>\n",
       "      <td>1058.124722</td>\n",
       "      <td>1275.394572</td>\n",
       "      <td>4.680143</td>\n",
       "      <td>0.294220</td>\n",
       "      <td>0.122141</td>\n",
       "      <td>2197.647307</td>\n",
       "      <td>890.759207</td>\n",
       "      <td>0.208180</td>\n",
       "      <td>7.687835</td>\n",
       "      <td>1593.550952</td>\n",
       "      <td>1810.827501</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>30</td>\n",
       "      <td>70.278312</td>\n",
       "      <td>1559.924027</td>\n",
       "      <td>708.630220</td>\n",
       "      <td>811.804426</td>\n",
       "      <td>6.270719</td>\n",
       "      <td>0.272724</td>\n",
       "      <td>0.303754</td>\n",
       "      <td>2912.313762</td>\n",
       "      <td>3147.511631</td>\n",
       "      <td>0.288239</td>\n",
       "      <td>9.791883</td>\n",
       "      <td>898.440860</td>\n",
       "      <td>1106.220076</td>\n",
       "      <td>0.1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>128.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">inf</th>\n",
       "      <th>2.0</th>\n",
       "      <td>30</td>\n",
       "      <td>73.212162</td>\n",
       "      <td>938.597728</td>\n",
       "      <td>383.010922</td>\n",
       "      <td>447.715970</td>\n",
       "      <td>5.705479</td>\n",
       "      <td>0.533680</td>\n",
       "      <td>0.306043</td>\n",
       "      <td>2246.284037</td>\n",
       "      <td>1277.606045</td>\n",
       "      <td>0.419861</td>\n",
       "      <td>7.908037</td>\n",
       "      <td>439.927944</td>\n",
       "      <td>515.473148</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>30</td>\n",
       "      <td>72.772508</td>\n",
       "      <td>660.344917</td>\n",
       "      <td>332.606673</td>\n",
       "      <td>379.706467</td>\n",
       "      <td>8.940485</td>\n",
       "      <td>0.632848</td>\n",
       "      <td>0.655991</td>\n",
       "      <td>3111.118672</td>\n",
       "      <td>3394.279023</td>\n",
       "      <td>0.644419</td>\n",
       "      <td>9.898967</td>\n",
       "      <td>386.470779</td>\n",
       "      <td>433.742163</td>\n",
       "      <td>0.1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>128.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">64</th>\n",
       "      <th rowspan=\"8\" valign=\"top\">kubeml</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">8.0</th>\n",
       "      <th>2.0</th>\n",
       "      <td>30</td>\n",
       "      <td>67.786502</td>\n",
       "      <td>3034.596671</td>\n",
       "      <td>2230.130607</td>\n",
       "      <td>2400.985173</td>\n",
       "      <td>4.170849</td>\n",
       "      <td>0.137543</td>\n",
       "      <td>0.052793</td>\n",
       "      <td>2192.059532</td>\n",
       "      <td>846.601945</td>\n",
       "      <td>0.095168</td>\n",
       "      <td>7.421509</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>128.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>30</td>\n",
       "      <td>68.512436</td>\n",
       "      <td>2372.274145</td>\n",
       "      <td>1278.153022</td>\n",
       "      <td>1461.038770</td>\n",
       "      <td>5.173503</td>\n",
       "      <td>0.134033</td>\n",
       "      <td>0.132637</td>\n",
       "      <td>2958.136559</td>\n",
       "      <td>3032.040704</td>\n",
       "      <td>0.133335</td>\n",
       "      <td>9.450940</td>\n",
       "      <td>1950.790920</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>256.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">16.0</th>\n",
       "      <th>2.0</th>\n",
       "      <td>30</td>\n",
       "      <td>69.796927</td>\n",
       "      <td>1873.111526</td>\n",
       "      <td>817.123322</td>\n",
       "      <td>1040.604199</td>\n",
       "      <td>4.533505</td>\n",
       "      <td>0.238244</td>\n",
       "      <td>0.055869</td>\n",
       "      <td>2548.438027</td>\n",
       "      <td>636.766035</td>\n",
       "      <td>0.147056</td>\n",
       "      <td>7.436015</td>\n",
       "      <td>1275.153679</td>\n",
       "      <td>1672.664227</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>128.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>30</td>\n",
       "      <td>68.894145</td>\n",
       "      <td>1379.382412</td>\n",
       "      <td>602.794919</td>\n",
       "      <td>802.239245</td>\n",
       "      <td>5.846716</td>\n",
       "      <td>0.207637</td>\n",
       "      <td>0.216608</td>\n",
       "      <td>2962.403526</td>\n",
       "      <td>3207.008330</td>\n",
       "      <td>0.212122</td>\n",
       "      <td>9.430619</td>\n",
       "      <td>1180.843523</td>\n",
       "      <td>1271.619655</td>\n",
       "      <td>0.1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>256.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">32.0</th>\n",
       "      <th>2.0</th>\n",
       "      <td>30</td>\n",
       "      <td>70.060706</td>\n",
       "      <td>1231.454501</td>\n",
       "      <td>528.128708</td>\n",
       "      <td>648.922469</td>\n",
       "      <td>4.746956</td>\n",
       "      <td>0.357685</td>\n",
       "      <td>0.089745</td>\n",
       "      <td>2662.525401</td>\n",
       "      <td>703.714088</td>\n",
       "      <td>0.223715</td>\n",
       "      <td>7.472947</td>\n",
       "      <td>831.112836</td>\n",
       "      <td>966.310691</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>128.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>30</td>\n",
       "      <td>70.428907</td>\n",
       "      <td>912.207939</td>\n",
       "      <td>435.278214</td>\n",
       "      <td>516.029984</td>\n",
       "      <td>6.762504</td>\n",
       "      <td>0.330906</td>\n",
       "      <td>0.275067</td>\n",
       "      <td>3515.265800</td>\n",
       "      <td>2930.299543</td>\n",
       "      <td>0.302986</td>\n",
       "      <td>9.643527</td>\n",
       "      <td>627.839136</td>\n",
       "      <td>708.969207</td>\n",
       "      <td>0.1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>256.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">inf</th>\n",
       "      <th>2.0</th>\n",
       "      <td>30</td>\n",
       "      <td>72.306380</td>\n",
       "      <td>618.092809</td>\n",
       "      <td>205.511294</td>\n",
       "      <td>252.242654</td>\n",
       "      <td>6.132659</td>\n",
       "      <td>0.457171</td>\n",
       "      <td>0.320025</td>\n",
       "      <td>2446.362674</td>\n",
       "      <td>1589.327154</td>\n",
       "      <td>0.388598</td>\n",
       "      <td>7.891078</td>\n",
       "      <td>265.603037</td>\n",
       "      <td>313.737996</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>128.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>30</td>\n",
       "      <td>72.969735</td>\n",
       "      <td>476.694337</td>\n",
       "      <td>234.962366</td>\n",
       "      <td>277.439108</td>\n",
       "      <td>9.617389</td>\n",
       "      <td>0.592493</td>\n",
       "      <td>0.507645</td>\n",
       "      <td>3901.034406</td>\n",
       "      <td>3300.002579</td>\n",
       "      <td>0.550069</td>\n",
       "      <td>10.091339</td>\n",
       "      <td>276.618464</td>\n",
       "      <td>309.842162</td>\n",
       "      <td>0.1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>256.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">128</th>\n",
       "      <th rowspan=\"8\" valign=\"top\">kubeml</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">8.0</th>\n",
       "      <th>2.0</th>\n",
       "      <td>30</td>\n",
       "      <td>67.949193</td>\n",
       "      <td>1658.000223</td>\n",
       "      <td>1028.047383</td>\n",
       "      <td>1361.420528</td>\n",
       "      <td>4.311303</td>\n",
       "      <td>0.155397</td>\n",
       "      <td>0.070899</td>\n",
       "      <td>2281.587145</td>\n",
       "      <td>1005.936492</td>\n",
       "      <td>0.113148</td>\n",
       "      <td>7.207255</td>\n",
       "      <td>1516.159982</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>256.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>30</td>\n",
       "      <td>67.543855</td>\n",
       "      <td>1329.524896</td>\n",
       "      <td>860.081136</td>\n",
       "      <td>1098.509059</td>\n",
       "      <td>6.079331</td>\n",
       "      <td>0.163253</td>\n",
       "      <td>0.153101</td>\n",
       "      <td>3348.248029</td>\n",
       "      <td>3145.426649</td>\n",
       "      <td>0.158177</td>\n",
       "      <td>9.352070</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>512.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">16.0</th>\n",
       "      <th>2.0</th>\n",
       "      <td>30</td>\n",
       "      <td>67.847199</td>\n",
       "      <td>1043.494251</td>\n",
       "      <td>742.621417</td>\n",
       "      <td>857.136080</td>\n",
       "      <td>4.663297</td>\n",
       "      <td>0.226404</td>\n",
       "      <td>0.113605</td>\n",
       "      <td>2402.018305</td>\n",
       "      <td>1137.370070</td>\n",
       "      <td>0.170005</td>\n",
       "      <td>7.307912</td>\n",
       "      <td>1036.021413</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>256.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>30</td>\n",
       "      <td>67.756904</td>\n",
       "      <td>826.509706</td>\n",
       "      <td>509.003978</td>\n",
       "      <td>653.674440</td>\n",
       "      <td>6.729271</td>\n",
       "      <td>0.249223</td>\n",
       "      <td>0.232237</td>\n",
       "      <td>3476.554553</td>\n",
       "      <td>3250.476954</td>\n",
       "      <td>0.240730</td>\n",
       "      <td>9.456752</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>512.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">32.0</th>\n",
       "      <th>2.0</th>\n",
       "      <td>30</td>\n",
       "      <td>68.823555</td>\n",
       "      <td>746.185348</td>\n",
       "      <td>434.428547</td>\n",
       "      <td>511.497045</td>\n",
       "      <td>5.019027</td>\n",
       "      <td>0.320070</td>\n",
       "      <td>0.135025</td>\n",
       "      <td>2722.828177</td>\n",
       "      <td>1065.434050</td>\n",
       "      <td>0.227548</td>\n",
       "      <td>7.387377</td>\n",
       "      <td>656.673349</td>\n",
       "      <td>733.743227</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>256.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>30</td>\n",
       "      <td>69.195079</td>\n",
       "      <td>603.920821</td>\n",
       "      <td>377.046772</td>\n",
       "      <td>416.870063</td>\n",
       "      <td>8.140851</td>\n",
       "      <td>0.314253</td>\n",
       "      <td>0.304274</td>\n",
       "      <td>3813.755865</td>\n",
       "      <td>3368.512131</td>\n",
       "      <td>0.309263</td>\n",
       "      <td>9.667072</td>\n",
       "      <td>523.827260</td>\n",
       "      <td>460.195127</td>\n",
       "      <td>0.1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>512.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">inf</th>\n",
       "      <th>2.0</th>\n",
       "      <td>30</td>\n",
       "      <td>70.696820</td>\n",
       "      <td>472.494852</td>\n",
       "      <td>175.715730</td>\n",
       "      <td>210.655472</td>\n",
       "      <td>5.998552</td>\n",
       "      <td>0.449263</td>\n",
       "      <td>0.276949</td>\n",
       "      <td>2806.799689</td>\n",
       "      <td>1604.278067</td>\n",
       "      <td>0.363106</td>\n",
       "      <td>7.737450</td>\n",
       "      <td>226.690396</td>\n",
       "      <td>267.942236</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>256.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>30</td>\n",
       "      <td>70.985158</td>\n",
       "      <td>325.926995</td>\n",
       "      <td>160.017986</td>\n",
       "      <td>181.691177</td>\n",
       "      <td>7.949603</td>\n",
       "      <td>0.554898</td>\n",
       "      <td>0.515668</td>\n",
       "      <td>3680.036927</td>\n",
       "      <td>3597.994239</td>\n",
       "      <td>0.535283</td>\n",
       "      <td>9.645527</td>\n",
       "      <td>187.670014</td>\n",
       "      <td>213.627789</td>\n",
       "      <td>0.1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>512.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">256</th>\n",
       "      <th rowspan=\"8\" valign=\"top\">kubeml</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">8.0</th>\n",
       "      <th>2.0</th>\n",
       "      <td>30</td>\n",
       "      <td>66.161797</td>\n",
       "      <td>1010.378293</td>\n",
       "      <td>903.922612</td>\n",
       "      <td>864.349527</td>\n",
       "      <td>4.483658</td>\n",
       "      <td>0.228119</td>\n",
       "      <td>0.070148</td>\n",
       "      <td>2933.070017</td>\n",
       "      <td>902.345289</td>\n",
       "      <td>0.149134</td>\n",
       "      <td>7.051384</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>512.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>30</td>\n",
       "      <td>64.550573</td>\n",
       "      <td>769.532170</td>\n",
       "      <td>780.824623</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.972017</td>\n",
       "      <td>0.208901</td>\n",
       "      <td>0.210933</td>\n",
       "      <td>3606.710192</td>\n",
       "      <td>3602.926345</td>\n",
       "      <td>0.209917</td>\n",
       "      <td>9.078223</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1024.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">16.0</th>\n",
       "      <th>2.0</th>\n",
       "      <td>30</td>\n",
       "      <td>66.510781</td>\n",
       "      <td>707.085116</td>\n",
       "      <td>514.809549</td>\n",
       "      <td>565.060399</td>\n",
       "      <td>4.733030</td>\n",
       "      <td>0.317595</td>\n",
       "      <td>0.095977</td>\n",
       "      <td>3197.629951</td>\n",
       "      <td>933.871487</td>\n",
       "      <td>0.206786</td>\n",
       "      <td>7.158824</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>512.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>30</td>\n",
       "      <td>65.775781</td>\n",
       "      <td>555.409487</td>\n",
       "      <td>426.252361</td>\n",
       "      <td>462.804594</td>\n",
       "      <td>7.712256</td>\n",
       "      <td>0.280692</td>\n",
       "      <td>0.295920</td>\n",
       "      <td>3899.665330</td>\n",
       "      <td>3977.750964</td>\n",
       "      <td>0.288306</td>\n",
       "      <td>9.492042</td>\n",
       "      <td>554.789812</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1024.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">32.0</th>\n",
       "      <th>2.0</th>\n",
       "      <td>30</td>\n",
       "      <td>65.296823</td>\n",
       "      <td>545.244145</td>\n",
       "      <td>358.938648</td>\n",
       "      <td>395.915646</td>\n",
       "      <td>5.045047</td>\n",
       "      <td>0.378432</td>\n",
       "      <td>0.154696</td>\n",
       "      <td>3274.180918</td>\n",
       "      <td>1135.677816</td>\n",
       "      <td>0.266564</td>\n",
       "      <td>7.299008</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>512.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>30</td>\n",
       "      <td>65.216875</td>\n",
       "      <td>381.095069</td>\n",
       "      <td>281.825391</td>\n",
       "      <td>357.378689</td>\n",
       "      <td>8.388168</td>\n",
       "      <td>0.397180</td>\n",
       "      <td>0.405915</td>\n",
       "      <td>4410.991196</td>\n",
       "      <td>3992.619670</td>\n",
       "      <td>0.401548</td>\n",
       "      <td>9.676466</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1024.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">inf</th>\n",
       "      <th>2.0</th>\n",
       "      <td>30</td>\n",
       "      <td>69.560859</td>\n",
       "      <td>408.100123</td>\n",
       "      <td>173.713432</td>\n",
       "      <td>201.738692</td>\n",
       "      <td>5.769889</td>\n",
       "      <td>0.440641</td>\n",
       "      <td>0.239986</td>\n",
       "      <td>3189.348040</td>\n",
       "      <td>1514.426000</td>\n",
       "      <td>0.340313</td>\n",
       "      <td>7.437468</td>\n",
       "      <td>262.852074</td>\n",
       "      <td>299.517968</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>512.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>30</td>\n",
       "      <td>66.348984</td>\n",
       "      <td>276.230160</td>\n",
       "      <td>235.753829</td>\n",
       "      <td>263.027773</td>\n",
       "      <td>6.885217</td>\n",
       "      <td>0.497205</td>\n",
       "      <td>0.556087</td>\n",
       "      <td>3724.766053</td>\n",
       "      <td>4022.205104</td>\n",
       "      <td>0.526646</td>\n",
       "      <td>9.063102</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1024.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    epochs        acc         time  \\\n",
       "batch_size system k    parallelism                                   \n",
       "32         kubeml 8.0  2.0              30  68.947183  5923.931139   \n",
       "                       4.0              30  69.346150  4517.181873   \n",
       "                  16.0 2.0              30  67.657583  3400.371185   \n",
       "                       4.0              30  69.545948  2539.809555   \n",
       "                  32.0 2.0              30  69.352577  2161.323538   \n",
       "                       4.0              30  70.278312  1559.924027   \n",
       "                  inf  2.0              30  73.212162   938.597728   \n",
       "                       4.0              30  72.772508   660.344917   \n",
       "64         kubeml 8.0  2.0              30  67.786502  3034.596671   \n",
       "                       4.0              30  68.512436  2372.274145   \n",
       "                  16.0 2.0              30  69.796927  1873.111526   \n",
       "                       4.0              30  68.894145  1379.382412   \n",
       "                  32.0 2.0              30  70.060706  1231.454501   \n",
       "                       4.0              30  70.428907   912.207939   \n",
       "                  inf  2.0              30  72.306380   618.092809   \n",
       "                       4.0              30  72.969735   476.694337   \n",
       "128        kubeml 8.0  2.0              30  67.949193  1658.000223   \n",
       "                       4.0              30  67.543855  1329.524896   \n",
       "                  16.0 2.0              30  67.847199  1043.494251   \n",
       "                       4.0              30  67.756904   826.509706   \n",
       "                  32.0 2.0              30  68.823555   746.185348   \n",
       "                       4.0              30  69.195079   603.920821   \n",
       "                  inf  2.0              30  70.696820   472.494852   \n",
       "                       4.0              30  70.985158   325.926995   \n",
       "256        kubeml 8.0  2.0              30  66.161797  1010.378293   \n",
       "                       4.0              30  64.550573   769.532170   \n",
       "                  16.0 2.0              30  66.510781   707.085116   \n",
       "                       4.0              30  65.775781   555.409487   \n",
       "                  32.0 2.0              30  65.296823   545.244145   \n",
       "                       4.0              30  65.216875   381.095069   \n",
       "                  inf  2.0              30  69.560859   408.100123   \n",
       "                       4.0              30  66.348984   276.230160   \n",
       "\n",
       "                                         tta_67  tta_cross_67  cpu_mean  \\\n",
       "batch_size system k    parallelism                                        \n",
       "32         kubeml 8.0  2.0          2881.126330   3473.112642  4.239913   \n",
       "                       4.0          1858.362935   2213.384787  4.947685   \n",
       "                  16.0 2.0          2417.470112   3057.576740  4.439632   \n",
       "                       4.0          1304.888241   1500.779184  5.617721   \n",
       "                  32.0 2.0          1058.124722   1275.394572  4.680143   \n",
       "                       4.0           708.630220    811.804426  6.270719   \n",
       "                  inf  2.0           383.010922    447.715970  5.705479   \n",
       "                       4.0           332.606673    379.706467  8.940485   \n",
       "64         kubeml 8.0  2.0          2230.130607   2400.985173  4.170849   \n",
       "                       4.0          1278.153022   1461.038770  5.173503   \n",
       "                  16.0 2.0           817.123322   1040.604199  4.533505   \n",
       "                       4.0           602.794919    802.239245  5.846716   \n",
       "                  32.0 2.0           528.128708    648.922469  4.746956   \n",
       "                       4.0           435.278214    516.029984  6.762504   \n",
       "                  inf  2.0           205.511294    252.242654  6.132659   \n",
       "                       4.0           234.962366    277.439108  9.617389   \n",
       "128        kubeml 8.0  2.0          1028.047383   1361.420528  4.311303   \n",
       "                       4.0           860.081136   1098.509059  6.079331   \n",
       "                  16.0 2.0           742.621417    857.136080  4.663297   \n",
       "                       4.0           509.003978    653.674440  6.729271   \n",
       "                  32.0 2.0           434.428547    511.497045  5.019027   \n",
       "                       4.0           377.046772    416.870063  8.140851   \n",
       "                  inf  2.0           175.715730    210.655472  5.998552   \n",
       "                       4.0           160.017986    181.691177  7.949603   \n",
       "256        kubeml 8.0  2.0           903.922612    864.349527  4.483658   \n",
       "                       4.0           780.824623           NaN  5.972017   \n",
       "                  16.0 2.0           514.809549    565.060399  4.733030   \n",
       "                       4.0           426.252361    462.804594  7.712256   \n",
       "                  32.0 2.0           358.938648    395.915646  5.045047   \n",
       "                       4.0           281.825391    357.378689  8.388168   \n",
       "                  inf  2.0           173.713432    201.738692  5.769889   \n",
       "                       4.0           235.753829    263.027773  6.885217   \n",
       "\n",
       "                                    gpu_0_mean_usage  gpu_1_mean_usage  \\\n",
       "batch_size system k    parallelism                                       \n",
       "32         kubeml 8.0  2.0                  0.115340          0.049599   \n",
       "                       4.0                  0.127413          0.122867   \n",
       "                  16.0 2.0                  0.189830          0.075637   \n",
       "                       4.0                  0.195410          0.192263   \n",
       "                  32.0 2.0                  0.294220          0.122141   \n",
       "                       4.0                  0.272724          0.303754   \n",
       "                  inf  2.0                  0.533680          0.306043   \n",
       "                       4.0                  0.632848          0.655991   \n",
       "64         kubeml 8.0  2.0                  0.137543          0.052793   \n",
       "                       4.0                  0.134033          0.132637   \n",
       "                  16.0 2.0                  0.238244          0.055869   \n",
       "                       4.0                  0.207637          0.216608   \n",
       "                  32.0 2.0                  0.357685          0.089745   \n",
       "                       4.0                  0.330906          0.275067   \n",
       "                  inf  2.0                  0.457171          0.320025   \n",
       "                       4.0                  0.592493          0.507645   \n",
       "128        kubeml 8.0  2.0                  0.155397          0.070899   \n",
       "                       4.0                  0.163253          0.153101   \n",
       "                  16.0 2.0                  0.226404          0.113605   \n",
       "                       4.0                  0.249223          0.232237   \n",
       "                  32.0 2.0                  0.320070          0.135025   \n",
       "                       4.0                  0.314253          0.304274   \n",
       "                  inf  2.0                  0.449263          0.276949   \n",
       "                       4.0                  0.554898          0.515668   \n",
       "256        kubeml 8.0  2.0                  0.228119          0.070148   \n",
       "                       4.0                  0.208901          0.210933   \n",
       "                  16.0 2.0                  0.317595          0.095977   \n",
       "                       4.0                  0.280692          0.295920   \n",
       "                  32.0 2.0                  0.378432          0.154696   \n",
       "                       4.0                  0.397180          0.405915   \n",
       "                  inf  2.0                  0.440641          0.239986   \n",
       "                       4.0                  0.497205          0.556087   \n",
       "\n",
       "                                    gpu_0_mean_memory  gpu_1_mean_memory  \\\n",
       "batch_size system k    parallelism                                         \n",
       "32         kubeml 8.0  2.0                2020.842444         872.712403   \n",
       "                       4.0                2759.478114        2984.310141   \n",
       "                  16.0 2.0                2123.003923         857.614829   \n",
       "                       4.0                2933.913220        2941.718915   \n",
       "                  32.0 2.0                2197.647307         890.759207   \n",
       "                       4.0                2912.313762        3147.511631   \n",
       "                  inf  2.0                2246.284037        1277.606045   \n",
       "                       4.0                3111.118672        3394.279023   \n",
       "64         kubeml 8.0  2.0                2192.059532         846.601945   \n",
       "                       4.0                2958.136559        3032.040704   \n",
       "                  16.0 2.0                2548.438027         636.766035   \n",
       "                       4.0                2962.403526        3207.008330   \n",
       "                  32.0 2.0                2662.525401         703.714088   \n",
       "                       4.0                3515.265800        2930.299543   \n",
       "                  inf  2.0                2446.362674        1589.327154   \n",
       "                       4.0                3901.034406        3300.002579   \n",
       "128        kubeml 8.0  2.0                2281.587145        1005.936492   \n",
       "                       4.0                3348.248029        3145.426649   \n",
       "                  16.0 2.0                2402.018305        1137.370070   \n",
       "                       4.0                3476.554553        3250.476954   \n",
       "                  32.0 2.0                2722.828177        1065.434050   \n",
       "                       4.0                3813.755865        3368.512131   \n",
       "                  inf  2.0                2806.799689        1604.278067   \n",
       "                       4.0                3680.036927        3597.994239   \n",
       "256        kubeml 8.0  2.0                2933.070017         902.345289   \n",
       "                       4.0                3606.710192        3602.926345   \n",
       "                  16.0 2.0                3197.629951         933.871487   \n",
       "                       4.0                3899.665330        3977.750964   \n",
       "                  32.0 2.0                3274.180918        1135.677816   \n",
       "                       4.0                4410.991196        3992.619670   \n",
       "                  inf  2.0                3189.348040        1514.426000   \n",
       "                       4.0                3724.766053        4022.205104   \n",
       "\n",
       "                                    gpu_usage   mem_mean       tta_69  \\\n",
       "batch_size system k    parallelism                                      \n",
       "32         kubeml 8.0  2.0           0.082470   7.716206  3876.004124   \n",
       "                       4.0           0.125140   9.476524  3602.101572   \n",
       "                  16.0 2.0           0.132734   7.693127  3153.281437   \n",
       "                       4.0           0.193836   9.727973  1953.288993   \n",
       "                  32.0 2.0           0.208180   7.687835  1593.550952   \n",
       "                       4.0           0.288239   9.791883   898.440860   \n",
       "                  inf  2.0           0.419861   7.908037   439.927944   \n",
       "                       4.0           0.644419   9.898967   386.470779   \n",
       "64         kubeml 8.0  2.0           0.095168   7.421509          NaN   \n",
       "                       4.0           0.133335   9.450940  1950.790920   \n",
       "                  16.0 2.0           0.147056   7.436015  1275.153679   \n",
       "                       4.0           0.212122   9.430619  1180.843523   \n",
       "                  32.0 2.0           0.223715   7.472947   831.112836   \n",
       "                       4.0           0.302986   9.643527   627.839136   \n",
       "                  inf  2.0           0.388598   7.891078   265.603037   \n",
       "                       4.0           0.550069  10.091339   276.618464   \n",
       "128        kubeml 8.0  2.0           0.113148   7.207255  1516.159982   \n",
       "                       4.0           0.158177   9.352070          NaN   \n",
       "                  16.0 2.0           0.170005   7.307912  1036.021413   \n",
       "                       4.0           0.240730   9.456752          NaN   \n",
       "                  32.0 2.0           0.227548   7.387377   656.673349   \n",
       "                       4.0           0.309263   9.667072   523.827260   \n",
       "                  inf  2.0           0.363106   7.737450   226.690396   \n",
       "                       4.0           0.535283   9.645527   187.670014   \n",
       "256        kubeml 8.0  2.0           0.149134   7.051384          NaN   \n",
       "                       4.0           0.209917   9.078223          NaN   \n",
       "                  16.0 2.0           0.206786   7.158824          NaN   \n",
       "                       4.0           0.288306   9.492042   554.789812   \n",
       "                  32.0 2.0           0.266564   7.299008          NaN   \n",
       "                       4.0           0.401548   9.676466          NaN   \n",
       "                  inf  2.0           0.340313   7.437468   262.852074   \n",
       "                       4.0           0.526646   9.063102          NaN   \n",
       "\n",
       "                                    tta_cross_69   lr  default_parallelism  \\\n",
       "batch_size system k    parallelism                                           \n",
       "32         kubeml 8.0  2.0           4367.454878  0.1                  2.0   \n",
       "                       4.0           4051.582116  0.1                  4.0   \n",
       "                  16.0 2.0                   NaN  0.1                  2.0   \n",
       "                       4.0           2058.124568  0.1                  4.0   \n",
       "                  32.0 2.0           1810.827501  0.1                  2.0   \n",
       "                       4.0           1106.220076  0.1                  4.0   \n",
       "                  inf  2.0            515.473148  0.1                  2.0   \n",
       "                       4.0            433.742163  0.1                  4.0   \n",
       "64         kubeml 8.0  2.0                   NaN  0.1                  2.0   \n",
       "                       4.0                   NaN  0.1                  4.0   \n",
       "                  16.0 2.0           1672.664227  0.1                  2.0   \n",
       "                       4.0           1271.619655  0.1                  4.0   \n",
       "                  32.0 2.0            966.310691  0.1                  2.0   \n",
       "                       4.0            708.969207  0.1                  4.0   \n",
       "                  inf  2.0            313.737996  0.1                  2.0   \n",
       "                       4.0            309.842162  0.1                  4.0   \n",
       "128        kubeml 8.0  2.0                   NaN  0.1                  2.0   \n",
       "                       4.0                   NaN  0.1                  4.0   \n",
       "                  16.0 2.0                   NaN  0.1                  2.0   \n",
       "                       4.0                   NaN  0.1                  4.0   \n",
       "                  32.0 2.0            733.743227  0.1                  2.0   \n",
       "                       4.0            460.195127  0.1                  4.0   \n",
       "                  inf  2.0            267.942236  0.1                  2.0   \n",
       "                       4.0            213.627789  0.1                  4.0   \n",
       "256        kubeml 8.0  2.0                   NaN  0.1                  2.0   \n",
       "                       4.0                   NaN  0.1                  4.0   \n",
       "                  16.0 2.0                   NaN  0.1                  2.0   \n",
       "                       4.0                   NaN  0.1                  4.0   \n",
       "                  32.0 2.0                   NaN  0.1                  2.0   \n",
       "                       4.0                   NaN  0.1                  4.0   \n",
       "                  inf  2.0            299.517968  0.1                  2.0   \n",
       "                       4.0                   NaN  0.1                  4.0   \n",
       "\n",
       "                                    validate_every  goal_accuracy  \\\n",
       "batch_size system k    parallelism                                  \n",
       "32         kubeml 8.0  2.0                     1.0          100.0   \n",
       "                       4.0                     1.0          100.0   \n",
       "                  16.0 2.0                     1.0          100.0   \n",
       "                       4.0                     1.0          100.0   \n",
       "                  32.0 2.0                     1.0          100.0   \n",
       "                       4.0                     1.0          100.0   \n",
       "                  inf  2.0                     1.0          100.0   \n",
       "                       4.0                     1.0          100.0   \n",
       "64         kubeml 8.0  2.0                     1.0          100.0   \n",
       "                       4.0                     1.0          100.0   \n",
       "                  16.0 2.0                     1.0          100.0   \n",
       "                       4.0                     1.0          100.0   \n",
       "                  32.0 2.0                     1.0          100.0   \n",
       "                       4.0                     1.0          100.0   \n",
       "                  inf  2.0                     1.0          100.0   \n",
       "                       4.0                     1.0          100.0   \n",
       "128        kubeml 8.0  2.0                     1.0          100.0   \n",
       "                       4.0                     1.0          100.0   \n",
       "                  16.0 2.0                     1.0          100.0   \n",
       "                       4.0                     1.0          100.0   \n",
       "                  32.0 2.0                     1.0          100.0   \n",
       "                       4.0                     1.0          100.0   \n",
       "                  inf  2.0                     1.0          100.0   \n",
       "                       4.0                     1.0          100.0   \n",
       "256        kubeml 8.0  2.0                     1.0          100.0   \n",
       "                       4.0                     1.0          100.0   \n",
       "                  16.0 2.0                     1.0          100.0   \n",
       "                       4.0                     1.0          100.0   \n",
       "                  32.0 2.0                     1.0          100.0   \n",
       "                       4.0                     1.0          100.0   \n",
       "                  inf  2.0                     1.0          100.0   \n",
       "                       4.0                     1.0          100.0   \n",
       "\n",
       "                                    global_batch  \n",
       "batch_size system k    parallelism                \n",
       "32         kubeml 8.0  2.0                  64.0  \n",
       "                       4.0                 128.0  \n",
       "                  16.0 2.0                  64.0  \n",
       "                       4.0                 128.0  \n",
       "                  32.0 2.0                  64.0  \n",
       "                       4.0                 128.0  \n",
       "                  inf  2.0                  64.0  \n",
       "                       4.0                 128.0  \n",
       "64         kubeml 8.0  2.0                 128.0  \n",
       "                       4.0                 256.0  \n",
       "                  16.0 2.0                 128.0  \n",
       "                       4.0                 256.0  \n",
       "                  32.0 2.0                 128.0  \n",
       "                       4.0                 256.0  \n",
       "                  inf  2.0                 128.0  \n",
       "                       4.0                 256.0  \n",
       "128        kubeml 8.0  2.0                 256.0  \n",
       "                       4.0                 512.0  \n",
       "                  16.0 2.0                 256.0  \n",
       "                       4.0                 512.0  \n",
       "                  32.0 2.0                 256.0  \n",
       "                       4.0                 512.0  \n",
       "                  inf  2.0                 256.0  \n",
       "                       4.0                 512.0  \n",
       "256        kubeml 8.0  2.0                 512.0  \n",
       "                       4.0                1024.0  \n",
       "                  16.0 2.0                 512.0  \n",
       "                       4.0                1024.0  \n",
       "                  32.0 2.0                 512.0  \n",
       "                       4.0                1024.0  \n",
       "                  inf  2.0                 512.0  \n",
       "                       4.0                1024.0  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.groupby(['batch_size', 'system', 'k', 'parallelism']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "_r = r[(r.parallelism==4) | (r.parallelism.isna())]\n",
    "\n",
    "f, ax = plt.subplots(1, 3, sharey=False, figsize=(20, 5))\n",
    "sns.barplot(x='batch_size', y='tta_cross_67', data=_r, hue='system', ax=ax[0])\n",
    "# sns.barplot(x='batch_size', y='tta_cross_67', data=df, ax=ax[1])\n",
    "# sns.barplot(x='batch_size', y='gpu_usage', data=df ,ax=ax[2])\n",
    "sns.despine()\n",
    "\n",
    "# plt.savefig('./figures/gpu/tta_99.png', dpi=300)\n",
    "\n",
    "# sns.barplot(x='k', y='tta_99', data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do ANOVA Linear Model to calculate the influence of the parameters\n",
    "\n",
    "Using ANOVA we can get an idea of how the different parameters interact with each other and their influence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the ANOVA test\n",
    "import researchpy as rp\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ANOVA(df: pd.DataFrame, y: str, use_all = False,verbose=False):\n",
    "    \"\"\"Run the ANOVA analysis with the batch, k and parallelism columns for the \n",
    "    given output variable\"\"\"\n",
    "    \n",
    "    # If use all is true we use all the variables to check either accuracy and time\n",
    "    # including also the iowait and the cpu to see what fully influences the stuff\n",
    "    \n",
    "    \n",
    "    if not use_all:\n",
    "        # Plot the summary dataframe\n",
    "        if verbose:\n",
    "            display(rp.summary_cont(df.groupby(['batch_size', 'k', 'parallelism']))[y])\n",
    "\n",
    "        model = ols(f'{y} ~ batch_size*k*parallelism', df).fit()\n",
    "        \n",
    "    else:\n",
    "        if y not in ['acc', 'time']:\n",
    "            raise ValueError('When use_all = True we predict either final_accuracy or time, not', y)\n",
    "        if verbose:\n",
    "            display(rp.summary_cont(df.groupby(['batch_size', 'k', 'parallelism']))[y])\n",
    "\n",
    "        model = ols(f'{y} ~ cpu*batch*njobs*cpu_mean*iowait_mean', df).fit()\n",
    "        \n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Overall model F({model.df_model: .0f},{model.df_resid: .0f}) = {model.fvalue: .3f}, p = {model.f_pvalue: .4f}\")\n",
    "        display(model.summary())\n",
    "    \n",
    "    res = sm.stats.anova_lm(model, typ=2)\n",
    "    \n",
    "    return res, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# df.k = df.k.map(lambda val: -1 if val == float('inf') else val)\n",
    "\n",
    "res, model = ANOVA(d, y='gpu_usage', verbose=True)\n",
    "\n",
    "res\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the distributions of time and accuracy as a function of K, Batch and parallelism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Plot the accuracy as a factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.rc('font', size=16)\n",
    "\n",
    "f, ax = plt.subplots(1, 3, sharey=True, figsize=(20, 5))\n",
    "sns.barplot(x='batch_size', y='acc', hue='k', data=df, ax=ax[0], capsize=.05)\n",
    "sns.barplot(x='k', y='acc', data=df, ax=ax[1], capsize=.05, hue='parallelism')\n",
    "sns.barplot(x='parallelism', y='acc', data=df, hue='k' ,ax=ax[2] ,capsize=.05)\n",
    "sns.despine()\n",
    "plt.legend(title='k', ncol=4, bbox_to_anchor=(0.075,1))\n",
    "\n",
    "for a in ax:\n",
    "    a.set_ylim([75, 100])\n",
    "\n",
    "\n",
    "\n",
    "# plt.savefig('./figures/resnet34/accuracy.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.rc('font', size=16)\n",
    "plt.figure(figsize=(15,10))\n",
    "sns.barplot(x='parallelism', y='tta_cross_99', data=df, capsize=.02, hue='batch_size')\n",
    "\n",
    "\n",
    "# plt.savefig('./figures/resnet34/acc_per_k_and_parallelism.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_pickle('./dataframes/lenet_tensorflow.pkl')\n",
    "d = pd.read_pickle('./dataframes/lenet_kubeml.pkl')\n",
    "d = d.loc[d.parallelism==2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.sort_values('tta_cross_99')[['batch_size', 'tta_cross_99']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Plot the time as a factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, 3, sharey=True, figsize=(20, 5))\n",
    "\n",
    "\n",
    "sns.barplot(x='batch_size', \n",
    "            y='tta_cross_99', \n",
    "            data=d ,\n",
    "            ax=ax[0],\n",
    "            estimator=np.min)\n",
    "\n",
    "sns.barplot(x='batch_size', y='tta_cross_99', data=df, ax=ax[1], estimator=np.min)\n",
    "# sns.barplot(x='parallelism', y='time', data=df, ax=ax[2], hue='k')\n",
    "\n",
    "# plt.savefig('./figures/resnet34/time.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rc('font', size=16)\n",
    "f, ax = plt.subplots(1, 3, figsize=(20,8), sharey=True)\n",
    "sns.barplot(x='k', y='time', data=df.loc[df.batch_size==32], capsize=.05, hue='parallelism', ax=ax[0])\n",
    "sns.barplot(x='k', y='time', data=df.loc[df.batch_size==64], capsize=.05, hue='parallelism', ax=ax[1])\n",
    "sns.barplot(x='k', y='time', data=df.loc[df.batch_size==128], capsize=.05, hue='parallelism', ax=ax[2])\n",
    "\n",
    "plt.savefig('./figures/resnet34/time_per_all.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the validation lines of k=-1 and batch = 32 with different parallelism\n",
    "def plot_loss_with_k_and_batch(k: int, batch:int, ax: plt.Axes = None):\n",
    "    d = df.loc[(df.k==k) & (df.batch_size==batch)].sort_values(by='parallelism', ascending=False)\n",
    "\n",
    "    plt.rc('font', size=13)\n",
    "    if ax is None:\n",
    "        f = plt.figure(figsize=(10, 5))\n",
    "        for _, row in d.iterrows():\n",
    "#             print(row.accuracy)\n",
    "            plt.plot(range(1,6), row.accuracy[:-1] if len(row.accuracy) == 6 else row.accuracy, label=str(row.parallelism))\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy (%)')\n",
    "        plt.title(f'Accuracy evolution with LeNet (batch={batch}, k={k})')\n",
    "        plt.legend(title='parallelism', bbox_to_anchor=(1.05, 0.8))\n",
    "        \n",
    "    else:\n",
    "        for _, row in d.iterrows():\n",
    "#             print(row.accuracy)\n",
    "            ax.plot(range(1,6), row.accuracy[:-1] if len(row.accuracy) == 6 else row.accuracy, label=str(row.parallelism))\n",
    "        ax.set_xlabel('Epoch')\n",
    "        ax.set_ylabel('Accuracy (%)')\n",
    "        ax.set_title(f'Batch={batch}, k={k}')\n",
    "        ax.legend(title='parallelism')\n",
    "        \n",
    "\n",
    "f, axes = plt.subplots(nrows=3, ncols=3, figsize=(20, 15), sharex=True)\n",
    "\n",
    "plt.suptitle('Behavior of K, Parallelism and Batch in Accuracy')\n",
    "\n",
    "plot_loss_with_k_and_batch(k=8, batch=32, ax=axes[0][0])\n",
    "plot_loss_with_k_and_batch(k=16, batch=32, ax=axes[0][1])   \n",
    "plot_loss_with_k_and_batch(k=64, batch=32, ax=axes[0][2])\n",
    "    \n",
    "plot_loss_with_k_and_batch(k=8, batch=64, ax=axes[1][0])\n",
    "plot_loss_with_k_and_batch(k=16, batch=64, ax=axes[1][1])   \n",
    "plot_loss_with_k_and_batch(k=64, batch=64, ax=axes[1][2])\n",
    "\n",
    "plot_loss_with_k_and_batch(k=8, batch=128, ax=axes[2][0])\n",
    "plot_loss_with_k_and_batch(k=16, batch=128, ax=axes[2][1])   \n",
    "plot_loss_with_k_and_batch(k=64, batch=128, ax=axes[2][2])\n",
    "\n",
    "\n",
    "# plt.savefig('./figures/accuracy_study.png', dpi=300)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Keep K and Batch set, vary parallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the validation lines of k=-1 and batch = 32 with different parallelism\n",
    "def plot_loss_with_k_and_batch(k: int, batch:int, ax: plt.Axes = None):\n",
    "    d = df.loc[(df.k==k) & (df.batch_size==batch)].sort_values(by='parallelism', ascending=False)\n",
    "\n",
    "    plt.rc('font', size=13)\n",
    "    if ax is None:\n",
    "        f = plt.figure(figsize=(10, 5))\n",
    "        for _, row in d.iterrows():\n",
    "#             print(row.accuracy)\n",
    "            plt.plot(range(1,6), row.accuracy[:-1] if len(row.accuracy) == 6 else row.accuracy, label=str(row.parallelism))\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy (%)')\n",
    "        plt.title(f'Accuracy evolution with LeNet (batch={batch}, k={k})')\n",
    "        plt.legend(title='parallelism', bbox_to_anchor=(1.05, 0.8))\n",
    "        \n",
    "    else:\n",
    "        for _, row in d.iterrows():\n",
    "#             print(row.accuracy)\n",
    "            ax.plot(range(1,6), row.accuracy[:-1] if len(row.accuracy) == 6 else row.accuracy, label=str(row.parallelism))\n",
    "        ax.set_xlabel('Epoch')\n",
    "        ax.set_ylabel('Accuracy (%)')\n",
    "        ax.set_title(f'Batch={batch}, k={k}')\n",
    "        ax.legend(title='parallelism')\n",
    "        \n",
    "\n",
    "f, axes = plt.subplots(nrows=3, ncols=3, figsize=(20, 15), sharex=True)\n",
    "\n",
    "plt.suptitle('Behavior of K, Parallelism and Batch in Accuracy')\n",
    "\n",
    "plot_loss_with_k_and_batch(k=8, batch=32, ax=axes[0][0])\n",
    "plot_loss_with_k_and_batch(k=16, batch=32, ax=axes[0][1])   \n",
    "plot_loss_with_k_and_batch(k=64, batch=32, ax=axes[0][2])\n",
    "    \n",
    "plot_loss_with_k_and_batch(k=8, batch=64, ax=axes[1][0])\n",
    "plot_loss_with_k_and_batch(k=16, batch=64, ax=axes[1][1])   \n",
    "plot_loss_with_k_and_batch(k=64, batch=64, ax=axes[1][2])\n",
    "\n",
    "plot_loss_with_k_and_batch(k=8, batch=128, ax=axes[2][0])\n",
    "plot_loss_with_k_and_batch(k=16, batch=128, ax=axes[2][1])   \n",
    "plot_loss_with_k_and_batch(k=64, batch=128, ax=axes[2][2])\n",
    "\n",
    "\n",
    "# plt.savefig('./figures/accuracy_study.png', dpi=300)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Keep Parallelism and batch set, vary K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the validation lines of k=-1 and batch = 32 with different parallelism\n",
    "def plot_loss_with_parallelism_and_batch(p: int, batch:int, ax: plt.Axes = None):\n",
    "    d = df.loc[(df.parallelism==p) & (df.batch_size==batch)].sort_values(by='k', ascending=False)\n",
    "    \n",
    "    approx_k = (60000/p)/batch\n",
    "\n",
    "    plt.rc('font', size=16)\n",
    "    if ax is None:\n",
    "        f = plt.figure(figsize=(10, 5))\n",
    "        for _, row in d.iterrows():\n",
    "#             print(row.accuracy)\n",
    "            plt.plot(range(1,6), row.accuracy[:-1] if len(row.accuracy) == 6 else row.accuracy, label=str(row.k))\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy (%)')\n",
    "        plt.title(f'Accuracy evolution with LeNet (batch={batch}, k={k})')\n",
    "        plt.legend(title='parallelism', bbox_to_anchor=(1.05, 0.8))\n",
    "        \n",
    "    else:\n",
    "        for _, row in d.iterrows():\n",
    "            label = str(row.k) if row.k != float('inf') else f'{row.k} ({int(approx_k)})'\n",
    "            ax.plot(range(1,6), row.accuracy[:-1] if len(row.accuracy) == 6 else row.accuracy, label=label)\n",
    "        ax.set_xlabel('Epoch')\n",
    "        ax.set_ylabel('Accuracy (%)')\n",
    "        ax.set_title(f'Batch={batch}, Parallelism={p}')\n",
    "        ax.legend(title='k')\n",
    "        \n",
    "\n",
    "f, axes = plt.subplots(nrows=3, ncols=3, figsize=(30, 20), sharex=True)\n",
    "\n",
    "plt.suptitle('Behavior of K, Parallelism and Batch in Accuracy')\n",
    "\n",
    "plot_loss_with_parallelism_and_batch(p=2, batch=32, ax=axes[0][0])\n",
    "plot_loss_with_parallelism_and_batch(p=4, batch=32, ax=axes[0][1])   \n",
    "plot_loss_with_parallelism_and_batch(p=8, batch=32, ax=axes[0][2])\n",
    "\n",
    "plot_loss_with_parallelism_and_batch(p=2, batch=64, ax=axes[1][0])\n",
    "plot_loss_with_parallelism_and_batch(p=4, batch=64, ax=axes[1][1])   \n",
    "plot_loss_with_parallelism_and_batch(p=8, batch=64, ax=axes[1][2])\n",
    "\n",
    "plot_loss_with_parallelism_and_batch(p=2, batch=128, ax=axes[2][0])\n",
    "plot_loss_with_parallelism_and_batch(p=4, batch=128, ax=axes[2][1])   \n",
    "plot_loss_with_parallelism_and_batch(p=8, batch=128, ax=axes[2][2])\n",
    "\n",
    "\n",
    "plt.savefig('./figures/accuracy_study_varying_k.png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Plot 3d dependencies between K and parallelism on time and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# f, ax = plt.subplots(1, 2, projection='3d')\n",
    "\n",
    "f = plt.figure()\n",
    "ax = f.add_subplot(1, 2, 1, projection='3d')\n",
    "\n",
    "from scipy.interpolate import griddata\n",
    "X, Y = np.meshgrid(df.k.map(lambda v: 500 if v == float('inf') else v), df.parallelism)\n",
    "Z = griddata((df.k.map(lambda v: 500 if v == float('inf') else v),\n",
    "              df.parallelism),\n",
    "              df.acc, (X, Y), method='cubic')\n",
    "\n",
    "ax.plot_surface(X, Y, Z, rstride=1, cstride=1, cmap='coolwarm',\n",
    "                       linewidth=0, antialiased=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "d = df.loc[df.batch==64]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
