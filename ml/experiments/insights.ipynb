{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Gather insights from the experiments run on GCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from common.utils import check_missing_experiments, join_df\n",
    "from common.metrics import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the TTA Formula "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate time to accuracy for different accuracies\n",
    "def tta_crossbow(acc:int, df: pd.DataFrame, acc_column='accuracy', time_column='epoch_duration'):\n",
    "    \"\"\"Computes the tta as in the crossbow paper\n",
    "    where the tta is the median of the last 5 epochs\"\"\"\n",
    "\n",
    "    res = []\n",
    "    for _, row in df.iterrows():\n",
    "        done = False\n",
    "        dur, accuracy = row[time_column], row[acc_column]\n",
    "        \n",
    "        for idx, (t, a) in enumerate(zip(dur, accuracy[:len(dur)])):\n",
    "            \n",
    "            # if there are less than 5 elements behind, continue\n",
    "            if idx < 4:\n",
    "                continue\n",
    "                \n",
    "            # calculate the median of the next five elements\n",
    "            if np.median(accuracy[idx - 4:idx+1]) >= acc:\n",
    "                res.append(t)\n",
    "                done = True\n",
    "                break\n",
    "\n",
    "        if not done:\n",
    "            res.append(np.nan)\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "def tta(acc:int, df:pd.DataFrame,  acc_column='accuracy', time_column='epoch_duration'):\n",
    "    \n",
    "    \n",
    "    res = []\n",
    "    for _, row in df.iterrows():\n",
    "        done=False\n",
    "        dur, accuracy = row[time_column], row[acc_column]\n",
    "        \n",
    "        for idx, (t, a) in enumerate(zip(dur, accuracy[:len(dur)])):\n",
    "         \n",
    "            if a >= acc:\n",
    "                res.append(t)\n",
    "                done = True\n",
    "                break\n",
    "\n",
    "        if not done:\n",
    "            res.append(np.nan)\n",
    "\n",
    "    return res\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KubeML Experiments\n",
    "\n",
    "How to treat the kubeml experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the train results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = join_df('./results/resnet/train')\n",
    "df = df[df.default_parallelism > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>hash</th>\n",
       "      <th>model_type</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>epochs</th>\n",
       "      <th>dataset</th>\n",
       "      <th>lr</th>\n",
       "      <th>function_name</th>\n",
       "      <th>default_parallelism</th>\n",
       "      <th>static_parallelism</th>\n",
       "      <th>validate_every</th>\n",
       "      <th>k</th>\n",
       "      <th>goal_accuracy</th>\n",
       "      <th>validation_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>parallelism</th>\n",
       "      <th>epoch_duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0386935d</td>\n",
       "      <td>69979e67b7622197</td>\n",
       "      <td>example</td>\n",
       "      <td>128</td>\n",
       "      <td>30</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>0.1</td>\n",
       "      <td>resnet</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>100</td>\n",
       "      <td>[2.0691979338851154, 1.761023279986804, 1.5237...</td>\n",
       "      <td>[21.41020569620253, 32.070806962025316, 47.784...</td>\n",
       "      <td>[3.4358344075022913, 1.9820176490715573, 1.680...</td>\n",
       "      <td>[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, ...</td>\n",
       "      <td>[46.101114115, 92.594095861, 138.901519929, 18...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>05666e5d</td>\n",
       "      <td>a2cc2a5d5cc402f9</td>\n",
       "      <td>example</td>\n",
       "      <td>128</td>\n",
       "      <td>30</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>0.1</td>\n",
       "      <td>resnet</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>100</td>\n",
       "      <td>[2.1832512511482722, 1.6482713418670847, 1.520...</td>\n",
       "      <td>[20.352056962025316, 39.47784810126582, 45.203...</td>\n",
       "      <td>[3.633183332122102, 1.9733640703619744, 1.6998...</td>\n",
       "      <td>[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, ...</td>\n",
       "      <td>[19.251981886, 39.583568774, 60.468725951, 79....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>06804c15</td>\n",
       "      <td>8b0f4cc587901704</td>\n",
       "      <td>example</td>\n",
       "      <td>256</td>\n",
       "      <td>30</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>0.1</td>\n",
       "      <td>resnet</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>100</td>\n",
       "      <td>[2.592677044868469, 1.7636539191007614, 1.6012...</td>\n",
       "      <td>[17.958984375, 33.1640625, 39.794921875, 44.40...</td>\n",
       "      <td>[4.071233475694851, 2.1830299715606536, 1.8489...</td>\n",
       "      <td>[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, ...</td>\n",
       "      <td>[26.678817886, 53.53629918, 78.251857194, 103....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0bb3e9ac</td>\n",
       "      <td>50f218a144063794</td>\n",
       "      <td>example</td>\n",
       "      <td>64</td>\n",
       "      <td>30</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>0.1</td>\n",
       "      <td>resnet</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>100</td>\n",
       "      <td>[2.01867682842692, 1.906603324944806, 1.419383...</td>\n",
       "      <td>[28.54299363057325, 28.61265923566879, 47.2730...</td>\n",
       "      <td>[2.624373005631635, 1.756249717129466, 1.54868...</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[102.032926309, 201.191873608, 299.563335538, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13190b0b</td>\n",
       "      <td>02b1f56c9c27a5b4</td>\n",
       "      <td>example</td>\n",
       "      <td>128</td>\n",
       "      <td>30</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>0.1</td>\n",
       "      <td>resnet</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>100</td>\n",
       "      <td>[5.0395004236245455, 1.5423524047754988, 1.449...</td>\n",
       "      <td>[10.116693037974683, 44.11590189873418, 47.567...</td>\n",
       "      <td>[2.962399608322552, 1.6866237937795874, 1.4655...</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[23.07288706, 46.800417874, 74.283571081, 98.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>f86d546d</td>\n",
       "      <td>4cf9aff8b0e3ec9e</td>\n",
       "      <td>example</td>\n",
       "      <td>64</td>\n",
       "      <td>30</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>0.1</td>\n",
       "      <td>resnet</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>100</td>\n",
       "      <td>[3.8288797876637455, 2.403817397014351, 1.9774...</td>\n",
       "      <td>[10, 10.26, 26.85, 35.03, 40.550000000000004, ...</td>\n",
       "      <td>[3.360841138029304, 2.3875012396939574, 1.9948...</td>\n",
       "      <td>[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, ...</td>\n",
       "      <td>[14.801709896, 31.24002004, 48.253630979, 64.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>f9dc984a</td>\n",
       "      <td>9c49eeb8a706275a</td>\n",
       "      <td>example</td>\n",
       "      <td>64</td>\n",
       "      <td>30</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>0.1</td>\n",
       "      <td>resnet</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>100</td>\n",
       "      <td>[1.705808477037272, 1.4686595649476264, 1.3200...</td>\n",
       "      <td>[37.430334394904456, 46.34753184713376, 52.478...</td>\n",
       "      <td>[2.712390104218212, 1.6115226571822105, 1.4244...</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[38.899608234, 82.870589755, 126.082686868, 16...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>f9ff89fc</td>\n",
       "      <td>7e945a487feff74e</td>\n",
       "      <td>example</td>\n",
       "      <td>128</td>\n",
       "      <td>30</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>0.1</td>\n",
       "      <td>resnet</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>100</td>\n",
       "      <td>[2.1471189894253695, 1.759268745591369, 1.5828...</td>\n",
       "      <td>[22.34, 33.46, 41.81, 47.64, 48.33999999999999...</td>\n",
       "      <td>[3.2209879354174644, 1.9946543223091535, 1.743...</td>\n",
       "      <td>[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, ...</td>\n",
       "      <td>[27.999510971, 57.792588364, 86.948315407, 115...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>fbcccc6c</td>\n",
       "      <td>afe503f94d4d8264</td>\n",
       "      <td>example</td>\n",
       "      <td>64</td>\n",
       "      <td>30</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>0.1</td>\n",
       "      <td>resnet</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>100</td>\n",
       "      <td>[1.8421584147556571, 1.573136512640935, 1.5360...</td>\n",
       "      <td>[31.22014331210191, 41.49084394904459, 42.8841...</td>\n",
       "      <td>[3.297370414573881, 1.8065422634400665, 1.6034...</td>\n",
       "      <td>[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, ...</td>\n",
       "      <td>[31.308027951, 61.831227029, 90.963939092, 120...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>fd57f4d1</td>\n",
       "      <td>1d0577c2b0263cbe</td>\n",
       "      <td>example</td>\n",
       "      <td>256</td>\n",
       "      <td>30</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>0.1</td>\n",
       "      <td>resnet</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>100</td>\n",
       "      <td>[1.930098184943199, 1.718866342306137, 1.75740...</td>\n",
       "      <td>[27.470703125, 36.1328125, 41.89453125, 45.966...</td>\n",
       "      <td>[3.2220769907747, 1.9234242487926871, 1.651999...</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[32.766331703, 66.356944417, 101.061452874, 13...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>96 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id              hash model_type  batch_size  epochs  dataset   lr  \\\n",
       "0    0386935d  69979e67b7622197    example         128      30  cifar10  0.1   \n",
       "1    05666e5d  a2cc2a5d5cc402f9    example         128      30  cifar10  0.1   \n",
       "2    06804c15  8b0f4cc587901704    example         256      30  cifar10  0.1   \n",
       "3    0bb3e9ac  50f218a144063794    example          64      30  cifar10  0.1   \n",
       "4    13190b0b  02b1f56c9c27a5b4    example         128      30  cifar10  0.1   \n",
       "..        ...               ...        ...         ...     ...      ...  ...   \n",
       "97   f86d546d  4cf9aff8b0e3ec9e    example          64      30  cifar10  0.1   \n",
       "98   f9dc984a  9c49eeb8a706275a    example          64      30  cifar10  0.1   \n",
       "99   f9ff89fc  7e945a487feff74e    example         128      30  cifar10  0.1   \n",
       "100  fbcccc6c  afe503f94d4d8264    example          64      30  cifar10  0.1   \n",
       "101  fd57f4d1  1d0577c2b0263cbe    example         256      30  cifar10  0.1   \n",
       "\n",
       "    function_name  default_parallelism  static_parallelism  validate_every  \\\n",
       "0          resnet                    4                True               1   \n",
       "1          resnet                    4                True               1   \n",
       "2          resnet                    4                True               1   \n",
       "3          resnet                    2                True               1   \n",
       "4          resnet                    2                True               1   \n",
       "..            ...                  ...                 ...             ...   \n",
       "97         resnet                    4                True               1   \n",
       "98         resnet                    2                True               1   \n",
       "99         resnet                    4                True               1   \n",
       "100        resnet                    4                True               1   \n",
       "101        resnet                    2                True               1   \n",
       "\n",
       "      k  goal_accuracy                                    validation_loss  \\\n",
       "0     8            100  [2.0691979338851154, 1.761023279986804, 1.5237...   \n",
       "1    32            100  [2.1832512511482722, 1.6482713418670847, 1.520...   \n",
       "2     8            100  [2.592677044868469, 1.7636539191007614, 1.6012...   \n",
       "3     8            100  [2.01867682842692, 1.906603324944806, 1.419383...   \n",
       "4    32            100  [5.0395004236245455, 1.5423524047754988, 1.449...   \n",
       "..   ..            ...                                                ...   \n",
       "97   -1            100  [3.8288797876637455, 2.403817397014351, 1.9774...   \n",
       "98   32            100  [1.705808477037272, 1.4686595649476264, 1.3200...   \n",
       "99   16            100  [2.1471189894253695, 1.759268745591369, 1.5828...   \n",
       "100  32            100  [1.8421584147556571, 1.573136512640935, 1.5360...   \n",
       "101   8            100  [1.930098184943199, 1.718866342306137, 1.75740...   \n",
       "\n",
       "                                              accuracy  \\\n",
       "0    [21.41020569620253, 32.070806962025316, 47.784...   \n",
       "1    [20.352056962025316, 39.47784810126582, 45.203...   \n",
       "2    [17.958984375, 33.1640625, 39.794921875, 44.40...   \n",
       "3    [28.54299363057325, 28.61265923566879, 47.2730...   \n",
       "4    [10.116693037974683, 44.11590189873418, 47.567...   \n",
       "..                                                 ...   \n",
       "97   [10, 10.26, 26.85, 35.03, 40.550000000000004, ...   \n",
       "98   [37.430334394904456, 46.34753184713376, 52.478...   \n",
       "99   [22.34, 33.46, 41.81, 47.64, 48.33999999999999...   \n",
       "100  [31.22014331210191, 41.49084394904459, 42.8841...   \n",
       "101  [27.470703125, 36.1328125, 41.89453125, 45.966...   \n",
       "\n",
       "                                            train_loss  \\\n",
       "0    [3.4358344075022913, 1.9820176490715573, 1.680...   \n",
       "1    [3.633183332122102, 1.9733640703619744, 1.6998...   \n",
       "2    [4.071233475694851, 2.1830299715606536, 1.8489...   \n",
       "3    [2.624373005631635, 1.756249717129466, 1.54868...   \n",
       "4    [2.962399608322552, 1.6866237937795874, 1.4655...   \n",
       "..                                                 ...   \n",
       "97   [3.360841138029304, 2.3875012396939574, 1.9948...   \n",
       "98   [2.712390104218212, 1.6115226571822105, 1.4244...   \n",
       "99   [3.2209879354174644, 1.9946543223091535, 1.743...   \n",
       "100  [3.297370414573881, 1.8065422634400665, 1.6034...   \n",
       "101  [3.2220769907747, 1.9234242487926871, 1.651999...   \n",
       "\n",
       "                                           parallelism  \\\n",
       "0    [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, ...   \n",
       "1    [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, ...   \n",
       "2    [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, ...   \n",
       "3    [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "4    [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "..                                                 ...   \n",
       "97   [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, ...   \n",
       "98   [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "99   [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, ...   \n",
       "100  [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, ...   \n",
       "101  [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "\n",
       "                                        epoch_duration  \n",
       "0    [46.101114115, 92.594095861, 138.901519929, 18...  \n",
       "1    [19.251981886, 39.583568774, 60.468725951, 79....  \n",
       "2    [26.678817886, 53.53629918, 78.251857194, 103....  \n",
       "3    [102.032926309, 201.191873608, 299.563335538, ...  \n",
       "4    [23.07288706, 46.800417874, 74.283571081, 98.0...  \n",
       "..                                                 ...  \n",
       "97   [14.801709896, 31.24002004, 48.253630979, 64.1...  \n",
       "98   [38.899608234, 82.870589755, 126.082686868, 16...  \n",
       "99   [27.999510971, 57.792588364, 86.948315407, 115...  \n",
       "100  [31.308027951, 61.831227029, 90.963939092, 120...  \n",
       "101  [32.766331703, 66.356944417, 101.061452874, 13...  \n",
       "\n",
       "[96 rows x 18 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get duplicated rows\n",
    "df[df.duplicated(['hash'], keep=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the extra variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Set the acc to the final accuracy\n",
    "df['acc'] = df.accuracy.map(lambda a: a[-1])\n",
    "\n",
    "# Set the time to the sum of the epoch durations\n",
    "df['time'] = df.epoch_duration.map(lambda t: t[-1])\n",
    "\n",
    "# Set the parallelism to the first since it is constant\n",
    "df.parallelism = df.parallelism.map(lambda l:l[0])\n",
    "\n",
    "# change -1 to inf so the order is right in the plot\n",
    "df.k = df.k.map(lambda val: float('inf') if val == -1 else val)\n",
    "\n",
    "df['global_batch'] = df.batch_size * df.parallelism\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the ttas\n",
    "df['tta_69'] = tta(69, df)\n",
    "df['tta_cross_69'] = tta_crossbow(69, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For the metrics in resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = join_df('./results/resnet/metrics')\n",
    "m = metrics.rename(columns={'exp_name':'id'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For the metrics in the lenet\n",
    "\n",
    "The first replication does not have the proper format, so we need to reformat it and combine it with the train results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics1 = join_df('./results/lenet/metrics/1/')\n",
    "metrics2 = join_df('./results/lenet/metrics/2/')\n",
    "metrics3 = join_df('./results/lenet/metrics/3/')\n",
    "\n",
    "cpu = metrics1.groupby('exp_name')['cpu'].apply(list)\n",
    "mem = metrics1.groupby('exp_name')['mem'].apply(list)\n",
    "exps = metrics1.groupby('exp_name')['exp_name']\n",
    "\n",
    "metrics1 = pd.DataFrame({\n",
    "    'cpu':cpu,\n",
    "    'mem':mem\n",
    "})\n",
    "metrics1['exp_name'] = metrics1.index\n",
    "\n",
    "# concat all metrics and rename the exp_name as in the train\n",
    "m = pd.concat([metrics1, metrics2, metrics3], ignore_index=True)\n",
    "m.rename(columns={'exp_name':'id'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to add extra summary columns to the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to compute the mean of each and add columns\n",
    "m['mem'] = m['mem'].map(lambda l: l[0] if isinstance(l[0], list) else l)\n",
    "m['cpu'] = m['cpu'].map(lambda l: l[0] if isinstance(l[0], list) else l)\n",
    "\n",
    "# cpu util\n",
    "m['cpu_mean'] = m['cpu'].map(lambda l: np.mean([s.percent for s in l]))\n",
    "\n",
    "# gpu mean mem and util\n",
    "m['gpu_0_mean_usage'] = m['gpu_0'].map(lambda l: np.mean([s.load for s in l if s.mem_used != 0]) if not isinstance(l, float) else l)\n",
    "m['gpu_1_mean_usage'] = m['gpu_1'].map(lambda l: np.mean([s.load for s in l if s.mem_used !=0]) if not isinstance(l, float) else l)\n",
    "m['gpu_0_mean_memory'] = m['gpu_0'].map(lambda l: np.mean([s.mem_used for s in l if s.mem_used != 0])if not isinstance(l, float) else l)\n",
    "m['gpu_1_mean_memory'] = m['gpu_1'].map(lambda l: np.mean([s.mem_used for s in l if s.mem_used != 0])if not isinstance(l, float) else l)\n",
    "m['gpu_usage'] = (m['gpu_0_mean_usage'] + m['gpu_1_mean_usage']) /2\n",
    "\n",
    "# memory mean util\n",
    "m['mem_mean'] = m['mem'].map(lambda l: np.mean([s.percent for s in l]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine and Save the whole experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = df.merge(m, on='id')\n",
    "d.to_pickle('./dataframes/resnet_kubeml.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Experiments\n",
    "\n",
    "How to treat the TF experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = join_df('./results/tf/lenet/train/1/', './results/tf/lenet/train/2', './results/tf/lenet/train/3')\n",
    "\n",
    "# Set the acc to the final accuracy\n",
    "df['acc'] = df.val_accuracy.map(lambda a: a[-1])\n",
    "\n",
    "# Set the time to the sum of the epoch durations\n",
    "df['time'] = df.times.map(lambda t: t[-1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the TTA\n",
    "resnet['tta_67'] = tta(0.69, resnet, time_column='times', acc_column='val_accuracy')\n",
    "resnet['tta_cross_67'] = tta_crossbow(0.69, resnet, time_column='times', acc_column='val_accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge the metrics from different  folders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "m = join_df('./results/tf/lenet/metrics/1/', './results/tf/lenet/metrics/2', './results/tf/lenet/metrics/3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to compute the mean of each and add columns\n",
    "m['mem'] = m['mem'].map(lambda l: l[0] if isinstance(l[0], list) else l)\n",
    "m['cpu'] = m['cpu'].map(lambda l: l[0] if isinstance(l[0], list) else l)\n",
    "\n",
    "# cpu util\n",
    "m['cpu_mean'] = m['cpu'].map(lambda l: np.mean([s.percent for s in l]))\n",
    "\n",
    "# gpu mean mem and util\n",
    "m['gpu_0_mean_usage'] = m['gpu_0'].map(lambda l: np.mean([s.load for s in l if s.mem_used != 0]) if not isinstance(l, float) else l)\n",
    "m['gpu_1_mean_usage'] = m['gpu_1'].map(lambda l: np.mean([s.load for s in l if s.mem_used !=0]) if not isinstance(l, float) else l)\n",
    "m['gpu_0_mean_memory'] = m['gpu_0'].map(lambda l: np.mean([s.mem_used for s in l if s.mem_used != 0])if not isinstance(l, float) else l)\n",
    "m['gpu_1_mean_memory'] = m['gpu_1'].map(lambda l: np.mean([s.mem_used for s in l if s.mem_used != 0])if not isinstance(l, float) else l)\n",
    "m['gpu_usage'] = (m['gpu_0_mean_usage'] + m['gpu_1_mean_usage']) /2\n",
    "\n",
    "# memory mean util\n",
    "m['mem_mean'] = m['mem'].map(lambda l: np.mean([s.percent for s in l]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join on the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.join(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet.to_pickle('./dataframes/resnet_tensorflow.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization of Results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the color palette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "green_red_palette = ['#264653', '#2A9D8F', '#E9C46A', '#F4A261', '#E76F51']\n",
    "blue_yellow_palette=  ['#0077b6', '#d62828', '#f77f00', '#fcbf49', '#eae2b7']\n",
    "cool_p = ['#f87575', '#ffa9a3', '#b9e6ff', '#5c95ff', '#7e6c6c']\n",
    "wall_p = ['#e63946', '#f1faee', '#a8dadc', '#457b9d', '#1d3557']\n",
    "\n",
    "sns.palplot(sns.color_palette(blue_yellow_palette))\n",
    "\n",
    "\n",
    "sns.set_theme(style='whitegrid', palette=blue_yellow_palette, )\n",
    "# sns.set_palette(blue_yellow_palette)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Read the experiments file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load all of the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load the tf experiments\n",
    "resnet = pd.read_pickle('./dataframes/resnet_tensorflow.pkl')\n",
    "lenet = pd.read_pickle('./dataframes/lenet_tensorflow.pkl')\n",
    "\n",
    "resnet.rename(columns={\n",
    "    'loss':'train_loss',\n",
    "    'val_accuracy':'accuracy',\n",
    "    'val_loss':'validation_loss',\n",
    "    'times':'epoch_duration',\n",
    "    'accuracy':'train_accuracy',\n",
    "    'val_accuracy':'accuracy'\n",
    "}, inplace=True)\n",
    "resnet['system'] = 'tensorflow'\n",
    "\n",
    "\n",
    "# set the columns of the \n",
    "\n",
    "# load the kubeml experiments\n",
    "kuberesnet = pd.read_pickle('./dataframes/resnet_kubeml.pkl')\n",
    "kuberesnet['model'] = 'resnet'\n",
    "kuberesnet['system'] = 'kubeml'\n",
    "\n",
    "kubelenet = pd.read_pickle('./dataframes/lenet_kubeml.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the full resnet stuff\n",
    "r.to_pickle('./dataframes/resnet.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>hash</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>epochs</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>validation_loss</th>\n",
       "      <th>epoch_duration</th>\n",
       "      <th>acc</th>\n",
       "      <th>...</th>\n",
       "      <th>dataset</th>\n",
       "      <th>lr</th>\n",
       "      <th>function_name</th>\n",
       "      <th>default_parallelism</th>\n",
       "      <th>static_parallelism</th>\n",
       "      <th>validate_every</th>\n",
       "      <th>k</th>\n",
       "      <th>goal_accuracy</th>\n",
       "      <th>parallelism</th>\n",
       "      <th>global_batch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>resnet</td>\n",
       "      <td>3b64b0be38fb8e94</td>\n",
       "      <td>32</td>\n",
       "      <td>30</td>\n",
       "      <td>[0.17032000422477722, 0.2808600068092346, 0.35...</td>\n",
       "      <td>[2.386634588241577, 1.8890058994293213, 1.7486...</td>\n",
       "      <td>[0.2615000009536743, 0.3276999890804291, 0.395...</td>\n",
       "      <td>[1.8862396478652954, 1.725894570350647, 1.6532...</td>\n",
       "      <td>[86.82871699333191, 161.10458970069885, 235.44...</td>\n",
       "      <td>0.654200</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>resnet</td>\n",
       "      <td>9848e15a8cb9456b</td>\n",
       "      <td>256</td>\n",
       "      <td>30</td>\n",
       "      <td>[0.20927999913692474, 0.35989999771118164, 0.4...</td>\n",
       "      <td>[2.9395194053649902, 1.7702909708023071, 1.570...</td>\n",
       "      <td>[0.14980000257492065, 0.17270000278949738, 0.3...</td>\n",
       "      <td>[2.747114896774292, 2.5870492458343506, 1.9928...</td>\n",
       "      <td>[26.537360668182373, 38.13520431518555, 49.665...</td>\n",
       "      <td>0.656100</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>resnet</td>\n",
       "      <td>e551b774a7b5ccc0</td>\n",
       "      <td>128</td>\n",
       "      <td>30</td>\n",
       "      <td>[0.21724000573158264, 0.33597999811172485, 0.4...</td>\n",
       "      <td>[2.6788625717163086, 1.7895755767822266, 1.611...</td>\n",
       "      <td>[0.23649999499320984, 0.2770000100135803, 0.44...</td>\n",
       "      <td>[2.0208323001861572, 1.976898431777954, 1.5212...</td>\n",
       "      <td>[33.218318939208984, 53.329391956329346, 73.47...</td>\n",
       "      <td>0.653100</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>resnet</td>\n",
       "      <td>eeaea0a56be5c6c9</td>\n",
       "      <td>64</td>\n",
       "      <td>30</td>\n",
       "      <td>[0.21930000185966492, 0.346560001373291, 0.435...</td>\n",
       "      <td>[2.442678451538086, 1.7620742321014404, 1.5449...</td>\n",
       "      <td>[0.20569999516010284, 0.38609999418258667, 0.4...</td>\n",
       "      <td>[2.4119229316711426, 1.6152931451797485, 1.422...</td>\n",
       "      <td>[51.08732867240906, 89.5283989906311, 127.9574...</td>\n",
       "      <td>0.658100</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>resnet</td>\n",
       "      <td>3b64b0be38fb8e94</td>\n",
       "      <td>32</td>\n",
       "      <td>30</td>\n",
       "      <td>[0.17776000499725342, 0.25637999176979065, 0.3...</td>\n",
       "      <td>[2.443981170654297, 1.9975420236587524, 1.8435...</td>\n",
       "      <td>[0.2281000018119812, 0.31769999861717224, 0.39...</td>\n",
       "      <td>[2.1482324600219727, 1.9316531419754028, 1.673...</td>\n",
       "      <td>[91.24218130111694, 167.12129759788513, 242.37...</td>\n",
       "      <td>0.682500</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>resnet</td>\n",
       "      <td>4cf9aff8b0e3ec9e</td>\n",
       "      <td>64</td>\n",
       "      <td>30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[3.360841138029304, 2.3875012396939574, 1.9948...</td>\n",
       "      <td>[10, 10.26, 26.85, 35.03, 40.550000000000004, ...</td>\n",
       "      <td>[3.8288797876637455, 2.403817397014351, 1.9774...</td>\n",
       "      <td>[14.801709896, 31.24002004, 48.253630979, 64.1...</td>\n",
       "      <td>72.910000</td>\n",
       "      <td>...</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>0.1</td>\n",
       "      <td>resnet</td>\n",
       "      <td>4.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>inf</td>\n",
       "      <td>100.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>256.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>resnet</td>\n",
       "      <td>9c49eeb8a706275a</td>\n",
       "      <td>64</td>\n",
       "      <td>30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[2.712390104218212, 1.6115226571822105, 1.4244...</td>\n",
       "      <td>[37.430334394904456, 46.34753184713376, 52.478...</td>\n",
       "      <td>[1.705808477037272, 1.4686595649476264, 1.3200...</td>\n",
       "      <td>[38.899608234, 82.870589755, 126.082686868, 16...</td>\n",
       "      <td>69.725318</td>\n",
       "      <td>...</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>0.1</td>\n",
       "      <td>resnet</td>\n",
       "      <td>2.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>128.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>resnet</td>\n",
       "      <td>7e945a487feff74e</td>\n",
       "      <td>128</td>\n",
       "      <td>30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[3.2209879354174644, 1.9946543223091535, 1.743...</td>\n",
       "      <td>[22.34, 33.46, 41.81, 47.64, 48.33999999999999...</td>\n",
       "      <td>[2.1471189894253695, 1.759268745591369, 1.5828...</td>\n",
       "      <td>[27.999510971, 57.792588364, 86.948315407, 115...</td>\n",
       "      <td>67.620000</td>\n",
       "      <td>...</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>0.1</td>\n",
       "      <td>resnet</td>\n",
       "      <td>4.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>512.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>resnet</td>\n",
       "      <td>afe503f94d4d8264</td>\n",
       "      <td>64</td>\n",
       "      <td>30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[3.297370414573881, 1.8065422634400665, 1.6034...</td>\n",
       "      <td>[31.22014331210191, 41.49084394904459, 42.8841...</td>\n",
       "      <td>[1.8421584147556571, 1.573136512640935, 1.5360...</td>\n",
       "      <td>[31.308027951, 61.831227029, 90.963939092, 120...</td>\n",
       "      <td>70.989252</td>\n",
       "      <td>...</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>0.1</td>\n",
       "      <td>resnet</td>\n",
       "      <td>4.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>256.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>resnet</td>\n",
       "      <td>1d0577c2b0263cbe</td>\n",
       "      <td>256</td>\n",
       "      <td>30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[3.2220769907747, 1.9234242487926871, 1.651999...</td>\n",
       "      <td>[27.470703125, 36.1328125, 41.89453125, 45.966...</td>\n",
       "      <td>[1.930098184943199, 1.718866342306137, 1.75740...</td>\n",
       "      <td>[32.766331703, 66.356944417, 101.061452874, 13...</td>\n",
       "      <td>67.402344</td>\n",
       "      <td>...</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>0.1</td>\n",
       "      <td>resnet</td>\n",
       "      <td>2.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>512.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>108 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      model              hash  batch_size  epochs  \\\n",
       "0    resnet  3b64b0be38fb8e94          32      30   \n",
       "1    resnet  9848e15a8cb9456b         256      30   \n",
       "2    resnet  e551b774a7b5ccc0         128      30   \n",
       "3    resnet  eeaea0a56be5c6c9          64      30   \n",
       "4    resnet  3b64b0be38fb8e94          32      30   \n",
       "..      ...               ...         ...     ...   \n",
       "103  resnet  4cf9aff8b0e3ec9e          64      30   \n",
       "104  resnet  9c49eeb8a706275a          64      30   \n",
       "105  resnet  7e945a487feff74e         128      30   \n",
       "106  resnet  afe503f94d4d8264          64      30   \n",
       "107  resnet  1d0577c2b0263cbe         256      30   \n",
       "\n",
       "                                        train_accuracy  \\\n",
       "0    [0.17032000422477722, 0.2808600068092346, 0.35...   \n",
       "1    [0.20927999913692474, 0.35989999771118164, 0.4...   \n",
       "2    [0.21724000573158264, 0.33597999811172485, 0.4...   \n",
       "3    [0.21930000185966492, 0.346560001373291, 0.435...   \n",
       "4    [0.17776000499725342, 0.25637999176979065, 0.3...   \n",
       "..                                                 ...   \n",
       "103                                                NaN   \n",
       "104                                                NaN   \n",
       "105                                                NaN   \n",
       "106                                                NaN   \n",
       "107                                                NaN   \n",
       "\n",
       "                                            train_loss  \\\n",
       "0    [2.386634588241577, 1.8890058994293213, 1.7486...   \n",
       "1    [2.9395194053649902, 1.7702909708023071, 1.570...   \n",
       "2    [2.6788625717163086, 1.7895755767822266, 1.611...   \n",
       "3    [2.442678451538086, 1.7620742321014404, 1.5449...   \n",
       "4    [2.443981170654297, 1.9975420236587524, 1.8435...   \n",
       "..                                                 ...   \n",
       "103  [3.360841138029304, 2.3875012396939574, 1.9948...   \n",
       "104  [2.712390104218212, 1.6115226571822105, 1.4244...   \n",
       "105  [3.2209879354174644, 1.9946543223091535, 1.743...   \n",
       "106  [3.297370414573881, 1.8065422634400665, 1.6034...   \n",
       "107  [3.2220769907747, 1.9234242487926871, 1.651999...   \n",
       "\n",
       "                                              accuracy  \\\n",
       "0    [0.2615000009536743, 0.3276999890804291, 0.395...   \n",
       "1    [0.14980000257492065, 0.17270000278949738, 0.3...   \n",
       "2    [0.23649999499320984, 0.2770000100135803, 0.44...   \n",
       "3    [0.20569999516010284, 0.38609999418258667, 0.4...   \n",
       "4    [0.2281000018119812, 0.31769999861717224, 0.39...   \n",
       "..                                                 ...   \n",
       "103  [10, 10.26, 26.85, 35.03, 40.550000000000004, ...   \n",
       "104  [37.430334394904456, 46.34753184713376, 52.478...   \n",
       "105  [22.34, 33.46, 41.81, 47.64, 48.33999999999999...   \n",
       "106  [31.22014331210191, 41.49084394904459, 42.8841...   \n",
       "107  [27.470703125, 36.1328125, 41.89453125, 45.966...   \n",
       "\n",
       "                                       validation_loss  \\\n",
       "0    [1.8862396478652954, 1.725894570350647, 1.6532...   \n",
       "1    [2.747114896774292, 2.5870492458343506, 1.9928...   \n",
       "2    [2.0208323001861572, 1.976898431777954, 1.5212...   \n",
       "3    [2.4119229316711426, 1.6152931451797485, 1.422...   \n",
       "4    [2.1482324600219727, 1.9316531419754028, 1.673...   \n",
       "..                                                 ...   \n",
       "103  [3.8288797876637455, 2.403817397014351, 1.9774...   \n",
       "104  [1.705808477037272, 1.4686595649476264, 1.3200...   \n",
       "105  [2.1471189894253695, 1.759268745591369, 1.5828...   \n",
       "106  [1.8421584147556571, 1.573136512640935, 1.5360...   \n",
       "107  [1.930098184943199, 1.718866342306137, 1.75740...   \n",
       "\n",
       "                                        epoch_duration        acc  ...  \\\n",
       "0    [86.82871699333191, 161.10458970069885, 235.44...   0.654200  ...   \n",
       "1    [26.537360668182373, 38.13520431518555, 49.665...   0.656100  ...   \n",
       "2    [33.218318939208984, 53.329391956329346, 73.47...   0.653100  ...   \n",
       "3    [51.08732867240906, 89.5283989906311, 127.9574...   0.658100  ...   \n",
       "4    [91.24218130111694, 167.12129759788513, 242.37...   0.682500  ...   \n",
       "..                                                 ...        ...  ...   \n",
       "103  [14.801709896, 31.24002004, 48.253630979, 64.1...  72.910000  ...   \n",
       "104  [38.899608234, 82.870589755, 126.082686868, 16...  69.725318  ...   \n",
       "105  [27.999510971, 57.792588364, 86.948315407, 115...  67.620000  ...   \n",
       "106  [31.308027951, 61.831227029, 90.963939092, 120...  70.989252  ...   \n",
       "107  [32.766331703, 66.356944417, 101.061452874, 13...  67.402344  ...   \n",
       "\n",
       "     dataset   lr  function_name default_parallelism static_parallelism  \\\n",
       "0        NaN  NaN            NaN                 NaN                NaN   \n",
       "1        NaN  NaN            NaN                 NaN                NaN   \n",
       "2        NaN  NaN            NaN                 NaN                NaN   \n",
       "3        NaN  NaN            NaN                 NaN                NaN   \n",
       "4        NaN  NaN            NaN                 NaN                NaN   \n",
       "..       ...  ...            ...                 ...                ...   \n",
       "103  cifar10  0.1         resnet                 4.0               True   \n",
       "104  cifar10  0.1         resnet                 2.0               True   \n",
       "105  cifar10  0.1         resnet                 4.0               True   \n",
       "106  cifar10  0.1         resnet                 4.0               True   \n",
       "107  cifar10  0.1         resnet                 2.0               True   \n",
       "\n",
       "    validate_every     k goal_accuracy  parallelism  global_batch  \n",
       "0              NaN   NaN           NaN          NaN           NaN  \n",
       "1              NaN   NaN           NaN          NaN           NaN  \n",
       "2              NaN   NaN           NaN          NaN           NaN  \n",
       "3              NaN   NaN           NaN          NaN           NaN  \n",
       "4              NaN   NaN           NaN          NaN           NaN  \n",
       "..             ...   ...           ...          ...           ...  \n",
       "103            1.0   inf         100.0          4.0         256.0  \n",
       "104            1.0  32.0         100.0          2.0         128.0  \n",
       "105            1.0  16.0         100.0          4.0         512.0  \n",
       "106            1.0  32.0         100.0          4.0         256.0  \n",
       "107            1.0   8.0         100.0          2.0         512.0  \n",
       "\n",
       "[108 rows x 40 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = pd.concat([resnet, kuberesnet], ignore_index=True)\n",
    "r\n",
    "# resnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Create new columns for representation\n",
    "\n",
    "- Final accuracy\n",
    "- Total time taken\n",
    "- Parallelism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Plot the Correlations between the K, Batch and Parallelism with time and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corr = df[['k', 'batch_size', 'parallelism', 'acc', 'time']].corr()\n",
    "sns.heatmap(corr,\n",
    "            annot=True,\n",
    ")\n",
    "\n",
    "# plt.savefig('./figures/resnet34/heat.png', dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df.sort_values('time')\n",
    "\n",
    "mean = df.groupby('hash').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Get the max accuracies and times and check the parameters used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# get the max accuracies\n",
    "df[['k', 'parallelism', 'acc','batch_size','time']].sort_values(by='time', ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Calculate TTA with different accuracies\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "s = df.sort_values('tta_cross_99')[['k', 'batch_size', 'parallelism', 'tta_cross_99', 'tta_99', 'acc', 'accuracy', 'epoch_duration']]\n",
    "\n",
    "# plot the best\n",
    "best = s.iloc[0]\n",
    "best\n",
    "\n",
    "\n",
    "x = range(1, len(best.accuracy)+1)\n",
    "plt.figure()\n",
    "plt.title(f'Best tta_99 (B={best.batch_size}, k={best.k}, P={best.parallelism})')\n",
    "sns.lineplot(x=best.epoch_duration, y = best.accuracy)\n",
    "sns.lineplot(x=best.epoch_duration, y= 99)\n",
    "plt.scatter(best.tta_cross_99, 99, marker='X', s=60, c='r')\n",
    "plt.xlabel('Time (s)', fontsize=15)\n",
    "plt.ylabel('Accuracy (%)', fontsize=15)\n",
    "\n",
    "# plt.savefig('./figures/gpu/best.png', dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>epochs</th>\n",
       "      <th>acc</th>\n",
       "      <th>time</th>\n",
       "      <th>tta_67</th>\n",
       "      <th>tta_cross_67</th>\n",
       "      <th>cpu_mean</th>\n",
       "      <th>gpu_0_mean_usage</th>\n",
       "      <th>gpu_1_mean_usage</th>\n",
       "      <th>gpu_0_mean_memory</th>\n",
       "      <th>gpu_1_mean_memory</th>\n",
       "      <th>gpu_usage</th>\n",
       "      <th>mem_mean</th>\n",
       "      <th>tta_69</th>\n",
       "      <th>tta_cross_69</th>\n",
       "      <th>lr</th>\n",
       "      <th>default_parallelism</th>\n",
       "      <th>validate_every</th>\n",
       "      <th>goal_accuracy</th>\n",
       "      <th>global_batch</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>batch_size</th>\n",
       "      <th>system</th>\n",
       "      <th>k</th>\n",
       "      <th>parallelism</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">32</th>\n",
       "      <th rowspan=\"8\" valign=\"top\">kubeml</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">8.0</th>\n",
       "      <th>2.0</th>\n",
       "      <td>30</td>\n",
       "      <td>68.947183</td>\n",
       "      <td>5923.931139</td>\n",
       "      <td>2881.126330</td>\n",
       "      <td>3473.112642</td>\n",
       "      <td>4.239913</td>\n",
       "      <td>0.115340</td>\n",
       "      <td>0.049599</td>\n",
       "      <td>2020.842444</td>\n",
       "      <td>872.712403</td>\n",
       "      <td>0.082470</td>\n",
       "      <td>7.716206</td>\n",
       "      <td>3876.004124</td>\n",
       "      <td>4367.454878</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>30</td>\n",
       "      <td>69.346150</td>\n",
       "      <td>4517.181873</td>\n",
       "      <td>1858.362935</td>\n",
       "      <td>2213.384787</td>\n",
       "      <td>4.947685</td>\n",
       "      <td>0.127413</td>\n",
       "      <td>0.122867</td>\n",
       "      <td>2759.478114</td>\n",
       "      <td>2984.310141</td>\n",
       "      <td>0.125140</td>\n",
       "      <td>9.476524</td>\n",
       "      <td>3602.101572</td>\n",
       "      <td>4051.582116</td>\n",
       "      <td>0.1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>128.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">16.0</th>\n",
       "      <th>2.0</th>\n",
       "      <td>30</td>\n",
       "      <td>67.657583</td>\n",
       "      <td>3400.371185</td>\n",
       "      <td>2417.470112</td>\n",
       "      <td>3057.576740</td>\n",
       "      <td>4.439632</td>\n",
       "      <td>0.189830</td>\n",
       "      <td>0.075637</td>\n",
       "      <td>2123.003923</td>\n",
       "      <td>857.614829</td>\n",
       "      <td>0.132734</td>\n",
       "      <td>7.693127</td>\n",
       "      <td>3153.281437</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>30</td>\n",
       "      <td>69.545948</td>\n",
       "      <td>2539.809555</td>\n",
       "      <td>1304.888241</td>\n",
       "      <td>1500.779184</td>\n",
       "      <td>5.617721</td>\n",
       "      <td>0.195410</td>\n",
       "      <td>0.192263</td>\n",
       "      <td>2933.913220</td>\n",
       "      <td>2941.718915</td>\n",
       "      <td>0.193836</td>\n",
       "      <td>9.727973</td>\n",
       "      <td>1953.288993</td>\n",
       "      <td>2058.124568</td>\n",
       "      <td>0.1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>128.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">32.0</th>\n",
       "      <th>2.0</th>\n",
       "      <td>30</td>\n",
       "      <td>69.352577</td>\n",
       "      <td>2161.323538</td>\n",
       "      <td>1058.124722</td>\n",
       "      <td>1275.394572</td>\n",
       "      <td>4.680143</td>\n",
       "      <td>0.294220</td>\n",
       "      <td>0.122141</td>\n",
       "      <td>2197.647307</td>\n",
       "      <td>890.759207</td>\n",
       "      <td>0.208180</td>\n",
       "      <td>7.687835</td>\n",
       "      <td>1593.550952</td>\n",
       "      <td>1810.827501</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>30</td>\n",
       "      <td>70.278312</td>\n",
       "      <td>1559.924027</td>\n",
       "      <td>708.630220</td>\n",
       "      <td>811.804426</td>\n",
       "      <td>6.270719</td>\n",
       "      <td>0.272724</td>\n",
       "      <td>0.303754</td>\n",
       "      <td>2912.313762</td>\n",
       "      <td>3147.511631</td>\n",
       "      <td>0.288239</td>\n",
       "      <td>9.791883</td>\n",
       "      <td>898.440860</td>\n",
       "      <td>1106.220076</td>\n",
       "      <td>0.1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>128.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">inf</th>\n",
       "      <th>2.0</th>\n",
       "      <td>30</td>\n",
       "      <td>73.212162</td>\n",
       "      <td>938.597728</td>\n",
       "      <td>383.010922</td>\n",
       "      <td>447.715970</td>\n",
       "      <td>5.705479</td>\n",
       "      <td>0.533680</td>\n",
       "      <td>0.306043</td>\n",
       "      <td>2246.284037</td>\n",
       "      <td>1277.606045</td>\n",
       "      <td>0.419861</td>\n",
       "      <td>7.908037</td>\n",
       "      <td>439.927944</td>\n",
       "      <td>515.473148</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>30</td>\n",
       "      <td>72.772508</td>\n",
       "      <td>660.344917</td>\n",
       "      <td>332.606673</td>\n",
       "      <td>379.706467</td>\n",
       "      <td>8.940485</td>\n",
       "      <td>0.632848</td>\n",
       "      <td>0.655991</td>\n",
       "      <td>3111.118672</td>\n",
       "      <td>3394.279023</td>\n",
       "      <td>0.644419</td>\n",
       "      <td>9.898967</td>\n",
       "      <td>386.470779</td>\n",
       "      <td>433.742163</td>\n",
       "      <td>0.1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>128.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">64</th>\n",
       "      <th rowspan=\"8\" valign=\"top\">kubeml</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">8.0</th>\n",
       "      <th>2.0</th>\n",
       "      <td>30</td>\n",
       "      <td>67.786502</td>\n",
       "      <td>3034.596671</td>\n",
       "      <td>2230.130607</td>\n",
       "      <td>2400.985173</td>\n",
       "      <td>4.170849</td>\n",
       "      <td>0.137543</td>\n",
       "      <td>0.052793</td>\n",
       "      <td>2192.059532</td>\n",
       "      <td>846.601945</td>\n",
       "      <td>0.095168</td>\n",
       "      <td>7.421509</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>128.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>30</td>\n",
       "      <td>68.512436</td>\n",
       "      <td>2372.274145</td>\n",
       "      <td>1278.153022</td>\n",
       "      <td>1461.038770</td>\n",
       "      <td>5.173503</td>\n",
       "      <td>0.134033</td>\n",
       "      <td>0.132637</td>\n",
       "      <td>2958.136559</td>\n",
       "      <td>3032.040704</td>\n",
       "      <td>0.133335</td>\n",
       "      <td>9.450940</td>\n",
       "      <td>1950.790920</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>256.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">16.0</th>\n",
       "      <th>2.0</th>\n",
       "      <td>30</td>\n",
       "      <td>69.796927</td>\n",
       "      <td>1873.111526</td>\n",
       "      <td>817.123322</td>\n",
       "      <td>1040.604199</td>\n",
       "      <td>4.533505</td>\n",
       "      <td>0.238244</td>\n",
       "      <td>0.055869</td>\n",
       "      <td>2548.438027</td>\n",
       "      <td>636.766035</td>\n",
       "      <td>0.147056</td>\n",
       "      <td>7.436015</td>\n",
       "      <td>1275.153679</td>\n",
       "      <td>1672.664227</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>128.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>30</td>\n",
       "      <td>68.894145</td>\n",
       "      <td>1379.382412</td>\n",
       "      <td>602.794919</td>\n",
       "      <td>802.239245</td>\n",
       "      <td>5.846716</td>\n",
       "      <td>0.207637</td>\n",
       "      <td>0.216608</td>\n",
       "      <td>2962.403526</td>\n",
       "      <td>3207.008330</td>\n",
       "      <td>0.212122</td>\n",
       "      <td>9.430619</td>\n",
       "      <td>1180.843523</td>\n",
       "      <td>1271.619655</td>\n",
       "      <td>0.1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>256.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">32.0</th>\n",
       "      <th>2.0</th>\n",
       "      <td>30</td>\n",
       "      <td>70.060706</td>\n",
       "      <td>1231.454501</td>\n",
       "      <td>528.128708</td>\n",
       "      <td>648.922469</td>\n",
       "      <td>4.746956</td>\n",
       "      <td>0.357685</td>\n",
       "      <td>0.089745</td>\n",
       "      <td>2662.525401</td>\n",
       "      <td>703.714088</td>\n",
       "      <td>0.223715</td>\n",
       "      <td>7.472947</td>\n",
       "      <td>831.112836</td>\n",
       "      <td>966.310691</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>128.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>30</td>\n",
       "      <td>70.428907</td>\n",
       "      <td>912.207939</td>\n",
       "      <td>435.278214</td>\n",
       "      <td>516.029984</td>\n",
       "      <td>6.762504</td>\n",
       "      <td>0.330906</td>\n",
       "      <td>0.275067</td>\n",
       "      <td>3515.265800</td>\n",
       "      <td>2930.299543</td>\n",
       "      <td>0.302986</td>\n",
       "      <td>9.643527</td>\n",
       "      <td>627.839136</td>\n",
       "      <td>708.969207</td>\n",
       "      <td>0.1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>256.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">inf</th>\n",
       "      <th>2.0</th>\n",
       "      <td>30</td>\n",
       "      <td>72.306380</td>\n",
       "      <td>618.092809</td>\n",
       "      <td>205.511294</td>\n",
       "      <td>252.242654</td>\n",
       "      <td>6.132659</td>\n",
       "      <td>0.457171</td>\n",
       "      <td>0.320025</td>\n",
       "      <td>2446.362674</td>\n",
       "      <td>1589.327154</td>\n",
       "      <td>0.388598</td>\n",
       "      <td>7.891078</td>\n",
       "      <td>265.603037</td>\n",
       "      <td>313.737996</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>128.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>30</td>\n",
       "      <td>72.969735</td>\n",
       "      <td>476.694337</td>\n",
       "      <td>234.962366</td>\n",
       "      <td>277.439108</td>\n",
       "      <td>9.617389</td>\n",
       "      <td>0.592493</td>\n",
       "      <td>0.507645</td>\n",
       "      <td>3901.034406</td>\n",
       "      <td>3300.002579</td>\n",
       "      <td>0.550069</td>\n",
       "      <td>10.091339</td>\n",
       "      <td>276.618464</td>\n",
       "      <td>309.842162</td>\n",
       "      <td>0.1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>256.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">128</th>\n",
       "      <th rowspan=\"8\" valign=\"top\">kubeml</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">8.0</th>\n",
       "      <th>2.0</th>\n",
       "      <td>30</td>\n",
       "      <td>67.949193</td>\n",
       "      <td>1658.000223</td>\n",
       "      <td>1028.047383</td>\n",
       "      <td>1361.420528</td>\n",
       "      <td>4.311303</td>\n",
       "      <td>0.155397</td>\n",
       "      <td>0.070899</td>\n",
       "      <td>2281.587145</td>\n",
       "      <td>1005.936492</td>\n",
       "      <td>0.113148</td>\n",
       "      <td>7.207255</td>\n",
       "      <td>1516.159982</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>256.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>30</td>\n",
       "      <td>67.543855</td>\n",
       "      <td>1329.524896</td>\n",
       "      <td>860.081136</td>\n",
       "      <td>1098.509059</td>\n",
       "      <td>6.079331</td>\n",
       "      <td>0.163253</td>\n",
       "      <td>0.153101</td>\n",
       "      <td>3348.248029</td>\n",
       "      <td>3145.426649</td>\n",
       "      <td>0.158177</td>\n",
       "      <td>9.352070</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>512.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">16.0</th>\n",
       "      <th>2.0</th>\n",
       "      <td>30</td>\n",
       "      <td>67.847199</td>\n",
       "      <td>1043.494251</td>\n",
       "      <td>742.621417</td>\n",
       "      <td>857.136080</td>\n",
       "      <td>4.663297</td>\n",
       "      <td>0.226404</td>\n",
       "      <td>0.113605</td>\n",
       "      <td>2402.018305</td>\n",
       "      <td>1137.370070</td>\n",
       "      <td>0.170005</td>\n",
       "      <td>7.307912</td>\n",
       "      <td>1036.021413</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>256.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>30</td>\n",
       "      <td>67.756904</td>\n",
       "      <td>826.509706</td>\n",
       "      <td>509.003978</td>\n",
       "      <td>653.674440</td>\n",
       "      <td>6.729271</td>\n",
       "      <td>0.249223</td>\n",
       "      <td>0.232237</td>\n",
       "      <td>3476.554553</td>\n",
       "      <td>3250.476954</td>\n",
       "      <td>0.240730</td>\n",
       "      <td>9.456752</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>512.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">32.0</th>\n",
       "      <th>2.0</th>\n",
       "      <td>30</td>\n",
       "      <td>68.823555</td>\n",
       "      <td>746.185348</td>\n",
       "      <td>434.428547</td>\n",
       "      <td>511.497045</td>\n",
       "      <td>5.019027</td>\n",
       "      <td>0.320070</td>\n",
       "      <td>0.135025</td>\n",
       "      <td>2722.828177</td>\n",
       "      <td>1065.434050</td>\n",
       "      <td>0.227548</td>\n",
       "      <td>7.387377</td>\n",
       "      <td>656.673349</td>\n",
       "      <td>733.743227</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>256.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>30</td>\n",
       "      <td>69.195079</td>\n",
       "      <td>603.920821</td>\n",
       "      <td>377.046772</td>\n",
       "      <td>416.870063</td>\n",
       "      <td>8.140851</td>\n",
       "      <td>0.314253</td>\n",
       "      <td>0.304274</td>\n",
       "      <td>3813.755865</td>\n",
       "      <td>3368.512131</td>\n",
       "      <td>0.309263</td>\n",
       "      <td>9.667072</td>\n",
       "      <td>523.827260</td>\n",
       "      <td>460.195127</td>\n",
       "      <td>0.1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>512.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">inf</th>\n",
       "      <th>2.0</th>\n",
       "      <td>30</td>\n",
       "      <td>70.696820</td>\n",
       "      <td>472.494852</td>\n",
       "      <td>175.715730</td>\n",
       "      <td>210.655472</td>\n",
       "      <td>5.998552</td>\n",
       "      <td>0.449263</td>\n",
       "      <td>0.276949</td>\n",
       "      <td>2806.799689</td>\n",
       "      <td>1604.278067</td>\n",
       "      <td>0.363106</td>\n",
       "      <td>7.737450</td>\n",
       "      <td>226.690396</td>\n",
       "      <td>267.942236</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>256.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>30</td>\n",
       "      <td>70.985158</td>\n",
       "      <td>325.926995</td>\n",
       "      <td>160.017986</td>\n",
       "      <td>181.691177</td>\n",
       "      <td>7.949603</td>\n",
       "      <td>0.554898</td>\n",
       "      <td>0.515668</td>\n",
       "      <td>3680.036927</td>\n",
       "      <td>3597.994239</td>\n",
       "      <td>0.535283</td>\n",
       "      <td>9.645527</td>\n",
       "      <td>187.670014</td>\n",
       "      <td>213.627789</td>\n",
       "      <td>0.1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>512.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">256</th>\n",
       "      <th rowspan=\"8\" valign=\"top\">kubeml</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">8.0</th>\n",
       "      <th>2.0</th>\n",
       "      <td>30</td>\n",
       "      <td>66.161797</td>\n",
       "      <td>1010.378293</td>\n",
       "      <td>903.922612</td>\n",
       "      <td>864.349527</td>\n",
       "      <td>4.483658</td>\n",
       "      <td>0.228119</td>\n",
       "      <td>0.070148</td>\n",
       "      <td>2933.070017</td>\n",
       "      <td>902.345289</td>\n",
       "      <td>0.149134</td>\n",
       "      <td>7.051384</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>512.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>30</td>\n",
       "      <td>64.550573</td>\n",
       "      <td>769.532170</td>\n",
       "      <td>780.824623</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.972017</td>\n",
       "      <td>0.208901</td>\n",
       "      <td>0.210933</td>\n",
       "      <td>3606.710192</td>\n",
       "      <td>3602.926345</td>\n",
       "      <td>0.209917</td>\n",
       "      <td>9.078223</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1024.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">16.0</th>\n",
       "      <th>2.0</th>\n",
       "      <td>30</td>\n",
       "      <td>66.510781</td>\n",
       "      <td>707.085116</td>\n",
       "      <td>514.809549</td>\n",
       "      <td>565.060399</td>\n",
       "      <td>4.733030</td>\n",
       "      <td>0.317595</td>\n",
       "      <td>0.095977</td>\n",
       "      <td>3197.629951</td>\n",
       "      <td>933.871487</td>\n",
       "      <td>0.206786</td>\n",
       "      <td>7.158824</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>512.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>30</td>\n",
       "      <td>65.775781</td>\n",
       "      <td>555.409487</td>\n",
       "      <td>426.252361</td>\n",
       "      <td>462.804594</td>\n",
       "      <td>7.712256</td>\n",
       "      <td>0.280692</td>\n",
       "      <td>0.295920</td>\n",
       "      <td>3899.665330</td>\n",
       "      <td>3977.750964</td>\n",
       "      <td>0.288306</td>\n",
       "      <td>9.492042</td>\n",
       "      <td>554.789812</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1024.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">32.0</th>\n",
       "      <th>2.0</th>\n",
       "      <td>30</td>\n",
       "      <td>65.296823</td>\n",
       "      <td>545.244145</td>\n",
       "      <td>358.938648</td>\n",
       "      <td>395.915646</td>\n",
       "      <td>5.045047</td>\n",
       "      <td>0.378432</td>\n",
       "      <td>0.154696</td>\n",
       "      <td>3274.180918</td>\n",
       "      <td>1135.677816</td>\n",
       "      <td>0.266564</td>\n",
       "      <td>7.299008</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>512.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>30</td>\n",
       "      <td>65.216875</td>\n",
       "      <td>381.095069</td>\n",
       "      <td>281.825391</td>\n",
       "      <td>357.378689</td>\n",
       "      <td>8.388168</td>\n",
       "      <td>0.397180</td>\n",
       "      <td>0.405915</td>\n",
       "      <td>4410.991196</td>\n",
       "      <td>3992.619670</td>\n",
       "      <td>0.401548</td>\n",
       "      <td>9.676466</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1024.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">inf</th>\n",
       "      <th>2.0</th>\n",
       "      <td>30</td>\n",
       "      <td>69.560859</td>\n",
       "      <td>408.100123</td>\n",
       "      <td>173.713432</td>\n",
       "      <td>201.738692</td>\n",
       "      <td>5.769889</td>\n",
       "      <td>0.440641</td>\n",
       "      <td>0.239986</td>\n",
       "      <td>3189.348040</td>\n",
       "      <td>1514.426000</td>\n",
       "      <td>0.340313</td>\n",
       "      <td>7.437468</td>\n",
       "      <td>262.852074</td>\n",
       "      <td>299.517968</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>512.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>30</td>\n",
       "      <td>66.348984</td>\n",
       "      <td>276.230160</td>\n",
       "      <td>235.753829</td>\n",
       "      <td>263.027773</td>\n",
       "      <td>6.885217</td>\n",
       "      <td>0.497205</td>\n",
       "      <td>0.556087</td>\n",
       "      <td>3724.766053</td>\n",
       "      <td>4022.205104</td>\n",
       "      <td>0.526646</td>\n",
       "      <td>9.063102</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1024.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    epochs        acc         time  \\\n",
       "batch_size system k    parallelism                                   \n",
       "32         kubeml 8.0  2.0              30  68.947183  5923.931139   \n",
       "                       4.0              30  69.346150  4517.181873   \n",
       "                  16.0 2.0              30  67.657583  3400.371185   \n",
       "                       4.0              30  69.545948  2539.809555   \n",
       "                  32.0 2.0              30  69.352577  2161.323538   \n",
       "                       4.0              30  70.278312  1559.924027   \n",
       "                  inf  2.0              30  73.212162   938.597728   \n",
       "                       4.0              30  72.772508   660.344917   \n",
       "64         kubeml 8.0  2.0              30  67.786502  3034.596671   \n",
       "                       4.0              30  68.512436  2372.274145   \n",
       "                  16.0 2.0              30  69.796927  1873.111526   \n",
       "                       4.0              30  68.894145  1379.382412   \n",
       "                  32.0 2.0              30  70.060706  1231.454501   \n",
       "                       4.0              30  70.428907   912.207939   \n",
       "                  inf  2.0              30  72.306380   618.092809   \n",
       "                       4.0              30  72.969735   476.694337   \n",
       "128        kubeml 8.0  2.0              30  67.949193  1658.000223   \n",
       "                       4.0              30  67.543855  1329.524896   \n",
       "                  16.0 2.0              30  67.847199  1043.494251   \n",
       "                       4.0              30  67.756904   826.509706   \n",
       "                  32.0 2.0              30  68.823555   746.185348   \n",
       "                       4.0              30  69.195079   603.920821   \n",
       "                  inf  2.0              30  70.696820   472.494852   \n",
       "                       4.0              30  70.985158   325.926995   \n",
       "256        kubeml 8.0  2.0              30  66.161797  1010.378293   \n",
       "                       4.0              30  64.550573   769.532170   \n",
       "                  16.0 2.0              30  66.510781   707.085116   \n",
       "                       4.0              30  65.775781   555.409487   \n",
       "                  32.0 2.0              30  65.296823   545.244145   \n",
       "                       4.0              30  65.216875   381.095069   \n",
       "                  inf  2.0              30  69.560859   408.100123   \n",
       "                       4.0              30  66.348984   276.230160   \n",
       "\n",
       "                                         tta_67  tta_cross_67  cpu_mean  \\\n",
       "batch_size system k    parallelism                                        \n",
       "32         kubeml 8.0  2.0          2881.126330   3473.112642  4.239913   \n",
       "                       4.0          1858.362935   2213.384787  4.947685   \n",
       "                  16.0 2.0          2417.470112   3057.576740  4.439632   \n",
       "                       4.0          1304.888241   1500.779184  5.617721   \n",
       "                  32.0 2.0          1058.124722   1275.394572  4.680143   \n",
       "                       4.0           708.630220    811.804426  6.270719   \n",
       "                  inf  2.0           383.010922    447.715970  5.705479   \n",
       "                       4.0           332.606673    379.706467  8.940485   \n",
       "64         kubeml 8.0  2.0          2230.130607   2400.985173  4.170849   \n",
       "                       4.0          1278.153022   1461.038770  5.173503   \n",
       "                  16.0 2.0           817.123322   1040.604199  4.533505   \n",
       "                       4.0           602.794919    802.239245  5.846716   \n",
       "                  32.0 2.0           528.128708    648.922469  4.746956   \n",
       "                       4.0           435.278214    516.029984  6.762504   \n",
       "                  inf  2.0           205.511294    252.242654  6.132659   \n",
       "                       4.0           234.962366    277.439108  9.617389   \n",
       "128        kubeml 8.0  2.0          1028.047383   1361.420528  4.311303   \n",
       "                       4.0           860.081136   1098.509059  6.079331   \n",
       "                  16.0 2.0           742.621417    857.136080  4.663297   \n",
       "                       4.0           509.003978    653.674440  6.729271   \n",
       "                  32.0 2.0           434.428547    511.497045  5.019027   \n",
       "                       4.0           377.046772    416.870063  8.140851   \n",
       "                  inf  2.0           175.715730    210.655472  5.998552   \n",
       "                       4.0           160.017986    181.691177  7.949603   \n",
       "256        kubeml 8.0  2.0           903.922612    864.349527  4.483658   \n",
       "                       4.0           780.824623           NaN  5.972017   \n",
       "                  16.0 2.0           514.809549    565.060399  4.733030   \n",
       "                       4.0           426.252361    462.804594  7.712256   \n",
       "                  32.0 2.0           358.938648    395.915646  5.045047   \n",
       "                       4.0           281.825391    357.378689  8.388168   \n",
       "                  inf  2.0           173.713432    201.738692  5.769889   \n",
       "                       4.0           235.753829    263.027773  6.885217   \n",
       "\n",
       "                                    gpu_0_mean_usage  gpu_1_mean_usage  \\\n",
       "batch_size system k    parallelism                                       \n",
       "32         kubeml 8.0  2.0                  0.115340          0.049599   \n",
       "                       4.0                  0.127413          0.122867   \n",
       "                  16.0 2.0                  0.189830          0.075637   \n",
       "                       4.0                  0.195410          0.192263   \n",
       "                  32.0 2.0                  0.294220          0.122141   \n",
       "                       4.0                  0.272724          0.303754   \n",
       "                  inf  2.0                  0.533680          0.306043   \n",
       "                       4.0                  0.632848          0.655991   \n",
       "64         kubeml 8.0  2.0                  0.137543          0.052793   \n",
       "                       4.0                  0.134033          0.132637   \n",
       "                  16.0 2.0                  0.238244          0.055869   \n",
       "                       4.0                  0.207637          0.216608   \n",
       "                  32.0 2.0                  0.357685          0.089745   \n",
       "                       4.0                  0.330906          0.275067   \n",
       "                  inf  2.0                  0.457171          0.320025   \n",
       "                       4.0                  0.592493          0.507645   \n",
       "128        kubeml 8.0  2.0                  0.155397          0.070899   \n",
       "                       4.0                  0.163253          0.153101   \n",
       "                  16.0 2.0                  0.226404          0.113605   \n",
       "                       4.0                  0.249223          0.232237   \n",
       "                  32.0 2.0                  0.320070          0.135025   \n",
       "                       4.0                  0.314253          0.304274   \n",
       "                  inf  2.0                  0.449263          0.276949   \n",
       "                       4.0                  0.554898          0.515668   \n",
       "256        kubeml 8.0  2.0                  0.228119          0.070148   \n",
       "                       4.0                  0.208901          0.210933   \n",
       "                  16.0 2.0                  0.317595          0.095977   \n",
       "                       4.0                  0.280692          0.295920   \n",
       "                  32.0 2.0                  0.378432          0.154696   \n",
       "                       4.0                  0.397180          0.405915   \n",
       "                  inf  2.0                  0.440641          0.239986   \n",
       "                       4.0                  0.497205          0.556087   \n",
       "\n",
       "                                    gpu_0_mean_memory  gpu_1_mean_memory  \\\n",
       "batch_size system k    parallelism                                         \n",
       "32         kubeml 8.0  2.0                2020.842444         872.712403   \n",
       "                       4.0                2759.478114        2984.310141   \n",
       "                  16.0 2.0                2123.003923         857.614829   \n",
       "                       4.0                2933.913220        2941.718915   \n",
       "                  32.0 2.0                2197.647307         890.759207   \n",
       "                       4.0                2912.313762        3147.511631   \n",
       "                  inf  2.0                2246.284037        1277.606045   \n",
       "                       4.0                3111.118672        3394.279023   \n",
       "64         kubeml 8.0  2.0                2192.059532         846.601945   \n",
       "                       4.0                2958.136559        3032.040704   \n",
       "                  16.0 2.0                2548.438027         636.766035   \n",
       "                       4.0                2962.403526        3207.008330   \n",
       "                  32.0 2.0                2662.525401         703.714088   \n",
       "                       4.0                3515.265800        2930.299543   \n",
       "                  inf  2.0                2446.362674        1589.327154   \n",
       "                       4.0                3901.034406        3300.002579   \n",
       "128        kubeml 8.0  2.0                2281.587145        1005.936492   \n",
       "                       4.0                3348.248029        3145.426649   \n",
       "                  16.0 2.0                2402.018305        1137.370070   \n",
       "                       4.0                3476.554553        3250.476954   \n",
       "                  32.0 2.0                2722.828177        1065.434050   \n",
       "                       4.0                3813.755865        3368.512131   \n",
       "                  inf  2.0                2806.799689        1604.278067   \n",
       "                       4.0                3680.036927        3597.994239   \n",
       "256        kubeml 8.0  2.0                2933.070017         902.345289   \n",
       "                       4.0                3606.710192        3602.926345   \n",
       "                  16.0 2.0                3197.629951         933.871487   \n",
       "                       4.0                3899.665330        3977.750964   \n",
       "                  32.0 2.0                3274.180918        1135.677816   \n",
       "                       4.0                4410.991196        3992.619670   \n",
       "                  inf  2.0                3189.348040        1514.426000   \n",
       "                       4.0                3724.766053        4022.205104   \n",
       "\n",
       "                                    gpu_usage   mem_mean       tta_69  \\\n",
       "batch_size system k    parallelism                                      \n",
       "32         kubeml 8.0  2.0           0.082470   7.716206  3876.004124   \n",
       "                       4.0           0.125140   9.476524  3602.101572   \n",
       "                  16.0 2.0           0.132734   7.693127  3153.281437   \n",
       "                       4.0           0.193836   9.727973  1953.288993   \n",
       "                  32.0 2.0           0.208180   7.687835  1593.550952   \n",
       "                       4.0           0.288239   9.791883   898.440860   \n",
       "                  inf  2.0           0.419861   7.908037   439.927944   \n",
       "                       4.0           0.644419   9.898967   386.470779   \n",
       "64         kubeml 8.0  2.0           0.095168   7.421509          NaN   \n",
       "                       4.0           0.133335   9.450940  1950.790920   \n",
       "                  16.0 2.0           0.147056   7.436015  1275.153679   \n",
       "                       4.0           0.212122   9.430619  1180.843523   \n",
       "                  32.0 2.0           0.223715   7.472947   831.112836   \n",
       "                       4.0           0.302986   9.643527   627.839136   \n",
       "                  inf  2.0           0.388598   7.891078   265.603037   \n",
       "                       4.0           0.550069  10.091339   276.618464   \n",
       "128        kubeml 8.0  2.0           0.113148   7.207255  1516.159982   \n",
       "                       4.0           0.158177   9.352070          NaN   \n",
       "                  16.0 2.0           0.170005   7.307912  1036.021413   \n",
       "                       4.0           0.240730   9.456752          NaN   \n",
       "                  32.0 2.0           0.227548   7.387377   656.673349   \n",
       "                       4.0           0.309263   9.667072   523.827260   \n",
       "                  inf  2.0           0.363106   7.737450   226.690396   \n",
       "                       4.0           0.535283   9.645527   187.670014   \n",
       "256        kubeml 8.0  2.0           0.149134   7.051384          NaN   \n",
       "                       4.0           0.209917   9.078223          NaN   \n",
       "                  16.0 2.0           0.206786   7.158824          NaN   \n",
       "                       4.0           0.288306   9.492042   554.789812   \n",
       "                  32.0 2.0           0.266564   7.299008          NaN   \n",
       "                       4.0           0.401548   9.676466          NaN   \n",
       "                  inf  2.0           0.340313   7.437468   262.852074   \n",
       "                       4.0           0.526646   9.063102          NaN   \n",
       "\n",
       "                                    tta_cross_69   lr  default_parallelism  \\\n",
       "batch_size system k    parallelism                                           \n",
       "32         kubeml 8.0  2.0           4367.454878  0.1                  2.0   \n",
       "                       4.0           4051.582116  0.1                  4.0   \n",
       "                  16.0 2.0                   NaN  0.1                  2.0   \n",
       "                       4.0           2058.124568  0.1                  4.0   \n",
       "                  32.0 2.0           1810.827501  0.1                  2.0   \n",
       "                       4.0           1106.220076  0.1                  4.0   \n",
       "                  inf  2.0            515.473148  0.1                  2.0   \n",
       "                       4.0            433.742163  0.1                  4.0   \n",
       "64         kubeml 8.0  2.0                   NaN  0.1                  2.0   \n",
       "                       4.0                   NaN  0.1                  4.0   \n",
       "                  16.0 2.0           1672.664227  0.1                  2.0   \n",
       "                       4.0           1271.619655  0.1                  4.0   \n",
       "                  32.0 2.0            966.310691  0.1                  2.0   \n",
       "                       4.0            708.969207  0.1                  4.0   \n",
       "                  inf  2.0            313.737996  0.1                  2.0   \n",
       "                       4.0            309.842162  0.1                  4.0   \n",
       "128        kubeml 8.0  2.0                   NaN  0.1                  2.0   \n",
       "                       4.0                   NaN  0.1                  4.0   \n",
       "                  16.0 2.0                   NaN  0.1                  2.0   \n",
       "                       4.0                   NaN  0.1                  4.0   \n",
       "                  32.0 2.0            733.743227  0.1                  2.0   \n",
       "                       4.0            460.195127  0.1                  4.0   \n",
       "                  inf  2.0            267.942236  0.1                  2.0   \n",
       "                       4.0            213.627789  0.1                  4.0   \n",
       "256        kubeml 8.0  2.0                   NaN  0.1                  2.0   \n",
       "                       4.0                   NaN  0.1                  4.0   \n",
       "                  16.0 2.0                   NaN  0.1                  2.0   \n",
       "                       4.0                   NaN  0.1                  4.0   \n",
       "                  32.0 2.0                   NaN  0.1                  2.0   \n",
       "                       4.0                   NaN  0.1                  4.0   \n",
       "                  inf  2.0            299.517968  0.1                  2.0   \n",
       "                       4.0                   NaN  0.1                  4.0   \n",
       "\n",
       "                                    validate_every  goal_accuracy  \\\n",
       "batch_size system k    parallelism                                  \n",
       "32         kubeml 8.0  2.0                     1.0          100.0   \n",
       "                       4.0                     1.0          100.0   \n",
       "                  16.0 2.0                     1.0          100.0   \n",
       "                       4.0                     1.0          100.0   \n",
       "                  32.0 2.0                     1.0          100.0   \n",
       "                       4.0                     1.0          100.0   \n",
       "                  inf  2.0                     1.0          100.0   \n",
       "                       4.0                     1.0          100.0   \n",
       "64         kubeml 8.0  2.0                     1.0          100.0   \n",
       "                       4.0                     1.0          100.0   \n",
       "                  16.0 2.0                     1.0          100.0   \n",
       "                       4.0                     1.0          100.0   \n",
       "                  32.0 2.0                     1.0          100.0   \n",
       "                       4.0                     1.0          100.0   \n",
       "                  inf  2.0                     1.0          100.0   \n",
       "                       4.0                     1.0          100.0   \n",
       "128        kubeml 8.0  2.0                     1.0          100.0   \n",
       "                       4.0                     1.0          100.0   \n",
       "                  16.0 2.0                     1.0          100.0   \n",
       "                       4.0                     1.0          100.0   \n",
       "                  32.0 2.0                     1.0          100.0   \n",
       "                       4.0                     1.0          100.0   \n",
       "                  inf  2.0                     1.0          100.0   \n",
       "                       4.0                     1.0          100.0   \n",
       "256        kubeml 8.0  2.0                     1.0          100.0   \n",
       "                       4.0                     1.0          100.0   \n",
       "                  16.0 2.0                     1.0          100.0   \n",
       "                       4.0                     1.0          100.0   \n",
       "                  32.0 2.0                     1.0          100.0   \n",
       "                       4.0                     1.0          100.0   \n",
       "                  inf  2.0                     1.0          100.0   \n",
       "                       4.0                     1.0          100.0   \n",
       "\n",
       "                                    global_batch  \n",
       "batch_size system k    parallelism                \n",
       "32         kubeml 8.0  2.0                  64.0  \n",
       "                       4.0                 128.0  \n",
       "                  16.0 2.0                  64.0  \n",
       "                       4.0                 128.0  \n",
       "                  32.0 2.0                  64.0  \n",
       "                       4.0                 128.0  \n",
       "                  inf  2.0                  64.0  \n",
       "                       4.0                 128.0  \n",
       "64         kubeml 8.0  2.0                 128.0  \n",
       "                       4.0                 256.0  \n",
       "                  16.0 2.0                 128.0  \n",
       "                       4.0                 256.0  \n",
       "                  32.0 2.0                 128.0  \n",
       "                       4.0                 256.0  \n",
       "                  inf  2.0                 128.0  \n",
       "                       4.0                 256.0  \n",
       "128        kubeml 8.0  2.0                 256.0  \n",
       "                       4.0                 512.0  \n",
       "                  16.0 2.0                 256.0  \n",
       "                       4.0                 512.0  \n",
       "                  32.0 2.0                 256.0  \n",
       "                       4.0                 512.0  \n",
       "                  inf  2.0                 256.0  \n",
       "                       4.0                 512.0  \n",
       "256        kubeml 8.0  2.0                 512.0  \n",
       "                       4.0                1024.0  \n",
       "                  16.0 2.0                 512.0  \n",
       "                       4.0                1024.0  \n",
       "                  32.0 2.0                 512.0  \n",
       "                       4.0                1024.0  \n",
       "                  inf  2.0                 512.0  \n",
       "                       4.0                1024.0  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.groupby(['batch_size', 'system', 'k', 'parallelism']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get only the results with k=-1 and \n",
    "_r = r[((r.parallelism==4) | (r.parallelism.isna())) & ((r.k==float('inf')) | (r.k.isna())) ]\n",
    "\n",
    "f, ax = plt.subplots(1, 3, sharey=True, figsize=(20, 5))\n",
    "sns.barplot(x='batch_size', y='tta_67', data=_r, hue='system', ax=ax[0])\n",
    "sns.barplot(x='batch_size', y='tta_cross_67', data=_r, hue='system', ax=ax[1])\n",
    "# sns.barplot(x='batch_size', y='gpu_usage', data=df ,ax=ax[2])\n",
    "sns.despine()\n",
    "\n",
    "# plt.savefig('./figures/gpu/tta_99.png', dpi=300)\n",
    "\n",
    "# sns.barplot(x='k', y='tta_99', data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do ANOVA Linear Model to calculate the influence of the parameters\n",
    "\n",
    "Using ANOVA we can get an idea of how the different parameters interact with each other and their influence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the ANOVA test\n",
    "import researchpy as rp\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ANOVA(df: pd.DataFrame, y: str, use_all = False,verbose=False):\n",
    "    \"\"\"Run the ANOVA analysis with the batch, k and parallelism columns for the \n",
    "    given output variable\"\"\"\n",
    "    \n",
    "    # If use all is true we use all the variables to check either accuracy and time\n",
    "    # including also the iowait and the cpu to see what fully influences the stuff\n",
    "    \n",
    "    \n",
    "    if not use_all:\n",
    "        # Plot the summary dataframe\n",
    "        if verbose:\n",
    "            display(rp.summary_cont(df.groupby(['batch_size', 'k', 'parallelism']))[y])\n",
    "\n",
    "        model = ols(f'{y} ~ batch_size*k*parallelism', df).fit()\n",
    "        \n",
    "    else:\n",
    "        if y not in ['acc', 'time']:\n",
    "            raise ValueError('When use_all = True we predict either final_accuracy or time, not', y)\n",
    "        if verbose:\n",
    "            display(rp.summary_cont(df.groupby(['batch_size', 'k', 'parallelism']))[y])\n",
    "\n",
    "        model = ols(f'{y} ~ cpu*batch*njobs*cpu_mean*iowait_mean', df).fit()\n",
    "        \n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Overall model F({model.df_model: .0f},{model.df_resid: .0f}) = {model.fvalue: .3f}, p = {model.f_pvalue: .4f}\")\n",
    "        display(model.summary())\n",
    "    \n",
    "    res = sm.stats.anova_lm(model, typ=2)\n",
    "    \n",
    "    return res, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# df.k = df.k.map(lambda val: -1 if val == float('inf') else val)\n",
    "\n",
    "res, model = ANOVA(d, y='gpu_usage', verbose=True)\n",
    "\n",
    "res\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the distributions of time and accuracy as a function of K, Batch and parallelism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Plot the accuracy as a factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.rc('font', size=16)\n",
    "\n",
    "f, ax = plt.subplots(1, 3, sharey=True, figsize=(20, 5))\n",
    "sns.barplot(x='batch_size', y='acc', hue='k', data=df, ax=ax[0], capsize=.05)\n",
    "sns.barplot(x='k', y='acc', data=df, ax=ax[1], capsize=.05, hue='parallelism')\n",
    "sns.barplot(x='parallelism', y='acc', data=df, hue='k' ,ax=ax[2] ,capsize=.05)\n",
    "sns.despine()\n",
    "plt.legend(title='k', ncol=4, bbox_to_anchor=(0.075,1))\n",
    "\n",
    "for a in ax:\n",
    "    a.set_ylim([75, 100])\n",
    "\n",
    "\n",
    "\n",
    "# plt.savefig('./figures/resnet34/accuracy.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.rc('font', size=16)\n",
    "plt.figure(figsize=(15,10))\n",
    "sns.barplot(x='parallelism', y='tta_cross_99', data=df, capsize=.02, hue='batch_size')\n",
    "\n",
    "\n",
    "# plt.savefig('./figures/resnet34/acc_per_k_and_parallelism.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_pickle('./dataframes/lenet_tensorflow.pkl')\n",
    "d = pd.read_pickle('./dataframes/lenet_kubeml.pkl')\n",
    "d = d.loc[d.parallelism==2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.sort_values('tta_cross_99')[['batch_size', 'tta_cross_99']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Plot the time as a factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, 3, sharey=True, figsize=(20, 5))\n",
    "\n",
    "\n",
    "sns.barplot(x='batch_size', \n",
    "            y='tta_cross_99', \n",
    "            data=d ,\n",
    "            ax=ax[0],\n",
    "            estimator=np.min)\n",
    "\n",
    "sns.barplot(x='batch_size', y='tta_cross_99', data=df, ax=ax[1], estimator=np.min)\n",
    "# sns.barplot(x='parallelism', y='time', data=df, ax=ax[2], hue='k')\n",
    "\n",
    "# plt.savefig('./figures/resnet34/time.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rc('font', size=16)\n",
    "f, ax = plt.subplots(1, 3, figsize=(20,8), sharey=True)\n",
    "sns.barplot(x='k', y='time', data=df.loc[df.batch_size==32], capsize=.05, hue='parallelism', ax=ax[0])\n",
    "sns.barplot(x='k', y='time', data=df.loc[df.batch_size==64], capsize=.05, hue='parallelism', ax=ax[1])\n",
    "sns.barplot(x='k', y='time', data=df.loc[df.batch_size==128], capsize=.05, hue='parallelism', ax=ax[2])\n",
    "\n",
    "plt.savefig('./figures/resnet34/time_per_all.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the validation lines of k=-1 and batch = 32 with different parallelism\n",
    "def plot_loss_with_k_and_batch(k: int, batch:int, ax: plt.Axes = None):\n",
    "    d = df.loc[(df.k==k) & (df.batch_size==batch)].sort_values(by='parallelism', ascending=False)\n",
    "\n",
    "    plt.rc('font', size=13)\n",
    "    if ax is None:\n",
    "        f = plt.figure(figsize=(10, 5))\n",
    "        for _, row in d.iterrows():\n",
    "#             print(row.accuracy)\n",
    "            plt.plot(range(1,6), row.accuracy[:-1] if len(row.accuracy) == 6 else row.accuracy, label=str(row.parallelism))\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy (%)')\n",
    "        plt.title(f'Accuracy evolution with LeNet (batch={batch}, k={k})')\n",
    "        plt.legend(title='parallelism', bbox_to_anchor=(1.05, 0.8))\n",
    "        \n",
    "    else:\n",
    "        for _, row in d.iterrows():\n",
    "#             print(row.accuracy)\n",
    "            ax.plot(range(1,6), row.accuracy[:-1] if len(row.accuracy) == 6 else row.accuracy, label=str(row.parallelism))\n",
    "        ax.set_xlabel('Epoch')\n",
    "        ax.set_ylabel('Accuracy (%)')\n",
    "        ax.set_title(f'Batch={batch}, k={k}')\n",
    "        ax.legend(title='parallelism')\n",
    "        \n",
    "\n",
    "f, axes = plt.subplots(nrows=3, ncols=3, figsize=(20, 15), sharex=True)\n",
    "\n",
    "plt.suptitle('Behavior of K, Parallelism and Batch in Accuracy')\n",
    "\n",
    "plot_loss_with_k_and_batch(k=8, batch=32, ax=axes[0][0])\n",
    "plot_loss_with_k_and_batch(k=16, batch=32, ax=axes[0][1])   \n",
    "plot_loss_with_k_and_batch(k=64, batch=32, ax=axes[0][2])\n",
    "    \n",
    "plot_loss_with_k_and_batch(k=8, batch=64, ax=axes[1][0])\n",
    "plot_loss_with_k_and_batch(k=16, batch=64, ax=axes[1][1])   \n",
    "plot_loss_with_k_and_batch(k=64, batch=64, ax=axes[1][2])\n",
    "\n",
    "plot_loss_with_k_and_batch(k=8, batch=128, ax=axes[2][0])\n",
    "plot_loss_with_k_and_batch(k=16, batch=128, ax=axes[2][1])   \n",
    "plot_loss_with_k_and_batch(k=64, batch=128, ax=axes[2][2])\n",
    "\n",
    "\n",
    "# plt.savefig('./figures/accuracy_study.png', dpi=300)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Keep K and Batch set, vary parallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the validation lines of k=-1 and batch = 32 with different parallelism\n",
    "def plot_loss_with_k_and_batch(k: int, batch:int, ax: plt.Axes = None):\n",
    "    d = df.loc[(df.k==k) & (df.batch_size==batch)].sort_values(by='parallelism', ascending=False)\n",
    "\n",
    "    plt.rc('font', size=13)\n",
    "    if ax is None:\n",
    "        f = plt.figure(figsize=(10, 5))\n",
    "        for _, row in d.iterrows():\n",
    "#             print(row.accuracy)\n",
    "            plt.plot(range(1,6), row.accuracy[:-1] if len(row.accuracy) == 6 else row.accuracy, label=str(row.parallelism))\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy (%)')\n",
    "        plt.title(f'Accuracy evolution with LeNet (batch={batch}, k={k})')\n",
    "        plt.legend(title='parallelism', bbox_to_anchor=(1.05, 0.8))\n",
    "        \n",
    "    else:\n",
    "        for _, row in d.iterrows():\n",
    "#             print(row.accuracy)\n",
    "            ax.plot(range(1,6), row.accuracy[:-1] if len(row.accuracy) == 6 else row.accuracy, label=str(row.parallelism))\n",
    "        ax.set_xlabel('Epoch')\n",
    "        ax.set_ylabel('Accuracy (%)')\n",
    "        ax.set_title(f'Batch={batch}, k={k}')\n",
    "        ax.legend(title='parallelism')\n",
    "        \n",
    "\n",
    "f, axes = plt.subplots(nrows=3, ncols=3, figsize=(20, 15), sharex=True)\n",
    "\n",
    "plt.suptitle('Behavior of K, Parallelism and Batch in Accuracy')\n",
    "\n",
    "plot_loss_with_k_and_batch(k=8, batch=32, ax=axes[0][0])\n",
    "plot_loss_with_k_and_batch(k=16, batch=32, ax=axes[0][1])   \n",
    "plot_loss_with_k_and_batch(k=64, batch=32, ax=axes[0][2])\n",
    "    \n",
    "plot_loss_with_k_and_batch(k=8, batch=64, ax=axes[1][0])\n",
    "plot_loss_with_k_and_batch(k=16, batch=64, ax=axes[1][1])   \n",
    "plot_loss_with_k_and_batch(k=64, batch=64, ax=axes[1][2])\n",
    "\n",
    "plot_loss_with_k_and_batch(k=8, batch=128, ax=axes[2][0])\n",
    "plot_loss_with_k_and_batch(k=16, batch=128, ax=axes[2][1])   \n",
    "plot_loss_with_k_and_batch(k=64, batch=128, ax=axes[2][2])\n",
    "\n",
    "\n",
    "# plt.savefig('./figures/accuracy_study.png', dpi=300)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Keep Parallelism and batch set, vary K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the validation lines of k=-1 and batch = 32 with different parallelism\n",
    "def plot_loss_with_parallelism_and_batch(p: int, batch:int, ax: plt.Axes = None):\n",
    "    d = df.loc[(df.parallelism==p) & (df.batch_size==batch)].sort_values(by='k', ascending=False)\n",
    "    \n",
    "    approx_k = (60000/p)/batch\n",
    "\n",
    "    plt.rc('font', size=16)\n",
    "    if ax is None:\n",
    "        f = plt.figure(figsize=(10, 5))\n",
    "        for _, row in d.iterrows():\n",
    "#             print(row.accuracy)\n",
    "            plt.plot(range(1,6), row.accuracy[:-1] if len(row.accuracy) == 6 else row.accuracy, label=str(row.k))\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy (%)')\n",
    "        plt.title(f'Accuracy evolution with LeNet (batch={batch}, k={k})')\n",
    "        plt.legend(title='parallelism', bbox_to_anchor=(1.05, 0.8))\n",
    "        \n",
    "    else:\n",
    "        for _, row in d.iterrows():\n",
    "            label = str(row.k) if row.k != float('inf') else f'{row.k} ({int(approx_k)})'\n",
    "            ax.plot(range(1,6), row.accuracy[:-1] if len(row.accuracy) == 6 else row.accuracy, label=label)\n",
    "        ax.set_xlabel('Epoch')\n",
    "        ax.set_ylabel('Accuracy (%)')\n",
    "        ax.set_title(f'Batch={batch}, Parallelism={p}')\n",
    "        ax.legend(title='k')\n",
    "        \n",
    "\n",
    "f, axes = plt.subplots(nrows=3, ncols=3, figsize=(30, 20), sharex=True)\n",
    "\n",
    "plt.suptitle('Behavior of K, Parallelism and Batch in Accuracy')\n",
    "\n",
    "plot_loss_with_parallelism_and_batch(p=2, batch=32, ax=axes[0][0])\n",
    "plot_loss_with_parallelism_and_batch(p=4, batch=32, ax=axes[0][1])   \n",
    "plot_loss_with_parallelism_and_batch(p=8, batch=32, ax=axes[0][2])\n",
    "\n",
    "plot_loss_with_parallelism_and_batch(p=2, batch=64, ax=axes[1][0])\n",
    "plot_loss_with_parallelism_and_batch(p=4, batch=64, ax=axes[1][1])   \n",
    "plot_loss_with_parallelism_and_batch(p=8, batch=64, ax=axes[1][2])\n",
    "\n",
    "plot_loss_with_parallelism_and_batch(p=2, batch=128, ax=axes[2][0])\n",
    "plot_loss_with_parallelism_and_batch(p=4, batch=128, ax=axes[2][1])   \n",
    "plot_loss_with_parallelism_and_batch(p=8, batch=128, ax=axes[2][2])\n",
    "\n",
    "\n",
    "plt.savefig('./figures/accuracy_study_varying_k.png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Plot 3d dependencies between K and parallelism on time and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# f, ax = plt.subplots(1, 2, projection='3d')\n",
    "\n",
    "f = plt.figure()\n",
    "ax = f.add_subplot(1, 2, 1, projection='3d')\n",
    "\n",
    "from scipy.interpolate import griddata\n",
    "X, Y = np.meshgrid(df.k.map(lambda v: 500 if v == float('inf') else v), df.parallelism)\n",
    "Z = griddata((df.k.map(lambda v: 500 if v == float('inf') else v),\n",
    "              df.parallelism),\n",
    "              df.acc, (X, Y), method='cubic')\n",
    "\n",
    "ax.plot_surface(X, Y, Z, rstride=1, cstride=1, cmap='coolwarm',\n",
    "                       linewidth=0, antialiased=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "d = df.loc[df.batch==64]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
