{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usage of mongoDB to save the datasets in numpy style\n",
    "\n",
    "Save the datasets (in this case we'll start with MNIST) in npy style and divide it in many minibatches so they are easier to handle.\n",
    "\n",
    "These are put into mongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\diego\\cs\\thesis\\venv\\lib\\site-packages\\numpy\\_distributor_init.py:32: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\users\\diego\\cs\\thesis\\venv\\lib\\site-packages\\numpy\\.libs\\libopenblas.NOIJJG62EMASZI6NYURL6JBKM4EVBGM7.gfortran-win_amd64.dll\n",
      "c:\\users\\diego\\cs\\thesis\\venv\\lib\\site-packages\\numpy\\.libs\\libopenblas.PYQHXLVVQ7VESDPUVUADXEVJOBGHJPAY.gfortran-win_amd64.dll\n",
      "  stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import pymongo\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import torchvision\n",
    "import torch.utils.data as tdata\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# keras is better to save the dataset\n",
    "# from keras.datasets import mnist\n",
    "# from keras.utils import np_utils\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "# Ip of minikube and the port of the mongo service\n",
    "MONGO_IP = '192.168.99.101'\n",
    "MONGO_PORT = 30933"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the pytorch dataset to save the data to mongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO max document size is 16 MB, this could give us problems in the future\n",
    "# when the datasets are so big, we should calculate the size (easy, and divide the dataset)\n",
    "def split_dataset(X, Y, subsets):\n",
    "    \"\"\"Splits the X and Y in N different subsets\"\"\"\n",
    "    X_split = np.split(X, subsets)\n",
    "    Y_split = np.split(Y, subsets)\n",
    "    \n",
    "    return X_split, Y_split\n",
    "\n",
    "def dataset_splits(data, labels, batch_size):\n",
    "    \"\"\" Given the data, return constantly sized\n",
    "    batches of the dataset, which will be saved to the\n",
    "    database\"\"\"\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        yield data[i:i + batch_size], labels[i:i + batch_size]\n",
    "\n",
    "\n",
    "def approx_size(a: np.array):\n",
    "    \"\"\" approx size of float 32 array in MB\"\"\"\n",
    "    return (32/8) * np.prod(a.shape) / 1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = datasets.MNIST('../redisai/data', train=True, download=False)\n",
    "val_data = datasets.MNIST('../redisai/data', train=False, download=False)\n",
    "\n",
    "# subsample the datasets \n",
    "# train_data.data, train_data.targets = train_data.data[:3000], train_data.targets[:3000]\n",
    "# val_data.data, val_data.targets = val_data.data[:2000], val_data.targets[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide the training data in 100 and the testing data in 10\n",
    "xtr, ytr = split_dataset(train_data.data, train_data.targets, 100)\n",
    "xtest, ytest = split_dataset(val_data.data, val_data.targets, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the training data shorter\n",
    "xtr, ytr = xtr[:10], ytr[:10]\n",
    "xtest, ytest = xtest[:2], ytest[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1000, 1000]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(t) for t in ytest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = dataset_splits(train_data.data, train_data.targets, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = next(tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('test.npy', val_data.data.numpy()[:2000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the Keras Dataset and analyze format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((trainX, trainY), (testX, testY)) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the datasets are of size (60000, 28, 28), for pytorch it expects tensors of shape (channels, 28, 28) unlike tensorflow \n",
    "# which expects them at the end\n",
    "trainX = trainX.reshape(trainX.shape[0], 1, 28, 28)\n",
    "testX = testX.reshape(testX.shape[0], 1, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a couple of images\n",
    "f, (ax1, ax2) = plt.subplots(1,2, sharey=True, figsize=(10,5))\n",
    "ax1.imshow(trainX[0].squeeze()) # Squeeze gets rid of that extra dimension at the beginning\n",
    "ax2.imshow(trainX[1].squeeze())\n",
    "print(f'Labels are (ax1)={trainY[0]} (ax2)={trainY[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the images to the interval [0,1]\n",
    "trainX = trainX.astype('float32') /255\n",
    "testX = testX.astype('float32') /255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the labels to categorical\n",
    "trainY = np_utils.to_categorical(trainY)\n",
    "testY = np_utils.to_categorical(testY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the dataset in N different subsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the mongo conn and how to get the db names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "client : pymongo.MongoClient = pymongo.MongoClient(MONGO_IP, MONGO_PORT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['admin', 'config', 'local']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.list_database_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtr, ytr = split_dataset(trainX, trainY, 100)\n",
    "xtest, ytest = split_dataset(testX, testY, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the training data shorter\n",
    "xtr, ytr = xtr[:10], ytr[:10]\n",
    "xtest, ytest = xtest[:2], ytest[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x.shape for x in ytest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mnist.train'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db['train'].full_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get a Connection to Mongo and connect to the database\n",
    "\n",
    "We'll create a dataset database and one collection will be MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO maybe create a database MNIST and a collection for train and another for test\n",
    "# that would allow us to skip the train part of it\n",
    "DB_NAME = 'mnist'\n",
    "COLLECTION_NAME = 'mnist'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.train.drop()\n",
    "db.test.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train', 'test']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Connect to the mongo database and create the mnist collection if we do not have it already\n",
    "client : pymongo.MongoClient = pymongo.MongoClient(MONGO_IP, MONGO_PORT)\n",
    "db = client[DB_NAME]\n",
    "db.create_collection('train')\n",
    "db.create_collection('test')\n",
    "db.list_collection_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Insert the training data\n",
    "db['train'].insert_many([\n",
    "    {'_id': i,\n",
    "     'data': pickle.dumps(data, pickle.HIGHEST_PROTOCOL), \n",
    "     'labels':pickle.dumps(labels, pickle.HIGHEST_PROTOCOL)} for i, (data, labels) in enumerate(zip(xtr, ytr))]).inserted_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Insert the test data\n",
    "# Insert the training data\n",
    "db['test'].insert_many([\n",
    "    {'_id': i,\n",
    "     'data':pickle.dumps(data, pickle.HIGHEST_PROTOCOL), \n",
    "     'labels':pickle.dumps(labels, pickle.HIGHEST_PROTOCOL)} for i, (data, labels) in enumerate(zip(xtest, ytest))]).inserted_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch one of the documents by index\n",
    "doc = db.test.find_one({\"_id\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(pickle.loads(doc['data'])[4].squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the array\n",
    "arr = pickle.loads(doc['data'])\n",
    "arr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a dataset from Torch that is able to load the examples from the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(a, n):\n",
    "    k, m = divmod(len(a), n)\n",
    "    return [a[i * k + min(i, m):(i + 1) * k + min(i + 1, m)] for i in range(n)]\n",
    "\n",
    "split(range(20), 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = db.train.find({'_id':{'$gte': 3, '$lte':20}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.vstack([xtest[0], xtest[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = db.test.find({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistDataset(tdata.Dataset):\n",
    "    \"\"\" The dataset is able to load a specific subset of samples from MONGO and combine them \"\"\"\n",
    "\n",
    "    def __init__(self, func_id, num_func, task, transform=None):\n",
    "        \"\"\" Based on the funcId and the Number of functions, estimate the range of the datasets\n",
    "        that we should get from mongo.\n",
    "\n",
    "        :arg func_id ID of the function creating the dataset\n",
    "        :arg num_func total number of functions invoked\n",
    "        :arg task either train or val\n",
    "        :arg transform transformations to be applied\n",
    "        \"\"\"\n",
    "\n",
    "        # create the mongo client\n",
    "        self.client = pymongo.MongoClient(MONGO_IP, MONGO_PORT)\n",
    "        self.db = self.client[DATABASE]\n",
    "\n",
    "        # get the number of documents\n",
    "        ndocs = self.db.train.count_documents({})\n",
    "\n",
    "        if task == 'train':\n",
    "            minibatch = self._split_minibatches(range(ndocs), num_func)[func_id]\n",
    "            print('I get minibatches', minibatch)\n",
    "\n",
    "            self.data, self.labels = self._load_data(minibatch)\n",
    "\n",
    "        # If task is validation we just tell it to load all the data\n",
    "        else:\n",
    "            self.data, self.labels = self._load_data()\n",
    "\n",
    "        self.transforms = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    # We could have 1 document per datapoint or group of datapoints?\n",
    "    # this is just useful for bigger datasets, but could save a lot of memory\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx]\n",
    "        y = self.labels[idx]\n",
    "\n",
    "        if self.transforms:\n",
    "            return self.transforms(x), y\n",
    "        else:\n",
    "            return x, y\n",
    "\n",
    "    # TODO look how we can maybe declare indexes in mongodb to speed this up\n",
    "    def _load_data(self, minibatches=None):\n",
    "        \"\"\"Return the data from the MongoDB already formatted as a numpy array\"\"\"\n",
    "\n",
    "        if minibatches is None:\n",
    "            # load all because it is the validation\n",
    "            batches = self.db.test.find({})\n",
    "        else:\n",
    "            # Load the objects from Mongo\n",
    "            batches = self.db.train.find({\n",
    "                '_id': {'$gte': minibatches.start, '$lte': minibatches.stop-1}\n",
    "            })\n",
    "\n",
    "        data, labels = None, None\n",
    "        for batch in batches:\n",
    "            # Load the data and the labels and append it to the variables above\n",
    "            d = pickle.loads(batch['data'])\n",
    "            l = pickle.loads(batch['labels'])\n",
    "\n",
    "            if data is None:\n",
    "                data, labels = d, l\n",
    "            else:\n",
    "                data = np.vstack([data, d])\n",
    "                labels = np.vstack([labels, l])\n",
    "\n",
    "        return data, labels.flatten()\n",
    "    \n",
    "    @staticmethod\n",
    "    def _split_minibatches(a, n):\n",
    "        \"\"\"Based on the number of minibatches return the ones assigned to each\n",
    "        function so that the count is approximately the same \"\"\"\n",
    "        k, m = divmod(len(a), n)\n",
    "        return [a[i * k + min(i, m):(i + 1) * k + min(i + 1, m)] for i in range(n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "d = MnistDataset(func_id=3, num_func=5, task='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, lab = d.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
