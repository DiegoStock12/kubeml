{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the network using the REDISAI db as an exchange place and debug the problems\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision.models as models\n",
    "import torch.utils.data as tdata\n",
    "\n",
    "import numpy as np\n",
    "import redisai as rai\n",
    "\n",
    "from dataclasses import dataclass\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pymongo\n",
    "\n",
    "# Ip of minikube and the port of the mongo service\n",
    "MONGO_IP = '192.168.99.101'\n",
    "MONGO_PORT = 30933\n",
    "\n",
    "# import the modules used in the program\n",
    "import train_utils\n",
    "import ml2rt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainParams:\n",
    "    ps_id: str\n",
    "    N: int\n",
    "    task: str\n",
    "    func_id: int\n",
    "    lr: float\n",
    "    batch_size: int\n",
    "    \n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "    \n",
    "def create_model(init: bool):\n",
    "    \"\"\"Creates the model used to train the network\n",
    "\n",
    "    For this example we'll be using the simple model from the MNIST examples\n",
    "    (https://github.com/pytorch/examples/blob/master/mnist/main.py)\n",
    "    \"\"\"\n",
    "\n",
    "    def init_weights(m: nn.Module):\n",
    "        \"\"\"Initialize the weights of the network\"\"\"\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            nn.init.constant_(m.bias, 0.01)\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            nn.init.constant_(m.bias, 0.01)\n",
    "\n",
    "    # Create the model and initialize the weights\n",
    "    model = Net()\n",
    "\n",
    "    # If the task is initializing the layers do so\n",
    "    if init:\n",
    "        print('Initializing layers...')\n",
    "        model.apply(init_weights)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO max document size is 16 MB, this could give us problems in the future\n",
    "# when the datasets are so big, we should calculate the size (easy, and divide the dataset)\n",
    "def split_dataset(X, Y, subsets):\n",
    "    \"\"\"Splits the X and Y in N different subsets\"\"\"\n",
    "    X_split = np.split(X, subsets)\n",
    "    Y_split = np.split(Y, subsets)\n",
    "    \n",
    "    return X_split, Y_split\n",
    "\n",
    "\n",
    "def approx_size(a: np.array):\n",
    "    \"\"\" approx size of float 32 array in MB\"\"\"\n",
    "    return (32/8) * np.prod(a.shape) / 1e6\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "47*128, 16*128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "\n",
    "train_data = datasets.MNIST('./data', train=True, download=False, transform=transform)\n",
    "val_data = datasets.MNIST('./data', train=False, download=False, transform=transform)\n",
    "\n",
    "# train_data.data, train_data.targets = train_data.data[:3000], train_data.targets[:3000]\n",
    "# val_data.data, val_data.targets = val_data.data[:2000], val_data.targets[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=32)\n",
    "val_loader = torch.utils.data.DataLoader(val_data, batch_size=32)\n",
    "len(train_loader), len(val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the train and test methods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: nn.Module, device,\n",
    "          train_loader: tdata.DataLoader,\n",
    "          optimizer: torch.optim.Optimizer, tensor_dict) -> float:\n",
    "    \"\"\"Loop used to train the network\"\"\"\n",
    "    model.train()\n",
    "    loss, tot = 0, 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "#         print(data.shape, target.shape)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        print(output.shape)\n",
    "\n",
    "        loss = F.nll_loss(output, target)\n",
    "        tot += loss.item()\n",
    "        loss.backward()\n",
    "\n",
    "        # Here save the gradients to publish on the database\n",
    "#         train_utils.update_tensor_dict(model, tensor_dict)\n",
    "        optimizer.step()\n",
    "        \n",
    "\n",
    "        if batch_idx % 5 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                1, batch_idx * len(data), len(train_loader.dataset),\n",
    "                   100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "    return tot/len(train_loader)\n",
    "\n",
    "\n",
    "def validate(model, device, val_loader: tdata.DataLoader) -> (float, float):\n",
    "    \"\"\"Loop used to validate the network\"\"\"\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in val_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(val_loader.dataset)\n",
    "\n",
    "    accuracy = 100. * correct / len(val_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(val_loader.dataset),\n",
    "        100. * correct / len(val_loader.dataset)))\n",
    "    return accuracy, test_loss\n",
    "\n",
    "def infer(model, device, data: np.array):\n",
    "    \"\"\"Forward the data through the network and return the predictions\"\"\"\n",
    "    model.eval()\n",
    "    data = transform(data).to(device)\n",
    "    data = data.permute(1, 2, 0).view(-1, 1, 28, 28)\n",
    "    out = model(data)\n",
    "    \n",
    "    preds = torch.argmax(out, axis=1)\n",
    "    return preds.cpu().numpy()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main entrypoint of the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "params = TrainParams(ps_id='example', func_id=0, N =2, task='train', lr=0.01, batch_size=128)\n",
    "\n",
    "\n",
    "torch.manual_seed(42) \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device('cpu')\n",
    "\n",
    "# Create the model\n",
    "model = create_model(init=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the redis connection\n",
    "addr = '192.168.99.101'\n",
    "port = 31618\n",
    "con = rai.Client(debug=True, host=addr, port=port)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train for a couple of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# create the tensor dict\n",
    "tdict = dict()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "for epoch in range(1,11):\n",
    "    print('Epoch', epoch)\n",
    "    train(model, device, train_loader, optimizer, tdict)\n",
    "    validate(model, device, val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(train_data.data[0].numpy().squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_test = np.random.rand(28, 28, 2).astype('float32')\n",
    "data = train_data.data[:6].numpy()\n",
    "t = transform(data).to(device)\n",
    "t = t.permute(1, 2, 0)\n",
    "print(t.shape)\n",
    "t = t.view(-1, 1, 28, 28)\n",
    "print(t.shape)\n",
    "\n",
    "plt.imshow(t[0].cpu().numpy().squeeze())\n",
    "print(t.shape)\n",
    "# t = torch.Tensor(input_test).to(device)\n",
    "# out = model(t)\n",
    "out=model.forward(t)\n",
    "\n",
    "a = np.array([4], dtype='int64')\n",
    "target = torch.as_tensor(a).to(device)\n",
    "pred = torch.argmax(out, axis=1)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = train_data.data[:10].numpy()\n",
    "infer(model, device, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "p = pickle.dumps(model.state_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cpu().state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model in the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = pymongo.MongoClient(MONGO_IP, MONGO_PORT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = client['kubeml']\n",
    "db.drop_collection('network')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db['network'].insert_one(\n",
    "    {\n",
    "        \"_id\": \"test\",\n",
    "        \"state_dict\": pickle.dumps(model.cpu().state_dict())\n",
    "}).inserted_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = db['network'].find_one({\"_id\": \"test\"})\n",
    "model.load_state_dict(pickle.loads(res['state']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Invoke the function with some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "ROUTER_ADDRESS = \"http://192.168.99.101:32422\"\n",
    "FUNCTION = \"network\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = train_data.data[0].numpy().tolist()\n",
    "# data = np.array(data, dtype='uint8')\n",
    "# infer(model, device, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "js = json.dumps({\"data\":data})\n",
    "with open('example.json', 'w') as f:\n",
    "    f.write(js)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# send the request \n",
    "# url = \"http://192.168.99.101:32422/inference?N=1&batchSize=128&funcId=0&lr=0.01&psId=test&task=infer\"\n",
    "url = \"http://192.168.99.101:32422/network?N=1&batchSize=0&funcId=0&lr=1&psId=test&task=infer\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.post(url, json={\"data\": data })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.json()['predictions']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
