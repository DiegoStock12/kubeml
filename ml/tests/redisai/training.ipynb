{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the network using the REDISAI db as an exchange place and debug the problems\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\diego\\cs\\thesis\\venv\\lib\\site-packages\\numpy\\_distributor_init.py:32: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\users\\diego\\cs\\thesis\\venv\\lib\\site-packages\\numpy\\.libs\\libopenblas.NOIJJG62EMASZI6NYURL6JBKM4EVBGM7.gfortran-win_amd64.dll\n",
      "c:\\users\\diego\\cs\\thesis\\venv\\lib\\site-packages\\numpy\\.libs\\libopenblas.PYQHXLVVQ7VESDPUVUADXEVJOBGHJPAY.gfortran-win_amd64.dll\n",
      "  stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision.models as models\n",
    "import torch.utils.data as tdata\n",
    "\n",
    "import numpy as np\n",
    "import redisai as rai\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# import the modules used in the program\n",
    "import train_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainParams:\n",
    "    ps_id: str\n",
    "    N: int\n",
    "    task: str\n",
    "    func_id: int\n",
    "    lr: float\n",
    "    batch_size: int\n",
    "    \n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "    \n",
    "def create_model(init: bool):\n",
    "    \"\"\"Creates the model used to train the network\n",
    "\n",
    "    For this example we'll be using the simple model from the MNIST examples\n",
    "    (https://github.com/pytorch/examples/blob/master/mnist/main.py)\n",
    "    \"\"\"\n",
    "\n",
    "    def init_weights(m: nn.Module):\n",
    "        \"\"\"Initialize the weights of the network\"\"\"\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            nn.init.constant_(m.bias, 0.01)\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            nn.init.constant_(m.bias, 0.01)\n",
    "\n",
    "    # Create the model and initialize the weights\n",
    "    model = Net()\n",
    "\n",
    "    # If the task is initializing the layers do so\n",
    "    if init:\n",
    "        print('Initializing layers...')\n",
    "        model.apply(init_weights)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO max document size is 16 MB, this could give us problems in the future\n",
    "# when the datasets are so big, we should calculate the size (easy, and divide the dataset)\n",
    "def split_dataset(X, Y, subsets):\n",
    "    \"\"\"Splits the X and Y in N different subsets\"\"\"\n",
    "    X_split = np.split(X, subsets)\n",
    "    Y_split = np.split(Y, subsets)\n",
    "    \n",
    "    return X_split, Y_split\n",
    "\n",
    "\n",
    "def approx_size(a: np.array):\n",
    "    \"\"\" approx size of float 32 array in MB\"\"\"\n",
    "    return (32/8) * np.prod(a.shape) / 1e6\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6016, 2048)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "47*128, 16*128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "\n",
    "train_data = datasets.MNIST('./data', train=True, download=False, transform=transform)\n",
    "val_data = datasets.MNIST('./data', train=False, download=False, transform=transform)\n",
    "\n",
    "train_data.data, train_data.targets = train_data.data[:3000], train_data.targets[:3000]\n",
    "val_data.data, val_data.targets = val_data.data[:2000], val_data.targets[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24, 16)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=128)\n",
    "val_loader = torch.utils.data.DataLoader(val_data, batch_size=128)\n",
    "len(train_loader), len(val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the train and test methods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: nn.Module, device,\n",
    "          train_loader: tdata.DataLoader,\n",
    "          optimizer: torch.optim.Optimizer, tensor_dict) -> float:\n",
    "    \"\"\"Loop used to train the network\"\"\"\n",
    "    model.train()\n",
    "    loss, tot = 0, 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "\n",
    "        loss = F.nll_loss(output, target)\n",
    "        tot += loss.item()\n",
    "        loss.backward()\n",
    "\n",
    "        # Here save the gradients to publish on the database\n",
    "        train_utils.update_tensor_dict(model, tensor_dict)\n",
    "        optimizer.step()\n",
    "        \n",
    "\n",
    "        if batch_idx % 5 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                1, batch_idx * len(data), len(train_loader.dataset),\n",
    "                   100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "    return tot/len(train_loader)\n",
    "\n",
    "\n",
    "def validate(model, device, val_loader: tdata.DataLoader) -> (float, float):\n",
    "    \"\"\"Loop used to validate the network\"\"\"\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in val_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(val_loader.dataset)\n",
    "\n",
    "    accuracy = 100. * correct / len(val_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(val_loader.dataset),\n",
    "        100. * correct / len(val_loader.dataset)))\n",
    "    return accuracy, test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main entrypoint of the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing layers...\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "params = TrainParams(ps_id='example', func_id=0, N =2, task='train', lr=0.01, batch_size=128)\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "device = device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Create the model\n",
    "\n",
    "model = create_model(True).to(device)\n",
    "model\n",
    "\n",
    "initial = deepcopy(model.state_dict())\n",
    "\n",
    "# train_utils.save_model_weights(model, params, con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial['conv2.bias']\n",
    "torch.set_printoptions(precision=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the redis connection\n",
    "addr = '192.168.99.101'\n",
    "port = 31618\n",
    "con = rai.Client(debug=True, host=addr, port=port)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights for layer conv1\n",
      "AI.TENSORGET 268b3327:conv1.weight META BLOB\n",
      "Loading bias for layer conv1\n",
      "AI.TENSORGET 268b3327:conv1.bias META BLOB\n",
      "Loading weights for layer conv2\n",
      "AI.TENSORGET 268b3327:conv2.weight META BLOB\n",
      "Loading bias for layer conv2\n",
      "AI.TENSORGET 268b3327:conv2.bias META BLOB\n",
      "Loading weights for layer fc1\n",
      "AI.TENSORGET 268b3327:fc1.weight META BLOB\n",
      "Loading bias for layer fc1\n",
      "AI.TENSORGET 268b3327:fc1.bias META BLOB\n",
      "Loading weights for layer fc2\n",
      "AI.TENSORGET 268b3327:fc2.weight META BLOB\n",
      "Loading bias for layer fc2\n",
      "AI.TENSORGET 268b3327:fc2.bias META BLOB\n",
      "Bias layer is tensor([0.0098, 0.0102, 0.0091, 0.0090, 0.0100, 0.0104, 0.0104, 0.0115, 0.0095,\n",
      "        0.0100])\n",
      "Loaded state dict from the database dict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\diego\\CS\\thesis\\ml\\tests\\redisai\\train_utils.py:149: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:141.)\n",
      "  state[weight_key[9:]] = torch.from_numpy(w)\n"
     ]
    }
   ],
   "source": [
    "train_utils.load_model_weights(model, '268b3327', con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 2.3226, Accuracy: 218/2000 (11%)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10.9, 2.322626625061035)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate(model, device, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train for a couple of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5\n",
      "Train Epoch: 1 [0/3000 (0%)]\tLoss: 2.331917\n",
      "Train Epoch: 1 [640/3000 (21%)]\tLoss: 2.240755\n",
      "Train Epoch: 1 [1280/3000 (42%)]\tLoss: 2.214773\n",
      "Train Epoch: 1 [1920/3000 (62%)]\tLoss: 2.153557\n",
      "Train Epoch: 1 [2560/3000 (83%)]\tLoss: 2.065098\n",
      "\n",
      "Test set: Average loss: 1.9604, Accuracy: 1177/2000 (59%)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(58.85, 1.9604190673828126)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the tensor dict\n",
    "tdict = dict()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "print('Epoch', i)\n",
    "train(model, device, train_loader, optimizer, tdict)\n",
    "validate(model, device, val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = deepcopy(model.state_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights for layer conv1\n",
      "AI.TENSORGET e27bf8b8:conv1.weight META BLOB\n",
      "Loading bias for layer conv1\n",
      "AI.TENSORGET e27bf8b8:conv1.bias META BLOB\n",
      "Loading weights for layer conv2\n",
      "AI.TENSORGET e27bf8b8:conv2.weight META BLOB\n",
      "Loading bias for layer conv2\n",
      "AI.TENSORGET e27bf8b8:conv2.bias META BLOB\n",
      "Loading weights for layer fc1\n",
      "AI.TENSORGET e27bf8b8:fc1.weight META BLOB\n",
      "Loading bias for layer fc1\n",
      "AI.TENSORGET e27bf8b8:fc1.bias META BLOB\n",
      "Loading weights for layer fc2\n",
      "AI.TENSORGET e27bf8b8:fc2.weight META BLOB\n",
      "Loading bias for layer fc2\n",
      "AI.TENSORGET e27bf8b8:fc2.bias META BLOB\n",
      "Bias layer is tensor([0.0095, 0.0102, 0.0094, 0.0098, 0.0101, 0.0101, 0.0103, 0.0107, 0.0100,\n",
      "        0.0099])\n",
      "Loaded state dict from the database dict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias'])\n"
     ]
    }
   ],
   "source": [
    "train_utils.load_model_weights(model, 'e27bf8b8', con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-5.3157e-04,  2.4102e-03,  4.9774e-03, -2.2192e-03,  1.1392e-03,\n",
       "        -1.0168e-03,  1.7234e-03, -1.9600e-03, -5.1682e-04, -1.1000e-03,\n",
       "         2.9163e-03,  4.1495e-03, -8.2636e-06,  4.6715e-04, -1.2589e-03,\n",
       "        -2.0384e-03,  5.5351e-03,  2.3986e-03,  2.8878e-03,  4.5331e-03,\n",
       "         3.3866e-03,  5.2829e-03,  9.3734e-04, -3.7543e-04,  6.0971e-03,\n",
       "        -2.5996e-03,  1.7083e-03, -4.0774e-04,  2.4535e-03,  1.4973e-03,\n",
       "         1.7151e-03, -1.7721e-03], device='cuda:0')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final['conv1.bias'] - dist['conv1.bias']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('conv1.weight.grad', tensor(0.0010779696, device='cuda:0')),\n",
       " ('conv1.bias.grad', tensor(0.0054637371, device='cuda:0')),\n",
       " ('conv2.weight.grad', tensor(0.0006221278, device='cuda:0')),\n",
       " ('conv2.bias.grad', tensor(0.0024095972, device='cuda:0')),\n",
       " ('fc1.weight.grad', tensor(0.0002331834, device='cuda:0')),\n",
       " ('fc1.bias.grad', tensor(0.0006510335, device='cuda:0')),\n",
       " ('fc2.weight.grad', tensor(-1.3606040739e-09, device='cuda:0')),\n",
       " ('fc2.bias.grad', tensor(-3.7252902985e-09, device='cuda:0'))]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(k, torch.mean(v)) for k, v in tdict.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "train_utils.save_gradients(tdict,params, con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.conv2.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdict['conv2.bias.grad']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.eq(model.conv2.weight, initial['conv2.weight']).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = con.tensorget(\"example:conv2.bias.grad/0\", as_numpy=False)\n",
    "a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
