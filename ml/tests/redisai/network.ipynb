{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test RedisAI with Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "# import the modules used in the program\n",
    "import train_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the network\n",
    "\n",
    "Take the network from the pytorch MNIST examples \n",
    "(https://github.com/pytorch/examples/blob/master/mnist/main.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the dataset and the data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "\n",
    "data = datasets.MNIST('./data', train=True, download=False, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(data, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a function to store the gradients during training at the end of each epoch and see how much time it takes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_d = {}\n",
    "\n",
    "# We get similar performance with both methods,\n",
    "# and with the second one we dont need to use twice the amount of GPU mem\n",
    "\n",
    "# Should we do this with a backup model or should be save the state in a dict for example?? we could add cuda tensors there\n",
    "def update_tensor(m: nn.Module, backup: nn.Module):\n",
    "    \"\"\"Saves all of the model layers and adds the gradients\n",
    "    \n",
    "    For this we need the two networks to reside in the GPU\n",
    "    which will use extra memory, instead of that we could use a dictionary\n",
    "    \"\"\"\n",
    "    for (n1, l1), (n2, l2) in zip(m.named_children(), backup.named_children()):\n",
    "        if hasattr(l1, 'weight'):\n",
    "            if l2.weight.grad is None:\n",
    "                l2.weight.grad = l1.weight.grad\n",
    "                l2.bias.grad = l1.bias.grad\n",
    "            else:\n",
    "                l2.weight.grad += l1.weight.grad\n",
    "                l2.bias.grad += l1.bias.grad\n",
    "\n",
    "\n",
    "# def update_tensor_dict(m:nn.Module, d:dict):\n",
    "#     def needs_saving(t):\n",
    "#         t = str(t)\n",
    "#         if 'conv' in t or 'linear' in t:\n",
    "#             return True\n",
    "#         return False\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         for n, l in m.named_modules():\n",
    "#             if needs_saving(type(l)):\n",
    "#                 if n in d:\n",
    "#                     d[f'{n}-weight-grad'] += l.weight.grad\n",
    "#                     d[f'{n}-bias-grad'] += l.bias.grad\n",
    "#                 else:\n",
    "#                     d[f'{n}-weight-grad'] = l.weight.grad\n",
    "#                     d[f'{n}-bias-grad'] = l.bias.grad\n",
    "                    \n",
    "def update_tensor_dict(m: nn.Module, d: dict):\n",
    "    \"\"\"Update the tensor dict so we can save it after the epoch is finished\"\"\"\n",
    "    with torch.no_grad():\n",
    "        for name, layer in m.named_modules():\n",
    "            if _is_optimizable(layer):\n",
    "                if name in d:\n",
    "                    d[f'{name}-weight-grad'] += layer.weight.grad\n",
    "                    if layer.bias is not None:\n",
    "                        d[f'{name}-bias-grad'] += layer.bias.grad\n",
    "                else:\n",
    "                    d[f'{name}-weight-grad'] = layer.weight.grad\n",
    "                    if layer.bias is not None:\n",
    "                        d[f'{name}-bias-grad'] = layer.bias.grad\n",
    "                    \n",
    "def _is_optimizable(layer: nn.Module) -> bool:\n",
    "    \"\"\"Should save layer returns just whether the layer is optimizable or not\n",
    "    and thus if it should be sent to the parameter server\"\"\"\n",
    "    t = str(type(layer))\n",
    "    if 'conv' in t or 'linear' in t:\n",
    "        return True\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = models.resnet18()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a save layers model that will simply check all the layers if they are \n",
    "# This should be inside the update_tensor_d\n",
    "for n, l in model.named_modules():\n",
    "    if 'conv' in str(type(l)) or 'linear' in str(type(l)):\n",
    "        print(n, type(l), l.bias is None, hasattr(l, 'weight'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the network and do a forward and backward pass to get the gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "model = Net()\n",
    "# backup = Net()\n",
    "# backup = copy.deepcopy(model)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "model.train()\n",
    "\n",
    "# Send the network to the GPU\n",
    "model= model.cuda()\n",
    "# backup.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_weights(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model weights from redis\n",
    "load_model_weights(backup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "count = 0\n",
    "\n",
    "model.load_state_dict(load_state_dict(model))\n",
    "tensor_d.clear()\n",
    "\n",
    "model = model.cuda()\n",
    "\n",
    "for inputs, targets in train_loader:\n",
    "    \n",
    "    count += len(inputs)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    if count % 2048 == 0:\n",
    "        print(f'Training {count}/{len(train_loader.dataset)} \\t {loss.item()}')\n",
    "    \n",
    "    inputs = inputs.cuda()\n",
    "    targets = targets.cuda()\n",
    "\n",
    "    # Zero the optimizer before the forward pass\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # forward pass\n",
    "    out = model(inputs)\n",
    "\n",
    "    loss = F.nll_loss(out, targets)\n",
    "    loss.backward()\n",
    "    \n",
    "    #Here update the models\n",
    "    update_tensor_dict(model, tensor_d)\n",
    "    \n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.conv1.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before backup.conv1.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start with the REDIS AI part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import redisai as rai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAI_KUBE = '192.168.99.101'\n",
    "RAI_PORT_KUBE = 31618\n",
    "RAI = '192.168.99.102'\n",
    "PORT = 6379\n",
    "\n",
    "con = rai.Client(host=RAI_KUBE, port=RAI_PORT_KUBE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.tensorset('grad-conv1', c1_grad.numpy(), dtype='float32')\n",
    "con.tensorset('bias-conv1', c1_bias.numpy(), dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.set('example', 'hola')\n",
    "con.set('exaaaaa', 'hola2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "a = con.tensorget('example:fc1-weight', as_numpy=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set all the model gradients to the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "psId = 'example'\n",
    "\n",
    "for n, l in model.named_children():\n",
    "    if hasattr(l, 'bias'):\n",
    "        key_w = f'{psId}:{n}-weight'\n",
    "        key_b = f'{psId}:{n}-bias'\n",
    "\n",
    "        print('Setting', key_w)\n",
    "        con.tensorset(key_w, l.weight.cpu().detach().numpy(), dtype='float32')\n",
    "\n",
    "        print('Setting', key_b)\n",
    "        con.tensorset(key_b, l.bias.cpu().detach().numpy(), dtype='float32')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the model gradients from the dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in tensor_d.items():\n",
    "    print('Setting' , k)\n",
    "    con.tensorset(f'{k}/1', v.cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try to get all of the layers from redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n, l in m2.named_children():\n",
    "    if hasattr(l, 'bias'):\n",
    "        key_w = f'{psId}:{n}-weight'\n",
    "        key_b = f'{psId}:{n}-bias'\n",
    "            \n",
    "        print('Getting', key_w)\n",
    "        t = con.tensorget(key_w)\n",
    "        l.weight = torch.nn.Parameter(torch.from_numpy(t))\n",
    "\n",
    "        print('Getting', key_b)\n",
    "        t = con.tensorget(key_b)\n",
    "        l.bias =torch.nn.Parameter(torch.from_numpy(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try to set the model to REDIS\n",
    "\n",
    "We can simply save the state dict and retrieve it super quickly from the following functions as a python object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_weights(m: nn.Module, id):\n",
    "    r\"\"\"After the init task we should save the model gradients to the database\n",
    "\n",
    "    Instead of looking if a layer has a bias term (some of the batch norm can have it,\n",
    "    look if the layer is of type conv or not\"\"\"\n",
    "    print('Saving model to the database')\n",
    "    with torch.no_grad():\n",
    "        for name, layer in m.named_modules():\n",
    "            if _is_optimizable(layer):\n",
    "\n",
    "                # Save the weights\n",
    "                print(f'Setting weights for layer {name}')\n",
    "                weight_key = f'{id}:{name}.weight'\n",
    "                con.tensorset(weight_key, layer.weight.cpu().detach().numpy(), dtype='float32')\n",
    "\n",
    "                # Save the bias if not None\n",
    "                if layer.bias is not None:\n",
    "                    print(f'Setting bias for layer {name}')\n",
    "                    bias_key = f'{id}:{name}.bias'\n",
    "                    con.tensorset(bias_key, layer.bias.cpu().detach().numpy(), dtype='float32')\n",
    "\n",
    "    print('Saved model to the database')\n",
    "    \n",
    "def load_model_weights(m: nn.Module, id: str):\n",
    "    \"\"\"Load the model weights saved in the database to start the new epoch\"\"\"\n",
    "    print('Loading model from database')\n",
    "    with torch.no_grad():\n",
    "        for name, layer in m.named_modules():\n",
    "            # only load and save layers that are optimizable (conv or fc)\n",
    "            if _is_optimizable(layer):\n",
    "\n",
    "                # Load the weight\n",
    "                print(f'Loading weights for layer {name}')\n",
    "                weight_key = f'{id}:{name}.weight'\n",
    "                w = con.tensorget(weight_key)\n",
    "                layer.weight = torch.nn.Parameter(torch.from_numpy(w))\n",
    "\n",
    "                # If the layer has an active bias retrieve it\n",
    "                # Some of the layers in resnet do not have bias\n",
    "                # or it is None. It is not needed with BN, so skip it\n",
    "                if layer.bias is not None:\n",
    "                    print(f'Loading bias for layer {name}')\n",
    "                    bias_key = f'{id}{name}.bias'\n",
    "                    w = con.tensorget(bias_key)\n",
    "                    layer.bias = torch.nn.Parameter(torch.from_numpy(w))\n",
    "\n",
    "    print('Model loaded from database')\n",
    "    \n",
    "def load_state_dict(m: nn.Module, id) -> dict:\n",
    "    d = dict()\n",
    "    for name, layer in m.named_modules():\n",
    "        # only load and save layers that are optimizable (conv or fc)\n",
    "        if _is_optimizable(layer):\n",
    "\n",
    "            # Load the weight\n",
    "            print(f'Loading weights for layer {name}')\n",
    "            weight_key = f'{id}:{name}.weight'\n",
    "            w = con.tensorget(weight_key)\n",
    "            # set the weight\n",
    "            d[weight_key[9:]] = torch.from_numpy(w)\n",
    "\n",
    "            # If the layer has an active bias retrieve it\n",
    "            # Some of the layers in resnet do not have bias\n",
    "            # or it is None. It is not needed with BN, so skip it\n",
    "            if layer.bias is not None:\n",
    "                print(f'Loading bias for layer {name}')\n",
    "                bias_key = f'{id}:{name}.bias'\n",
    "                w = con.tensorget(bias_key)\n",
    "                # set the bias\n",
    "                d[bias_key[9:]] = torch.from_numpy(w)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d = load_state_dict(model, id='aa11a789' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.state_dict().keys()\n",
    "\n",
    "# model.load_state_dict(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "p = pickle.dumps(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.set('model', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "d = con.get('model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pickle.loads(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the layer names in redis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "m = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to get the weighted layers and save the model\n",
    "[n for n, l in m.named_children() if hasattr(l, \"bias\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = \" \".join([n for n, l in m.named_children() if hasattr(l, \"bias\")])\n",
    "con.set(\"layers\", l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in [n for n, l in m.named_children() if hasattr(l, \"bias\")]:\n",
    "    con.rpush('layers', l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "layers = ['conv1', 'conv2', 'fc1', 'fc2']\n",
    "l = json.dumps(layers)\n",
    "\n",
    "l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the layers by making an http request to the server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(l)\n",
    "\n",
    "requests.post(\"http://localhost:58682/layers\", data=l)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
