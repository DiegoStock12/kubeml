{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test RedisAI with Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\diego\\cs\\thesis\\venv\\lib\\site-packages\\numpy\\_distributor_init.py:32: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\users\\diego\\cs\\thesis\\venv\\lib\\site-packages\\numpy\\.libs\\libopenblas.NOIJJG62EMASZI6NYURL6JBKM4EVBGM7.gfortran-win_amd64.dll\n",
      "c:\\users\\diego\\cs\\thesis\\venv\\lib\\site-packages\\numpy\\.libs\\libopenblas.PYQHXLVVQ7VESDPUVUADXEVJOBGHJPAY.gfortran-win_amd64.dll\n",
      "  stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the network\n",
    "\n",
    "Take the network from the pytorch MNIST examples \n",
    "(https://github.com/pytorch/examples/blob/master/mnist/main.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the dataset and the data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "\n",
    "data = datasets.MNIST('./data', train=True, download=False, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.targets.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(data, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = iter(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a single tensor to forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, target = it.next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a function to store the gradients during training at the end of each epoch and see how much time it takes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_d = {}\n",
    "\n",
    "# We get similar performance with both methods,\n",
    "# and with the second one we dont need to use twice the amount of GPU mem\n",
    "\n",
    "# Should we do this with a backup model or should be save the state in a dict for example?? we could add cuda tensors there\n",
    "def update_tensor(m: nn.Module, backup: nn.Module):\n",
    "    \"\"\"Saves all of the model layers and adds the gradients\n",
    "    \n",
    "    For this we need the two networks to reside in the GPU\n",
    "    which will use extra memory, instead of that we could use a dictionary\n",
    "    \"\"\"\n",
    "    for (n1, l1), (n2, l2) in zip(m.named_children(), backup.named_children()):\n",
    "        if hasattr(l1, 'weight'):\n",
    "            if l2.weight.grad is None:\n",
    "                l2.weight.grad = l1.weight.grad\n",
    "                l2.bias.grad = l1.bias.grad\n",
    "            else:\n",
    "                l2.weight.grad += l1.weight.grad\n",
    "                l2.bias.grad += l1.bias.grad\n",
    "\n",
    "\n",
    "# def update_tensor_dict(m:nn.Module, d:dict):\n",
    "#     def needs_saving(t):\n",
    "#         t = str(t)\n",
    "#         if 'conv' in t or 'linear' in t:\n",
    "#             return True\n",
    "#         return False\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         for n, l in m.named_modules():\n",
    "#             if needs_saving(type(l)):\n",
    "#                 if n in d:\n",
    "#                     d[f'{n}-weight-grad'] += l.weight.grad\n",
    "#                     d[f'{n}-bias-grad'] += l.bias.grad\n",
    "#                 else:\n",
    "#                     d[f'{n}-weight-grad'] = l.weight.grad\n",
    "#                     d[f'{n}-bias-grad'] = l.bias.grad\n",
    "                    \n",
    "def update_tensor_dict(m: nn.Module, d: dict):\n",
    "    \"\"\"Update the tensor dict so we can save it after the epoch is finished\"\"\"\n",
    "    with torch.no_grad():\n",
    "        for name, layer in m.named_modules():\n",
    "            if _is_optimizable(layer):\n",
    "                if name in d:\n",
    "                    d[f'{name}-weight-grad'] += layer.weight.grad\n",
    "                    if layer.bias is not None:\n",
    "                        d[f'{name}-bias-grad'] += layer.bias.grad\n",
    "                else:\n",
    "                    d[f'{name}-weight-grad'] = layer.weight.grad\n",
    "                    if layer.bias is not None:\n",
    "                        d[f'{name}-bias-grad'] = layer.bias.grad\n",
    "                    \n",
    "def _is_optimizable(layer: nn.Module) -> bool:\n",
    "    \"\"\"Should save layer returns just whether the layer is optimizable or not\n",
    "    and thus if it should be sent to the parameter server\"\"\"\n",
    "    t = str(type(layer))\n",
    "    if 'conv' in t or 'linear' in t:\n",
    "        return True\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = models.resnet18()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1 <class 'torch.nn.modules.conv.Conv2d'> False True\n",
      "conv2 <class 'torch.nn.modules.conv.Conv2d'> False True\n",
      "fc1 <class 'torch.nn.modules.linear.Linear'> False True\n",
      "fc2 <class 'torch.nn.modules.linear.Linear'> False True\n"
     ]
    }
   ],
   "source": [
    "# Create a save layers model that will simply check all the layers if they are \n",
    "# This should be inside the update_tensor_d\n",
    "for n, l in model.named_modules():\n",
    "    if 'conv' in str(type(l)) or 'linear' in str(type(l)):\n",
    "        print(n, type(l), l.bias is None, hasattr(l, 'weight'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the network and do a forward and backward pass to get the gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (dropout1): Dropout(p=0.25, inplace=False)\n",
       "  (dropout2): Dropout(p=0.5, inplace=False)\n",
       "  (fc1): Linear(in_features=9216, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "model = Net()\n",
    "backup = Net()\n",
    "# backup = copy.deepcopy(model)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "model.train()\n",
    "\n",
    "# Send the network to the GPU\n",
    "# model= model\n",
    "# backup.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[ 0.0235,  0.2356,  0.2535],\n",
       "          [-0.2728,  0.2479,  0.0950],\n",
       "          [ 0.0617,  0.2122,  0.0183]]],\n",
       "\n",
       "\n",
       "        [[[-0.2865, -0.1107,  0.0394],\n",
       "          [ 0.1674,  0.0032,  0.0407],\n",
       "          [ 0.2226, -0.1953,  0.2298]]],\n",
       "\n",
       "\n",
       "        [[[-0.0193,  0.0140, -0.2342],\n",
       "          [-0.2632, -0.0091, -0.1257],\n",
       "          [-0.0227,  0.1940,  0.0351]]],\n",
       "\n",
       "\n",
       "        [[[ 0.2221,  0.1044, -0.1893],\n",
       "          [-0.0967,  0.0654, -0.1620],\n",
       "          [-0.2458, -0.1814, -0.0647]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1544,  0.0308,  0.3172],\n",
       "          [ 0.1629, -0.2015, -0.1664],\n",
       "          [ 0.1575, -0.1471, -0.0732]]],\n",
       "\n",
       "\n",
       "        [[[ 0.2990, -0.0592, -0.3025],\n",
       "          [ 0.1203,  0.0926,  0.2259],\n",
       "          [ 0.2874,  0.2990,  0.1681]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0245, -0.0052,  0.0436],\n",
       "          [-0.0732, -0.3006,  0.2841],\n",
       "          [ 0.2441,  0.2335, -0.2484]]],\n",
       "\n",
       "\n",
       "        [[[ 0.2053, -0.0040,  0.1647],\n",
       "          [-0.3236,  0.3273, -0.0845],\n",
       "          [-0.2851, -0.0678,  0.2508]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0708,  0.2984,  0.1142],\n",
       "          [ 0.2992, -0.3229,  0.1541],\n",
       "          [-0.1427,  0.2120,  0.2803]]],\n",
       "\n",
       "\n",
       "        [[[-0.0618,  0.2323, -0.2783],\n",
       "          [-0.1637, -0.0271,  0.0607],\n",
       "          [ 0.1376,  0.0847, -0.2997]]],\n",
       "\n",
       "\n",
       "        [[[-0.2487,  0.2123, -0.1350],\n",
       "          [ 0.0465, -0.1427,  0.1005],\n",
       "          [-0.2769, -0.1383, -0.1673]]],\n",
       "\n",
       "\n",
       "        [[[-0.3268,  0.1193,  0.2870],\n",
       "          [ 0.1363, -0.0667,  0.1216],\n",
       "          [-0.1567, -0.2541, -0.0161]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0884, -0.2374, -0.1537],\n",
       "          [-0.2894, -0.3078, -0.0422],\n",
       "          [-0.3057, -0.2943,  0.1394]]],\n",
       "\n",
       "\n",
       "        [[[-0.1259, -0.1472,  0.2729],\n",
       "          [-0.3165,  0.2262,  0.1645],\n",
       "          [ 0.2195,  0.3189, -0.2988]]],\n",
       "\n",
       "\n",
       "        [[[-0.1317,  0.0122,  0.3333],\n",
       "          [-0.0171,  0.1658, -0.1716],\n",
       "          [-0.0848,  0.1387,  0.1611]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1496, -0.0044, -0.0218],\n",
       "          [ 0.1429, -0.0069,  0.1062],\n",
       "          [-0.2815, -0.2267,  0.1861]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0502, -0.2784,  0.0691],\n",
       "          [ 0.0538, -0.2748, -0.3204],\n",
       "          [-0.2652, -0.2861, -0.0968]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1557, -0.0345,  0.1796],\n",
       "          [ 0.0488,  0.0536, -0.1346],\n",
       "          [-0.0963,  0.0958,  0.1557]]],\n",
       "\n",
       "\n",
       "        [[[-0.2692, -0.1421, -0.2811],\n",
       "          [-0.2074,  0.1332, -0.1148],\n",
       "          [ 0.1123,  0.0375,  0.2233]]],\n",
       "\n",
       "\n",
       "        [[[-0.3137, -0.2382,  0.0806],\n",
       "          [ 0.2053,  0.0450,  0.0995],\n",
       "          [-0.1161,  0.0385, -0.1317]]],\n",
       "\n",
       "\n",
       "        [[[-0.2536,  0.2402, -0.2805],\n",
       "          [ 0.0198, -0.1501, -0.0149],\n",
       "          [ 0.2732,  0.1068,  0.3167]]],\n",
       "\n",
       "\n",
       "        [[[-0.0472,  0.1039, -0.1868],\n",
       "          [ 0.0220,  0.0278,  0.0958],\n",
       "          [-0.1939,  0.1512, -0.1758]]],\n",
       "\n",
       "\n",
       "        [[[ 0.2385,  0.0470,  0.0579],\n",
       "          [ 0.0637,  0.1110, -0.3198],\n",
       "          [ 0.0983,  0.1274,  0.1845]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1610,  0.0711,  0.0983],\n",
       "          [ 0.0251,  0.0498,  0.2635],\n",
       "          [-0.1454, -0.0165,  0.2558]]],\n",
       "\n",
       "\n",
       "        [[[ 0.2015,  0.0253, -0.2170],\n",
       "          [ 0.3036, -0.0875, -0.1389],\n",
       "          [ 0.2646, -0.0355, -0.3228]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1189, -0.0279, -0.2156],\n",
       "          [ 0.2571,  0.2433,  0.0196],\n",
       "          [ 0.1702,  0.3322, -0.1374]]],\n",
       "\n",
       "\n",
       "        [[[ 0.3149,  0.1913, -0.3214],\n",
       "          [ 0.0689,  0.0528, -0.2682],\n",
       "          [-0.1279,  0.1213, -0.3134]]],\n",
       "\n",
       "\n",
       "        [[[ 0.2982,  0.2683,  0.2860],\n",
       "          [ 0.2703, -0.1268,  0.2739],\n",
       "          [-0.1890,  0.3090,  0.0302]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0474,  0.0205, -0.3197],\n",
       "          [-0.3313, -0.1159, -0.2488],\n",
       "          [ 0.2835,  0.0889,  0.2601]]],\n",
       "\n",
       "\n",
       "        [[[-0.2332,  0.0022,  0.2910],\n",
       "          [-0.0739,  0.3319,  0.1342],\n",
       "          [-0.1484, -0.1634, -0.2763]]],\n",
       "\n",
       "\n",
       "        [[[-0.2868, -0.2010, -0.1074],\n",
       "          [ 0.2835, -0.1228,  0.1380],\n",
       "          [-0.1328,  0.0587, -0.1439]]],\n",
       "\n",
       "\n",
       "        [[[-0.3044, -0.2081, -0.0995],\n",
       "          [-0.2745,  0.2183, -0.1731],\n",
       "          [-0.3019,  0.1990, -0.3329]]]], requires_grad=True)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to the database\n",
      "Setting weights for layer conv1\n",
      "Setting bias for layer conv1\n",
      "Setting weights for layer conv2\n",
      "Setting bias for layer conv2\n",
      "Setting weights for layer fc1\n",
      "Setting bias for layer fc1\n",
      "Setting weights for layer fc2\n",
      "Setting bias for layer fc2\n",
      "Saved model to the database\n"
     ]
    }
   ],
   "source": [
    "save_model_weights(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from database\n",
      "Loading weights for layer conv1\n",
      "Loading bias for layer conv1\n",
      "Loading weights for layer conv2\n",
      "Loading bias for layer conv2\n",
      "Loading weights for layer fc1\n",
      "Loading bias for layer fc1\n",
      "Loading weights for layer fc2\n",
      "Loading bias for layer fc2\n",
      "Model loaded from database\n"
     ]
    }
   ],
   "source": [
    "# Load model weights from redis\n",
    "load_model_weights(backup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 2048/60000 \t 1.2616820335388184\n",
      "Training 4096/60000 \t 0.68885338306427\n",
      "Training 6144/60000 \t 0.6343638896942139\n",
      "Training 8192/60000 \t 0.7935402393341064\n",
      "Training 10240/60000 \t 0.575653612613678\n",
      "Training 12288/60000 \t 0.5969637632369995\n",
      "Training 14336/60000 \t 0.766459584236145\n",
      "Training 16384/60000 \t 0.4128168821334839\n",
      "Training 18432/60000 \t 0.3919685482978821\n",
      "Training 20480/60000 \t 0.5609372854232788\n",
      "Training 22528/60000 \t 0.36649125814437866\n",
      "Training 24576/60000 \t 0.33986788988113403\n",
      "Training 26624/60000 \t 0.5276138186454773\n",
      "Training 28672/60000 \t 0.5822106599807739\n",
      "Training 30720/60000 \t 0.6331431269645691\n",
      "Training 32768/60000 \t 0.37781375646591187\n",
      "Training 34816/60000 \t 0.6807652711868286\n",
      "Training 36864/60000 \t 0.40601012110710144\n",
      "Training 38912/60000 \t 0.5818591117858887\n",
      "Training 40960/60000 \t 0.28688257932662964\n",
      "Training 43008/60000 \t 0.5239367485046387\n",
      "Training 45056/60000 \t 0.5885693430900574\n",
      "Training 47104/60000 \t 0.425136536359787\n",
      "Training 49152/60000 \t 0.6892682313919067\n",
      "Training 51200/60000 \t 0.3987225294113159\n",
      "Training 53248/60000 \t 0.49696409702301025\n",
      "Training 55296/60000 \t 0.2831105589866638\n",
      "Training 57344/60000 \t 0.38883084058761597\n",
      "Training 59392/60000 \t 0.2622810900211334\n",
      "Wall time: 9.03 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "count = 0\n",
    "\n",
    "\n",
    "tensor_d.clear()\n",
    "\n",
    "model = model.cuda()\n",
    "\n",
    "for inputs, targets in train_loader:\n",
    "    \n",
    "    count += len(inputs)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    if count % 2048 == 0:\n",
    "        print(f'Training {count}/{len(train_loader.dataset)} \\t {loss.item()}')\n",
    "    \n",
    "    inputs = inputs.cuda()\n",
    "    targets = targets.cuda()\n",
    "\n",
    "    # Zero the optimizer before the forward pass\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # forward pass\n",
    "    out = model(inputs)\n",
    "\n",
    "    loss = F.nll_loss(out, targets)\n",
    "    loss.backward()\n",
    "    \n",
    "    #Here update the models\n",
    "    update_tensor_dict(model, tensor_d)\n",
    "    \n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[ 4.6149e-02,  2.5382e-02, -1.3919e-01],\n",
       "          [ 9.6464e-02,  2.1503e-01,  1.5693e-04],\n",
       "          [-1.7648e-01, -2.4237e-02,  6.4049e-02]]],\n",
       "\n",
       "\n",
       "        [[[-3.3523e-03, -3.0967e-02,  1.1224e-02],\n",
       "          [-1.0897e-01, -3.8601e-02,  9.7182e-03],\n",
       "          [-2.4516e-02, -2.1932e-02, -1.6157e-01]]],\n",
       "\n",
       "\n",
       "        [[[-2.8873e-02,  1.9957e-01,  3.3108e-01],\n",
       "          [-1.8774e-01,  1.5224e-01,  5.6744e-02],\n",
       "          [-5.9952e-01, -1.5616e-01, -4.9861e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 1.1531e-01,  2.2534e-01,  3.4327e-01],\n",
       "          [-3.0778e-01, -6.4887e-01, -4.0805e-01],\n",
       "          [-2.4696e-02,  2.3278e-01,  1.4784e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 2.8062e-01,  1.2544e-01, -5.1792e-01],\n",
       "          [-1.1023e-01,  1.4122e-02,  2.1188e-01],\n",
       "          [-3.9787e-01,  2.2591e-01,  1.5227e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 1.5141e-01, -2.1847e-02, -3.5513e-01],\n",
       "          [ 1.4703e-01, -1.5702e-01, -2.6748e-01],\n",
       "          [-2.7645e-01,  6.2271e-02, -1.9959e-01]]],\n",
       "\n",
       "\n",
       "        [[[-2.4078e-02, -1.1554e-01,  2.1567e-01],\n",
       "          [-5.6019e-02, -1.8340e-01, -2.1200e-01],\n",
       "          [-2.2825e-02,  2.5480e-02, -1.2767e-01]]],\n",
       "\n",
       "\n",
       "        [[[-1.0054e-01,  6.9154e-02, -5.9735e-02],\n",
       "          [-1.6162e-01,  9.4254e-03, -6.9683e-02],\n",
       "          [ 1.8265e-01, -1.0380e-01, -6.9773e-02]]],\n",
       "\n",
       "\n",
       "        [[[-2.1552e-02,  6.8996e-02, -2.6894e-01],\n",
       "          [ 1.5696e-01,  1.4090e-01,  1.8412e-01],\n",
       "          [ 2.5693e-01, -3.8493e-01,  1.6866e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 7.1984e-02,  5.4780e-02, -6.5815e-02],\n",
       "          [-1.1165e-01,  3.3974e-02,  6.6175e-02],\n",
       "          [ 1.5834e-01, -2.0041e-01, -1.1495e-01]]],\n",
       "\n",
       "\n",
       "        [[[-5.6381e-01, -8.8269e-02,  3.7612e-02],\n",
       "          [-6.0427e-02, -1.3222e-02, -8.9024e-02],\n",
       "          [ 2.6543e-01, -4.5413e-02,  1.4923e-01]]],\n",
       "\n",
       "\n",
       "        [[[-2.4291e-01,  5.4412e-02, -4.8316e-03],\n",
       "          [-1.9246e-01, -3.5184e-01, -3.3341e-01],\n",
       "          [ 1.1738e-01,  2.1076e-01, -7.3254e-02]]],\n",
       "\n",
       "\n",
       "        [[[-2.9593e-01,  1.0923e-01,  2.4832e-01],\n",
       "          [-8.4954e-02,  2.6851e-01, -3.7096e-01],\n",
       "          [-1.9026e-01, -1.9163e-01,  1.3516e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 1.7848e-02, -1.4107e-01,  2.1012e-01],\n",
       "          [-8.6272e-02,  2.0122e-01, -2.4627e-01],\n",
       "          [-4.5322e-01,  2.9415e-01,  2.4717e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 1.6683e-01, -1.5847e-01, -8.2337e-02],\n",
       "          [-2.0590e-01, -1.2419e-01,  8.8342e-02],\n",
       "          [-1.6635e-01, -5.8455e-02, -1.7692e-01]]],\n",
       "\n",
       "\n",
       "        [[[-3.0190e-01, -3.8100e-02,  1.8920e-01],\n",
       "          [ 2.6270e-01,  5.1818e-02,  1.0957e-01],\n",
       "          [-5.6958e-02, -1.1030e-01, -1.5478e-01]]],\n",
       "\n",
       "\n",
       "        [[[-9.6236e-02, -2.0171e-01, -1.0501e-01],\n",
       "          [ 2.9912e-01,  7.5408e-03, -2.7163e-01],\n",
       "          [-2.7699e-01,  3.5442e-02, -1.6926e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 1.9801e-01,  2.2630e-01,  9.0414e-02],\n",
       "          [ 1.7151e-01,  1.3927e-01, -1.7383e-01],\n",
       "          [ 9.7363e-03, -3.3082e-01, -3.7007e-01]]],\n",
       "\n",
       "\n",
       "        [[[-1.0144e-01, -1.9346e-01,  1.4042e-01],\n",
       "          [ 1.2111e-01, -7.6977e-02, -4.0347e-02],\n",
       "          [ 6.6766e-02, -2.8597e-01,  2.7013e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 1.4823e-01,  9.7808e-02, -4.1862e-01],\n",
       "          [-1.6894e-01, -2.1619e-01,  6.4059e-02],\n",
       "          [ 1.6781e-01,  7.2140e-02, -3.6731e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 1.8481e-01, -3.1172e-02, -9.3194e-03],\n",
       "          [ 4.1978e-02,  9.5680e-04,  1.8271e-01],\n",
       "          [-3.7717e-01, -4.9256e-02, -2.6825e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 6.1868e-02, -1.5397e-01, -9.5390e-02],\n",
       "          [-1.7540e-02, -2.2116e-01, -2.7122e-01],\n",
       "          [ 1.0472e-01, -1.1396e-01,  1.7255e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 1.7697e-01,  2.8498e-04,  1.1062e-01],\n",
       "          [-3.3442e-01, -2.4708e-01,  2.1974e-01],\n",
       "          [-1.2508e-01, -1.9393e-01, -4.6017e-02]]],\n",
       "\n",
       "\n",
       "        [[[-3.5698e-01,  2.3531e-02, -1.3425e-01],\n",
       "          [ 2.0762e-01,  1.7211e-01,  1.6394e-01],\n",
       "          [ 1.7932e-01, -1.6872e-01, -1.6218e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 1.4543e-02,  2.2748e-01,  1.3302e-01],\n",
       "          [-2.5602e-01, -5.0231e-01, -5.1170e-01],\n",
       "          [-1.9999e-01,  2.3879e-01,  2.0184e-01]]],\n",
       "\n",
       "\n",
       "        [[[-2.2629e-01, -1.2207e-01,  4.6575e-03],\n",
       "          [ 1.2790e-01, -3.3120e-01,  4.9659e-02],\n",
       "          [-3.3961e-02,  1.9035e-02, -1.3599e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 1.2382e-01,  6.0766e-02,  2.5422e-01],\n",
       "          [-2.4093e-01, -2.3448e-01, -1.0341e-01],\n",
       "          [ 1.4174e-01, -1.4443e-01, -1.7710e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 1.0242e-01, -1.0735e-01,  2.7192e-01],\n",
       "          [-1.0054e-01, -8.7075e-02, -4.2338e-01],\n",
       "          [-8.7953e-02,  1.2455e-01,  1.0210e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 9.2179e-02,  2.1799e-01, -1.2180e-01],\n",
       "          [-3.3440e-01, -3.1774e-01, -2.8739e-02],\n",
       "          [ 2.8975e-01, -2.4339e-01,  8.8082e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 5.7757e-03, -2.1975e-01,  2.4326e-01],\n",
       "          [ 1.5047e-01, -3.2758e-02, -2.6051e-01],\n",
       "          [-3.5518e-01,  3.7737e-03,  2.2855e-01]]],\n",
       "\n",
       "\n",
       "        [[[-2.9319e-01,  5.9031e-02, -4.8833e-02],\n",
       "          [-1.7843e-01, -6.6490e-02,  7.5385e-02],\n",
       "          [ 3.4655e-01, -2.0286e-01, -1.4674e-01]]],\n",
       "\n",
       "\n",
       "        [[[-1.1970e-01, -2.5258e-01,  2.5133e-01],\n",
       "          [-2.0340e-01,  1.4801e-01,  2.1317e-01],\n",
       "          [-3.4550e-01, -2.8482e-01,  3.6078e-01]]]], device='cuda:0',\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.conv1.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[-0.3718, -0.1068, -0.0426],\n",
       "          [ 0.2245,  0.1057, -0.2058],\n",
       "          [ 0.0035, -0.0243, -0.4423]]],\n",
       "\n",
       "\n",
       "        [[[-0.1578,  0.2234, -0.0661],\n",
       "          [-0.3676,  0.0397,  0.1246],\n",
       "          [ 0.2546, -0.0928, -0.2072]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0256,  0.1274,  0.1096],\n",
       "          [-0.2469, -0.3871, -0.0428],\n",
       "          [ 0.0592, -0.2570,  0.0906]]],\n",
       "\n",
       "\n",
       "        [[[-0.0878,  0.3395,  0.1095],\n",
       "          [-0.3896, -0.1001, -0.0935],\n",
       "          [ 0.2531,  0.2041,  0.1415]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1875, -0.1457, -0.0546],\n",
       "          [ 0.2055, -0.0356,  0.2497],\n",
       "          [ 0.1004,  0.0297, -0.2204]]],\n",
       "\n",
       "\n",
       "        [[[-0.1961, -0.3088, -0.2178],\n",
       "          [-0.1301, -0.1537, -0.1746],\n",
       "          [-0.0933,  0.3111,  0.2850]]],\n",
       "\n",
       "\n",
       "        [[[-0.1250, -0.1383, -0.0649],\n",
       "          [-0.0854, -0.0944,  0.1091],\n",
       "          [-0.0667,  0.0095, -0.1420]]],\n",
       "\n",
       "\n",
       "        [[[ 0.2797,  0.0304,  0.1671],\n",
       "          [-0.0938, -0.1174,  0.2365],\n",
       "          [-0.1415, -0.2085, -0.3060]]],\n",
       "\n",
       "\n",
       "        [[[-0.2415, -0.2675, -0.1665],\n",
       "          [-0.1457,  0.1657,  0.0904],\n",
       "          [-0.0782, -0.0153, -0.0194]]],\n",
       "\n",
       "\n",
       "        [[[-0.2678,  0.2532, -0.1863],\n",
       "          [-0.2584,  0.1718,  0.1137],\n",
       "          [ 0.1248,  0.0820,  0.1255]]],\n",
       "\n",
       "\n",
       "        [[[-0.2403, -0.2006,  0.0714],\n",
       "          [-0.1501, -0.3058, -0.3760],\n",
       "          [ 0.1327,  0.3635, -0.2932]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1056, -0.0941, -0.5819],\n",
       "          [ 0.2152,  0.2034,  0.0373],\n",
       "          [-0.3045, -0.3135, -0.0990]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1219, -0.3232, -0.0161],\n",
       "          [-0.4318,  0.1913, -0.0973],\n",
       "          [-0.0339,  0.1599, -0.4241]]],\n",
       "\n",
       "\n",
       "        [[[ 0.4728,  0.4162,  0.0873],\n",
       "          [-0.3022, -0.6619, -0.1498],\n",
       "          [-0.7444,  0.2464,  0.0731]]],\n",
       "\n",
       "\n",
       "        [[[-0.3994, -0.3683,  0.2900],\n",
       "          [-0.3270,  0.0875,  0.1897],\n",
       "          [-0.3313, -0.3889,  0.1607]]],\n",
       "\n",
       "\n",
       "        [[[-0.1686, -0.1570,  0.2312],\n",
       "          [-0.2616, -0.1529,  0.2590],\n",
       "          [-0.0456,  0.0546, -0.0374]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1205,  0.3328, -0.3016],\n",
       "          [-0.1203, -0.1844, -0.3084],\n",
       "          [ 0.1169,  0.0411,  0.2659]]],\n",
       "\n",
       "\n",
       "        [[[ 0.2722, -0.0572,  0.0880],\n",
       "          [ 0.1156, -0.0790, -0.3550],\n",
       "          [-0.0612,  0.0988,  0.1234]]],\n",
       "\n",
       "\n",
       "        [[[-0.2972,  0.2061, -0.0878],\n",
       "          [ 0.1590, -0.2632, -0.0615],\n",
       "          [ 0.0347, -0.0862, -0.0940]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0357, -0.1511, -0.0330],\n",
       "          [ 0.1701, -0.2336, -0.0721],\n",
       "          [-0.1157, -0.3320, -0.2007]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0911, -0.2066, -0.0687],\n",
       "          [ 0.1038,  0.0131, -0.0269],\n",
       "          [-0.1483, -0.0607, -0.0119]]],\n",
       "\n",
       "\n",
       "        [[[-0.3157, -0.1092,  0.0945],\n",
       "          [-0.3219,  0.0084, -0.1697],\n",
       "          [ 0.2122,  0.1347,  0.3054]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1439, -0.0880, -0.2584],\n",
       "          [ 0.1768, -0.2645, -0.3142],\n",
       "          [ 0.0010,  0.1970,  0.0246]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0153,  0.1767, -0.1499],\n",
       "          [ 0.0668, -0.4853,  0.0946],\n",
       "          [-0.4422,  0.1867, -0.3303]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0690,  0.0511,  0.3796],\n",
       "          [-0.0828, -0.4589, -0.2697],\n",
       "          [ 0.0679, -0.1270,  0.0211]]],\n",
       "\n",
       "\n",
       "        [[[ 0.2385, -0.2079, -0.0302],\n",
       "          [ 0.2059, -0.3131,  0.0583],\n",
       "          [ 0.0829, -0.0226,  0.1087]]],\n",
       "\n",
       "\n",
       "        [[[-0.1215, -0.0093,  0.2411],\n",
       "          [-0.1131,  0.4245, -0.0966],\n",
       "          [-0.6471, -0.2267, -0.5430]]],\n",
       "\n",
       "\n",
       "        [[[-0.2029, -0.2149, -0.1813],\n",
       "          [-0.1659,  0.0315, -0.2796],\n",
       "          [ 0.2441,  0.0272, -0.0536]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0573, -0.2308,  0.1232],\n",
       "          [-0.0457,  0.2041, -0.1694],\n",
       "          [ 0.0537, -0.1691,  0.1368]]],\n",
       "\n",
       "\n",
       "        [[[ 0.3143, -0.2012, -0.1635],\n",
       "          [ 0.1709, -0.0946,  0.2165],\n",
       "          [-0.2559, -0.2286, -0.2526]]],\n",
       "\n",
       "\n",
       "        [[[-0.1772,  0.1425, -0.4400],\n",
       "          [ 0.2887,  0.2090, -0.1411],\n",
       "          [-0.1609, -0.2194, -0.3293]]],\n",
       "\n",
       "\n",
       "        [[[-0.1644, -0.2208,  0.2693],\n",
       "          [-0.1949,  0.2577, -0.2833],\n",
       "          [ 0.1919, -0.1880,  0.0802]]]], requires_grad=True)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "before backup.conv1.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start with the REDIS AI part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import redisai as rai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAI_KUBE = '192.168.99.101'\n",
    "RAI_PORT_KUBE = 31618\n",
    "RAI = '192.168.99.102'\n",
    "PORT = 6379\n",
    "\n",
    "con = rai.Client(host=RAI, port=PORT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'OK'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con.tensorset('grad-conv1', c1_grad.numpy(), dtype='float32')\n",
    "con.tensorset('bias-conv1', c1_bias.numpy(), dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con.set('example', 'hola')\n",
    "con.set('exaaaaa', 'hola2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.23 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "a = con.tensorget('example:fc1-weight', as_numpy=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set all the model gradients to the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting example:conv1-weight\n",
      "Setting example:conv1-bias\n",
      "Setting example:conv2-weight\n",
      "Setting example:conv2-bias\n",
      "Setting example:fc1-weight\n",
      "Setting example:fc1-bias\n",
      "Setting example:fc2-weight\n",
      "Setting example:fc2-bias\n",
      "Wall time: 41.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "psId = 'example'\n",
    "\n",
    "for n, l in model.named_children():\n",
    "    if hasattr(l, 'bias'):\n",
    "        key_w = f'{psId}:{n}-weight'\n",
    "        key_b = f'{psId}:{n}-bias'\n",
    "\n",
    "        print('Setting', key_w)\n",
    "        con.tensorset(key_w, l.weight.cpu().detach().numpy(), dtype='float32')\n",
    "\n",
    "        print('Setting', key_b)\n",
    "        con.tensorset(key_b, l.bias.cpu().detach().numpy(), dtype='float32')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the model gradients from the dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting conv1-weight-grad\n",
      "Setting conv1-bias-grad\n",
      "Setting conv2-weight-grad\n",
      "Setting conv2-bias-grad\n",
      "Setting fc1-weight-grad\n",
      "Setting fc1-bias-grad\n",
      "Setting fc2-weight-grad\n",
      "Setting fc2-bias-grad\n"
     ]
    }
   ],
   "source": [
    "for k, v in tensor_d.items():\n",
    "    print('Setting' , k)\n",
    "    con.tensorset(f'{k}/1', v.cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try to get all of the layers from redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting example:conv1-weight\n",
      "Getting example:conv1-bias\n",
      "Getting example:conv2-weight\n",
      "Getting example:conv2-bias\n",
      "Getting example:fc1-weight\n",
      "Getting example:fc1-bias\n",
      "Getting example:fc2-weight\n",
      "Getting example:fc2-bias\n"
     ]
    }
   ],
   "source": [
    "for n, l in m2.named_children():\n",
    "    if hasattr(l, 'bias'):\n",
    "        key_w = f'{psId}:{n}-weight'\n",
    "        key_b = f'{psId}:{n}-bias'\n",
    "            \n",
    "        print('Getting', key_w)\n",
    "        t = con.tensorget(key_w)\n",
    "        l.weight = torch.nn.Parameter(torch.from_numpy(t))\n",
    "\n",
    "        print('Getting', key_b)\n",
    "        t = con.tensorget(key_b)\n",
    "        l.bias =torch.nn.Parameter(torch.from_numpy(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try to set the model to REDIS\n",
    "\n",
    "We can simply save the state dict and retrieve it super quickly from the following functions as a python object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_weights(m: nn.Module):\n",
    "    r\"\"\"After the init task we should save the model gradients to the database\n",
    "\n",
    "    Instead of looking if a layer has a bias term (some of the batch norm can have it,\n",
    "    look if the layer is of type conv or not\"\"\"\n",
    "    print('Saving model to the database')\n",
    "    with torch.no_grad():\n",
    "        for name, layer in m.named_modules():\n",
    "            if _is_optimizable(layer):\n",
    "\n",
    "                # Save the weights\n",
    "                print(f'Setting weights for layer {name}')\n",
    "                weight_key = f'{name}.weight'\n",
    "                con.tensorset(weight_key, layer.weight.cpu().detach().numpy(), dtype='float32')\n",
    "\n",
    "                # Save the bias if not None\n",
    "                if layer.bias is not None:\n",
    "                    print(f'Setting bias for layer {name}')\n",
    "                    bias_key = f'{name}.bias'\n",
    "                    con.tensorset(bias_key, layer.bias.cpu().detach().numpy(), dtype='float32')\n",
    "\n",
    "    print('Saved model to the database')\n",
    "    \n",
    "def load_model_weights(m: nn.Module):\n",
    "    \"\"\"Load the model weights saved in the database to start the new epoch\"\"\"\n",
    "    print('Loading model from database')\n",
    "    with torch.no_grad():\n",
    "        for name, layer in m.named_modules():\n",
    "            # only load and save layers that are optimizable (conv or fc)\n",
    "            if _is_optimizable(layer):\n",
    "\n",
    "                # Load the weight\n",
    "                print(f'Loading weights for layer {name}')\n",
    "                weight_key = f'{name}-weight'\n",
    "                w = con.tensorget(weight_key)\n",
    "                layer.weight = torch.nn.Parameter(torch.from_numpy(w))\n",
    "\n",
    "                # If the layer has an active bias retrieve it\n",
    "                # Some of the layers in resnet do not have bias\n",
    "                # or it is None. It is not needed with BN, so skip it\n",
    "                if layer.bias is not None:\n",
    "                    print(f'Loading bias for layer {name}')\n",
    "                    bias_key = f'{name}-bias'\n",
    "                    w = con.tensorget(bias_key)\n",
    "                    layer.bias = torch.nn.Parameter(torch.from_numpy(w))\n",
    "\n",
    "    print('Model loaded from database')\n",
    "    \n",
    "def load_state_dict(m: nn.Module) -> dict:\n",
    "    d = dict()\n",
    "    for name, layer in m.named_modules():\n",
    "        # only load and save layers that are optimizable (conv or fc)\n",
    "        if _is_optimizable(layer):\n",
    "\n",
    "            # Load the weight\n",
    "            print(f'Loading weights for layer {name}')\n",
    "            weight_key = f'{name}.weight'\n",
    "            w = con.tensorget(weight_key)\n",
    "            # set the weight\n",
    "            d[weight_key] = torch.from_numpy(w)\n",
    "\n",
    "            # If the layer has an active bias retrieve it\n",
    "            # Some of the layers in resnet do not have bias\n",
    "            # or it is None. It is not needed with BN, so skip it\n",
    "            if layer.bias is not None:\n",
    "                print(f'Loading bias for layer {name}')\n",
    "                bias_key = f'{name}.bias'\n",
    "                w = con.tensorget(bias_key)\n",
    "                # set the bias\n",
    "                d[bias_key] = torch.from_numpy(w)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights for layer conv1\n",
      "Loading bias for layer conv1\n",
      "Loading weights for layer conv2\n",
      "Loading bias for layer conv2\n",
      "Loading weights for layer fc1\n",
      "Loading bias for layer fc1\n",
      "Loading weights for layer fc2\n",
      "Loading bias for layer fc2\n"
     ]
    }
   ],
   "source": [
    "d = load_state_dict(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('conv1.weight',\n",
       "              tensor([[[[ 0.1247, -0.2343,  0.2413],\n",
       "                        [ 0.2814, -0.2949, -0.1152],\n",
       "                        [ 0.1794,  0.3214,  0.1535]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.2677, -0.2998,  0.2019],\n",
       "                        [-0.0371, -0.2684,  0.3079],\n",
       "                        [-0.1485, -0.1790,  0.1690]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0169, -0.2387,  0.1334],\n",
       "                        [ 0.0769,  0.2841,  0.1754],\n",
       "                        [ 0.1311,  0.0290, -0.0110]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0936, -0.2159, -0.1453],\n",
       "                        [-0.1982,  0.3253,  0.0502],\n",
       "                        [ 0.2596, -0.1228, -0.2570]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0238,  0.1362,  0.2403],\n",
       "                        [ 0.1665, -0.2756,  0.0901],\n",
       "                        [ 0.0269, -0.2431,  0.0739]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0283, -0.1184,  0.2224],\n",
       "                        [ 0.2876,  0.1003,  0.0171],\n",
       "                        [ 0.3254,  0.0351, -0.2718]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1060, -0.1569, -0.1678],\n",
       "                        [ 0.0926, -0.2940,  0.0310],\n",
       "                        [-0.1452, -0.3215,  0.2278]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.2087, -0.1302,  0.2816],\n",
       "                        [ 0.0435, -0.2506,  0.3203],\n",
       "                        [ 0.0833, -0.2626, -0.1649]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.2190, -0.1964, -0.3256],\n",
       "                        [ 0.1943,  0.0211,  0.1013],\n",
       "                        [ 0.1907, -0.2018, -0.0925]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0492,  0.2690, -0.2379],\n",
       "                        [ 0.0020,  0.0485, -0.2642],\n",
       "                        [-0.0905, -0.1493,  0.0986]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.1057, -0.3093, -0.2767],\n",
       "                        [-0.2187,  0.3241,  0.3186],\n",
       "                        [-0.3152,  0.0268, -0.1521]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1795, -0.3062,  0.2991],\n",
       "                        [-0.1849, -0.2924, -0.2218],\n",
       "                        [ 0.2065, -0.0773, -0.1366]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.2205,  0.1720,  0.2690],\n",
       "                        [ 0.0198,  0.0783,  0.2252],\n",
       "                        [-0.1337, -0.0486, -0.2206]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.2706,  0.1703, -0.1690],\n",
       "                        [ 0.1192, -0.0109, -0.0215],\n",
       "                        [-0.1182,  0.2284,  0.1592]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.1622,  0.0578, -0.1928],\n",
       "                        [ 0.2806, -0.0453, -0.1415],\n",
       "                        [-0.0471, -0.0205, -0.2808]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0781, -0.1415,  0.1409],\n",
       "                        [ 0.2078,  0.2646, -0.2408],\n",
       "                        [ 0.3144,  0.2644,  0.0706]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.3320, -0.2150,  0.1265],\n",
       "                        [ 0.0584,  0.3283,  0.1183],\n",
       "                        [ 0.0210,  0.1241,  0.3298]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.2746, -0.2968, -0.0102],\n",
       "                        [-0.1572,  0.0736, -0.2738],\n",
       "                        [ 0.1013,  0.0974, -0.2914]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0358,  0.0615, -0.1912],\n",
       "                        [-0.2387,  0.0762, -0.0883],\n",
       "                        [ 0.1041,  0.2567,  0.1519]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.3163,  0.2404, -0.2741],\n",
       "                        [-0.1645,  0.1094, -0.0427],\n",
       "                        [ 0.2345,  0.1663, -0.0512]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1575, -0.2630, -0.2015],\n",
       "                        [-0.1346,  0.0184,  0.3013],\n",
       "                        [ 0.3260,  0.1866, -0.1477]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.2786,  0.1309, -0.1751],\n",
       "                        [ 0.0363,  0.2865,  0.0417],\n",
       "                        [ 0.1138, -0.1176, -0.1190]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.2823,  0.0676,  0.1213],\n",
       "                        [ 0.2004, -0.0658,  0.1690],\n",
       "                        [ 0.3041,  0.3293,  0.1985]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.3233,  0.1186,  0.1138],\n",
       "                        [ 0.2104,  0.0249,  0.0212],\n",
       "                        [-0.0751,  0.3209, -0.2349]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.1769,  0.1650, -0.2573],\n",
       "                        [-0.1091,  0.1555, -0.0794],\n",
       "                        [-0.2734, -0.0381,  0.2775]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.1274,  0.1051, -0.0945],\n",
       "                        [ 0.2017,  0.3282,  0.0739],\n",
       "                        [-0.2947, -0.0428, -0.0159]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.1362, -0.1850, -0.1964],\n",
       "                        [-0.0043, -0.3110, -0.2698],\n",
       "                        [ 0.2767, -0.0441, -0.3085]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.1891,  0.3210,  0.1046],\n",
       "                        [ 0.0201,  0.2842,  0.1063],\n",
       "                        [ 0.1419,  0.0378,  0.0771]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0639, -0.1568, -0.1104],\n",
       "                        [ 0.3197, -0.3324, -0.2833],\n",
       "                        [ 0.0871, -0.1071, -0.1978]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.2013,  0.0359, -0.1809],\n",
       "                        [-0.3041, -0.0569,  0.2412],\n",
       "                        [ 0.2046, -0.0615, -0.0074]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.2029, -0.2125,  0.0676],\n",
       "                        [-0.0291,  0.1438, -0.2022],\n",
       "                        [-0.2324, -0.1334, -0.0909]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.1656, -0.2777, -0.2354],\n",
       "                        [-0.2589,  0.1045, -0.2118],\n",
       "                        [-0.1223, -0.0657,  0.2587]]]])),\n",
       "             ('conv1.bias',\n",
       "              tensor([ 0.2995, -0.2079, -0.0506, -0.0454,  0.0472,  0.2238, -0.1567, -0.2324,\n",
       "                      -0.0155,  0.3325,  0.2041, -0.0635,  0.1784,  0.1864,  0.1260,  0.2865,\n",
       "                      -0.1601,  0.1420,  0.1556,  0.1161,  0.1498, -0.2225,  0.0055, -0.0917,\n",
       "                      -0.2924, -0.2891,  0.2378, -0.2575, -0.1193,  0.2368,  0.2110, -0.3113])),\n",
       "             ('conv2.weight',\n",
       "              tensor([[[[-0.0384,  0.0104,  0.0047],\n",
       "                        [ 0.0186, -0.0052,  0.0306],\n",
       "                        [-0.0049,  0.0247, -0.0095]],\n",
       "              \n",
       "                       [[ 0.0436,  0.0526, -0.0016],\n",
       "                        [ 0.0405,  0.0279,  0.0582],\n",
       "                        [ 0.0502, -0.0216,  0.0152]],\n",
       "              \n",
       "                       [[-0.0261, -0.0176, -0.0436],\n",
       "                        [-0.0028, -0.0001, -0.0025],\n",
       "                        [ 0.0451,  0.0337, -0.0063]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0491,  0.0072,  0.0137],\n",
       "                        [ 0.0381,  0.0308,  0.0218],\n",
       "                        [-0.0442, -0.0209, -0.0411]],\n",
       "              \n",
       "                       [[-0.0020,  0.0192,  0.0250],\n",
       "                        [ 0.0302,  0.0135,  0.0099],\n",
       "                        [-0.0174, -0.0390,  0.0130]],\n",
       "              \n",
       "                       [[-0.0575,  0.0227,  0.0115],\n",
       "                        [-0.0516,  0.0259,  0.0469],\n",
       "                        [-0.0225,  0.0285,  0.0354]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0230, -0.0527, -0.0521],\n",
       "                        [-0.0216,  0.0294, -0.0015],\n",
       "                        [ 0.0552,  0.0045,  0.0007]],\n",
       "              \n",
       "                       [[-0.0239, -0.0435, -0.0108],\n",
       "                        [ 0.0374,  0.0310, -0.0371],\n",
       "                        [-0.0173,  0.0535,  0.0356]],\n",
       "              \n",
       "                       [[-0.0145,  0.0014,  0.0305],\n",
       "                        [-0.0066,  0.0054,  0.0574],\n",
       "                        [-0.0519,  0.0051, -0.0180]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0320, -0.0555,  0.0417],\n",
       "                        [-0.0281, -0.0148,  0.0471],\n",
       "                        [-0.0215,  0.0266, -0.0062]],\n",
       "              \n",
       "                       [[ 0.0206,  0.0457,  0.0215],\n",
       "                        [ 0.0069,  0.0559,  0.0243],\n",
       "                        [ 0.0377, -0.0459,  0.0497]],\n",
       "              \n",
       "                       [[-0.0005,  0.0574, -0.0222],\n",
       "                        [-0.0116,  0.0524,  0.0495],\n",
       "                        [-0.0223,  0.0115,  0.0136]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0238, -0.0406, -0.0054],\n",
       "                        [-0.0469,  0.0464,  0.0474],\n",
       "                        [-0.0391,  0.0557, -0.0563]],\n",
       "              \n",
       "                       [[-0.0515,  0.0458,  0.0312],\n",
       "                        [-0.0534, -0.0437,  0.0570],\n",
       "                        [-0.0265, -0.0015, -0.0307]],\n",
       "              \n",
       "                       [[ 0.0071,  0.0425,  0.0520],\n",
       "                        [-0.0545,  0.0285, -0.0589],\n",
       "                        [-0.0161,  0.0574,  0.0372]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0425,  0.0033,  0.0232],\n",
       "                        [ 0.0411,  0.0446, -0.0397],\n",
       "                        [ 0.0018,  0.0459, -0.0015]],\n",
       "              \n",
       "                       [[-0.0381,  0.0144,  0.0456],\n",
       "                        [ 0.0089,  0.0557, -0.0447],\n",
       "                        [-0.0298, -0.0262,  0.0380]],\n",
       "              \n",
       "                       [[ 0.0164,  0.0272,  0.0553],\n",
       "                        [ 0.0062, -0.0023,  0.0301],\n",
       "                        [ 0.0476, -0.0254, -0.0138]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0059, -0.0322, -0.0326],\n",
       "                        [ 0.0448,  0.0466,  0.0431],\n",
       "                        [ 0.0299,  0.0211, -0.0168]],\n",
       "              \n",
       "                       [[ 0.0424,  0.0482, -0.0233],\n",
       "                        [-0.0371,  0.0471,  0.0380],\n",
       "                        [ 0.0040,  0.0095,  0.0240]],\n",
       "              \n",
       "                       [[ 0.0200, -0.0298, -0.0103],\n",
       "                        [-0.0413, -0.0232, -0.0322],\n",
       "                        [ 0.0239,  0.0388,  0.0474]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0064, -0.0261,  0.0324],\n",
       "                        [-0.0393, -0.0054,  0.0336],\n",
       "                        [-0.0451, -0.0070,  0.0305]],\n",
       "              \n",
       "                       [[ 0.0111,  0.0111,  0.0386],\n",
       "                        [ 0.0031, -0.0201,  0.0018],\n",
       "                        [ 0.0406,  0.0479, -0.0185]],\n",
       "              \n",
       "                       [[ 0.0314,  0.0375, -0.0428],\n",
       "                        [ 0.0458,  0.0343,  0.0258],\n",
       "                        [ 0.0544, -0.0544,  0.0513]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0266, -0.0145, -0.0531],\n",
       "                        [-0.0255,  0.0026, -0.0340],\n",
       "                        [-0.0574, -0.0054,  0.0496]],\n",
       "              \n",
       "                       [[ 0.0481,  0.0549,  0.0522],\n",
       "                        [-0.0568, -0.0511,  0.0455],\n",
       "                        [ 0.0356, -0.0129, -0.0331]],\n",
       "              \n",
       "                       [[-0.0166, -0.0283, -0.0469],\n",
       "                        [ 0.0285,  0.0108,  0.0198],\n",
       "                        [ 0.0220,  0.0076,  0.0301]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0289,  0.0574, -0.0221],\n",
       "                        [-0.0378,  0.0268, -0.0321],\n",
       "                        [ 0.0155,  0.0419,  0.0389]],\n",
       "              \n",
       "                       [[ 0.0004,  0.0028, -0.0576],\n",
       "                        [-0.0030, -0.0324, -0.0174],\n",
       "                        [ 0.0383, -0.0501, -0.0151]],\n",
       "              \n",
       "                       [[ 0.0209, -0.0454,  0.0494],\n",
       "                        [ 0.0231, -0.0216,  0.0457],\n",
       "                        [ 0.0357, -0.0004, -0.0498]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0231,  0.0117,  0.0465],\n",
       "                        [-0.0531,  0.0348, -0.0337],\n",
       "                        [-0.0182,  0.0557, -0.0151]],\n",
       "              \n",
       "                       [[ 0.0577, -0.0487,  0.0200],\n",
       "                        [-0.0512, -0.0453, -0.0421],\n",
       "                        [ 0.0245, -0.0352,  0.0330]],\n",
       "              \n",
       "                       [[-0.0078, -0.0474, -0.0431],\n",
       "                        [ 0.0213,  0.0161, -0.0437],\n",
       "                        [ 0.0227, -0.0253,  0.0183]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0148,  0.0424,  0.0398],\n",
       "                        [ 0.0395, -0.0548, -0.0584],\n",
       "                        [-0.0013,  0.0571,  0.0566]],\n",
       "              \n",
       "                       [[-0.0237,  0.0246,  0.0146],\n",
       "                        [ 0.0063,  0.0217,  0.0339],\n",
       "                        [ 0.0441, -0.0308, -0.0570]],\n",
       "              \n",
       "                       [[-0.0290,  0.0563, -0.0185],\n",
       "                        [-0.0159, -0.0452, -0.0030],\n",
       "                        [ 0.0121, -0.0217,  0.0476]]]])),\n",
       "             ('conv2.bias',\n",
       "              tensor([ 0.0432,  0.0551, -0.0415, -0.0355,  0.0536,  0.0101,  0.0495,  0.0159,\n",
       "                      -0.0521, -0.0288,  0.0195, -0.0118, -0.0347, -0.0039,  0.0441,  0.0371,\n",
       "                      -0.0348,  0.0216,  0.0058,  0.0206, -0.0530,  0.0134, -0.0364, -0.0115,\n",
       "                      -0.0431,  0.0169, -0.0204, -0.0166,  0.0552,  0.0077, -0.0050,  0.0378,\n",
       "                      -0.0191,  0.0314, -0.0061, -0.0068, -0.0049, -0.0163,  0.0493, -0.0420,\n",
       "                      -0.0073, -0.0525, -0.0377,  0.0525, -0.0462, -0.0007,  0.0155, -0.0470,\n",
       "                      -0.0391,  0.0074, -0.0283,  0.0275, -0.0196,  0.0240,  0.0126,  0.0137,\n",
       "                       0.0557, -0.0551, -0.0060,  0.0347, -0.0005,  0.0376,  0.0277,  0.0483])),\n",
       "             ('fc1.weight',\n",
       "              tensor([[-0.0070,  0.0016, -0.0050,  ...,  0.0054,  0.0015, -0.0093],\n",
       "                      [ 0.0003, -0.0004,  0.0039,  ..., -0.0097, -0.0028, -0.0049],\n",
       "                      [-0.0074, -0.0003, -0.0039,  ...,  0.0090, -0.0082,  0.0029],\n",
       "                      ...,\n",
       "                      [-0.0104, -0.0100,  0.0023,  ...,  0.0082,  0.0028,  0.0098],\n",
       "                      [-0.0094,  0.0073, -0.0093,  ..., -0.0024,  0.0017, -0.0027],\n",
       "                      [ 0.0044, -0.0056, -0.0102,  ..., -0.0050, -0.0061,  0.0090]])),\n",
       "             ('fc1.bias',\n",
       "              tensor([-4.6180e-03, -3.2917e-03, -6.9536e-03, -1.0340e-02,  7.8915e-03,\n",
       "                       2.6675e-03,  8.4643e-03,  7.6002e-03, -1.2844e-03, -3.1385e-04,\n",
       "                       5.7034e-03, -9.6633e-03,  1.7595e-03,  5.4528e-03, -9.1192e-03,\n",
       "                       3.5367e-03, -5.3126e-03,  6.4501e-03,  6.1487e-03,  9.4620e-03,\n",
       "                       2.7314e-03, -8.9125e-03,  6.5928e-03,  1.9799e-03,  1.0051e-02,\n",
       "                       4.1180e-03,  2.7621e-03, -3.7905e-03,  5.3583e-03,  6.7356e-03,\n",
       "                       7.9255e-03,  5.0112e-03,  6.1937e-04, -2.1977e-03, -7.8272e-03,\n",
       "                      -4.1541e-03,  8.6165e-03,  1.8413e-03,  7.2036e-03,  5.0438e-03,\n",
       "                       5.5189e-03, -4.1967e-03, -6.7150e-03,  2.7584e-03, -9.5702e-03,\n",
       "                      -3.2314e-04, -5.8567e-03,  3.0438e-03,  9.8426e-04, -9.9090e-03,\n",
       "                      -8.7517e-03, -6.7785e-03, -1.4810e-03,  1.9983e-03, -9.3202e-03,\n",
       "                       6.2206e-04, -2.6936e-03,  8.8916e-03, -2.5338e-03, -4.2063e-03,\n",
       "                      -8.4956e-05,  7.4269e-03,  3.6085e-03,  3.0639e-03,  6.4696e-03,\n",
       "                      -9.7157e-03, -9.4571e-04, -9.8568e-03, -9.7352e-03, -7.3107e-03,\n",
       "                       7.8248e-04,  7.4229e-03, -6.4949e-03,  2.9071e-03,  7.6883e-03,\n",
       "                      -8.6734e-03, -9.1169e-03, -5.3964e-03, -5.8166e-05,  1.0232e-02,\n",
       "                       9.5215e-03,  8.0723e-03,  3.2283e-03,  6.6150e-03,  1.6620e-03,\n",
       "                      -2.9504e-03, -3.8070e-03,  4.0347e-04, -9.7498e-03, -5.3684e-03,\n",
       "                       4.4931e-03, -6.3881e-04, -6.6863e-03, -6.9763e-03, -8.8737e-03,\n",
       "                      -5.5864e-03,  6.2474e-03, -9.0128e-03,  1.0033e-02,  7.6529e-03,\n",
       "                      -8.3498e-03,  5.1295e-03,  1.5541e-03, -6.5935e-03, -1.6854e-03,\n",
       "                      -9.9526e-03, -8.7118e-03,  1.9663e-05,  5.7853e-03, -7.1165e-03,\n",
       "                      -1.2842e-03, -5.8104e-03, -1.0275e-03,  6.5078e-04, -8.7676e-03,\n",
       "                      -1.1386e-03, -8.4657e-03,  9.1704e-03,  3.2313e-03, -2.1498e-03,\n",
       "                      -9.8848e-04,  8.8118e-03,  2.4656e-03, -4.7301e-03, -9.5358e-03,\n",
       "                       4.2989e-03,  3.2628e-04, -5.8447e-03])),\n",
       "             ('fc2.weight',\n",
       "              tensor([[-0.0510,  0.0116, -0.0369,  ...,  0.0451,  0.0071,  0.0069],\n",
       "                      [ 0.0365,  0.0055, -0.0806,  ...,  0.0276, -0.0369,  0.0576],\n",
       "                      [-0.0733,  0.0694,  0.0395,  ..., -0.0265,  0.0268, -0.0748],\n",
       "                      ...,\n",
       "                      [ 0.0432, -0.0457, -0.0447,  ...,  0.0634, -0.0639,  0.0525],\n",
       "                      [-0.0325,  0.0696,  0.0082,  ..., -0.0460,  0.0489,  0.0244],\n",
       "                      [ 0.0003,  0.0086, -0.0187,  ...,  0.0616, -0.0811,  0.0511]])),\n",
       "             ('fc2.bias',\n",
       "              tensor([-0.0794,  0.0834,  0.0008,  0.0558,  0.0645, -0.0091, -0.0188,  0.0394,\n",
       "                      -0.0705, -0.0398]))])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "p = pickle.dumps(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con.set('model', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 29.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "d = con.get('model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pickle.loads(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the layer names in redis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "m = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['conv1', 'conv2', 'fc1', 'fc2']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How to get the weighted layers and save the model\n",
    "[n for n, l in m.named_children() if hasattr(l, \"bias\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = \" \".join([n for n, l in m.named_children() if hasattr(l, \"bias\")])\n",
    "con.set(\"layers\", l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in [n for n, l in m.named_children() if hasattr(l, \"bias\")]:\n",
    "    con.rpush('layers', l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[\"conv1\", \"conv2\", \"fc1\", \"fc2\"]'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "layers = ['conv1', 'conv2', 'fc1', 'fc2']\n",
    "l = json.dumps(layers)\n",
    "\n",
    "l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the layers by making an http request to the server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"conv1\", \"conv2\", \"fc1\", \"fc2\"]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(l)\n",
    "\n",
    "requests.post(\"http://localhost:58682/layers\", data=l)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
